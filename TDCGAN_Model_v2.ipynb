{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6lhMl8UJuZwl",
        "gd-1Os0wvyg1",
        "blzcqzz13WEr",
        "g0Ftjen4hT3n",
        "nLjfD-UWeEXT",
        "MAAobTgjeBnz",
        "4QbGTMgHd2fT",
        "gVLPBl20yhyz",
        "yPnxOR1BeNlo",
        "wMW4T3EmtT2E",
        "23KIY8XptvUZ",
        "aq6EPGPqx6n5",
        "UBcvbKLOfwbt",
        "neWHtdZt9K7M",
        "CMtU6unFqkOM",
        "1iBIrTvuwMHI",
        "JAPkmd_UFwh5",
        "Lbn9WwSDFNIS",
        "Gp27KKt4FSI6",
        "xPHRALUrIg0i",
        "WQDd0w8yIvh4",
        "ki5YaBkUwgsO",
        "OIQ10x9sx7ki",
        "sk0A1I0Ax7lE",
        "eaUlQjsCx7lF",
        "Hn8YhYkbx7lG",
        "U6ynaJX5-wME",
        "4sidzQ5A-wMl",
        "3aE7UTuexLLM",
        "RFlEUROixLLO",
        "JEI7YYh-xLLQ",
        "HK5cWwDgxLLR",
        "1mpdF4pQxLLT",
        "uBymezFqxLLT"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO9gOqcmmkjwzpWcXfqvE9o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ac2308269ba48d5942d8460677faf8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_736fab2900b54f7391c72ba7d807ec8d",
              "IPY_MODEL_fc17e67498c04663bd22eff4d7d7fa33",
              "IPY_MODEL_61a21e41a3cd4790aa57e80201b7e5b2"
            ],
            "layout": "IPY_MODEL_4bf270b45c0144eb83e1073fcd073f6d"
          }
        },
        "736fab2900b54f7391c72ba7d807ec8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70147ad69d0f48e3a38e603e1324a9be",
            "placeholder": "​",
            "style": "IPY_MODEL_2bda29b21fa444c4a26503d80f57d05a",
            "value": "Epoch:   0%"
          }
        },
        "fc17e67498c04663bd22eff4d7d7fa33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a85a6fea7881416e97fbb0a4de378c1b",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ca2a236a3754641b29635aa7dbe79db",
            "value": 0
          }
        },
        "61a21e41a3cd4790aa57e80201b7e5b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_310ddc25840e4f9fb2cc94defec23939",
            "placeholder": "​",
            "style": "IPY_MODEL_903fb65cad6d4db681dbb446aa37a8a3",
            "value": " 0/500 [00:06&lt;?, ?epoch/s]"
          }
        },
        "4bf270b45c0144eb83e1073fcd073f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70147ad69d0f48e3a38e603e1324a9be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bda29b21fa444c4a26503d80f57d05a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a85a6fea7881416e97fbb0a4de378c1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ca2a236a3754641b29635aa7dbe79db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "310ddc25840e4f9fb2cc94defec23939": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "903fb65cad6d4db681dbb446aa37a8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SolinSM/TDCGAN/blob/main/TDCGAN_Model_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn8D62ciqzh2"
      },
      "source": [
        "**TDCGAN Model**\n",
        "Need To add Folder Called 'Datasets' -> and inside it add your CSV files\n",
        "\n",
        "Here nead to run each step in this list before Run Code\n",
        "\n",
        "*   init libraries\n",
        "*   init Global Definition\n",
        "*   Color List + Style Text\n",
        "-----\n",
        "*   Read Multi Files csv\n",
        "*   Pre-process Data\n",
        "-----\n",
        "*   Build Models\n",
        "    *   Build Generator Model\n",
        "    *   Build Discriminator Model\n",
        "    *   Build Combined Model\n",
        "*   Print Models Summary\n",
        "-----\n",
        "*   Train\n",
        "    *   Train Models  + Calculate Losses + Plot Losses\n",
        "-----\n",
        "*   evaluate_on_real\n",
        "*   generate_synthetic_data    (not added yet)\n",
        "-----\n",
        "*   Run Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lhMl8UJuZwl"
      },
      "source": [
        "# **libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoevigoMucqs",
        "outputId": "bb736e0f-dfe6-4bb0-dcb8-61a7bdb3748e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#for read csv file\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "# pre-process\n",
        "##for stop word\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "## not used now\n",
        "#import unicodedata     # Remove accents\n",
        "#import string\n",
        "\n",
        "import sklearn\n",
        "\n",
        "## for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer      ## WordPunctTokenizer --> splits words based on punctuation boundaries.\n",
        "\n",
        "## for divide data to (train / test/ validate)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# for one-hor encode (sentence to 2D)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# for TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "\n",
        "# for TDCGAN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Concatenate,\n",
        "    Embedding,\n",
        "    Dense,\n",
        "    LeakyReLU,\n",
        "    BatchNormalization,\n",
        "    Dropout,\n",
        "    Reshape,\n",
        ")\n",
        "\n",
        "import keras\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# taqadum in arabic , progress/process in english\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(tf. __version__)      # tensorflow version\n",
        "\n",
        "!python --version           # python version\n",
        "\n",
        "print(sklearn.__version__)         # scikit-learn version\n",
        "\n",
        "print(keras.__version__)           # keras version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsB-rc6YGFnD",
        "outputId": "b8c4a682-1b0c-46b8-818e-d1a81efdc764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "Python 3.10.12\n",
            "1.2.2\n",
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd-1Os0wvyg1"
      },
      "source": [
        "# **definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PycJ6J6zv06N"
      },
      "outputs": [],
      "source": [
        "NUM_DISCRIMINATORS = 3\n",
        "GENERATOR_DROPOUT_RATE = 0.2  #0.1\n",
        "DISCRIMINATOR_DROPOUT_RATE = 0.3      #Adjust the dropout rate to prevent overfitting during training.\n",
        "LEAKY_RELU_ALPA = 0.2\n",
        "\n",
        "NUM_EPOCHS = 500   #1000\n",
        "BATCH_SIZE = 128\n",
        "OPTIMIZER_LR = 0.0001                 # learning rate\n",
        "OPTIMIZER_BETAS = (0.5, 0.999)\n",
        "\n",
        "# Save losses for plotting\n",
        "d0_real_losses = []   # left discriminator losses   (disc 0)\n",
        "d0_fake_losses = []   # left discriminator losses   (disc 0)\n",
        "d0_losses      = []   # discriminator losses        (disc 0)\n",
        "\n",
        "d1_real_losses = []   # Middle discriminator losses (disc 1)\n",
        "d1_fake_losses = []   # Middle discriminator losses (disc 1)\n",
        "d1_losses      = []   # discriminator losses        (disc 1)\n",
        "\n",
        "d2_real_losses = []   # right discriminator losses  (disc 2)\n",
        "d2_fake_losses = []   # right discriminator losses  (disc 2)\n",
        "d2_losses      = []   # discriminator losses        (disc 2)\n",
        "\n",
        "g_losses       = []   # generator losses\n",
        "d_losses       = []   # discriminator losses\n",
        "\n",
        "\n",
        "\n",
        "# Suppress warnings from numpy\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blzcqzz13WEr"
      },
      "source": [
        "# **Color list + TextStyle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKQbM4263W02"
      },
      "outputs": [],
      "source": [
        "colors_list = [\n",
        "    'Red', 'Green', 'Blue', 'Purple', 'Orange', 'Pink', 'Brown', 'Yellow',\n",
        "    'Cyan', 'Magenta', 'Lime', 'Teal', 'Lavender', 'Maroon', 'Navy', 'Olive', 'Silver', 'Gold',\n",
        "    'Indigo', 'Turquoise', 'Beige', 'Crimson', 'Salmon','Tan','Lime', 'Fuchsia', 'Plum',\n",
        "    'Tomato', 'Violet']\n",
        "\n",
        "class TextStyle:\n",
        "    # Font Styles\n",
        "    BOLD = '\\033[1m'\n",
        "    DIM = '\\033[2m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "    BLINK = '\\033[5m'\n",
        "    REVERSE = '\\033[7m'\n",
        "    RESET_ALL = '\\033[0m'\n",
        "\n",
        "    # Font Colors\n",
        "    BLACK = '\\033[30m'\n",
        "    RED = '\\033[31m'\n",
        "    GREEN = '\\033[32m'\n",
        "    YELLOW = '\\033[33m'\n",
        "    BLUE = '\\033[34m'\n",
        "    MAGENTA = '\\033[35m'\n",
        "    CYAN = '\\033[36m'\n",
        "    WHITE = '\\033[37m'\n",
        "\n",
        "    # Background Colors\n",
        "    BG_BLACK = '\\033[40m'\n",
        "    BG_RED = '\\033[41m'\n",
        "    BG_GREEN = '\\033[42m'\n",
        "    BG_YELLOW = '\\033[43m'\n",
        "    BG_BLUE = '\\033[44m'\n",
        "    BG_MAGENTA = '\\033[45m'\n",
        "    BG_CYAN = '\\033[46m'\n",
        "    BG_WHITE = '\\033[47m'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0Ftjen4hT3n"
      },
      "source": [
        "# **Read Multi Files csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km03UwAwhdIF"
      },
      "outputs": [],
      "source": [
        "\"\"\" ----------------- Read CSV File Function ----------------- \"\"\"\n",
        "def read_csv_files(dataset_directory, percent):\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Handling Read CSV Files ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "  files = [f for f in os.listdir(dataset_directory) if f.endswith('.csv')]\n",
        "\n",
        "  if files == []:\n",
        "    print('Not found any csv files')\n",
        "  else:\n",
        "    print('Your files are: ', files)\n",
        "\n",
        "    np_array_values = []\n",
        "    data_df = pd.DataFrame()\n",
        "    firstFile = True\n",
        "\n",
        "    for file in files:\n",
        "      file_path = os.path.join(dataset_directory, file)   # csv_file_path\n",
        "      print('File Path: ', file_path)\n",
        "\n",
        "      try:\n",
        "        df = ''\n",
        "        df = pd.read_csv(file_path, encoding = \"ISO-8859-1\")  #.head()   #,low_memory=False   ISO-8859-1\n",
        "        total_rows = len(df)\n",
        "        print('Total rows in df/file: ', total_rows)\n",
        "\n",
        "        num_rows = int(total_rows * (percent / 100))\n",
        "        print('Total rows in df/file 100%: ', num_rows)\n",
        "\n",
        "\n",
        "        \"\"\" Start From Teacher Code \"\"\"\n",
        "        # Generate a list of random indices\n",
        "        random_indices = random.sample(range(total_rows), num_rows)\n",
        "        #print('random_indices: ' , random_indices)\n",
        "\n",
        "        # Select the random rows from the DataFrame\n",
        "        temp_df = df.iloc[random_indices]\n",
        "        if(firstFile):\n",
        "          # Concatenate all DataFrames into one\n",
        "          data_df = temp_df.copy()\n",
        "          firstFile = False\n",
        "        else:\n",
        "          # Concatenate all DataFrames into one\n",
        "          data_df = pd.concat([data_df,temp_df], ignore_index=True)\n",
        "\n",
        "        print(data_df)\n",
        "        return data_df\n",
        "        \"\"\" End From Teacher Code \"\"\"\n",
        "\n",
        "        ## Add DataFrame to new CSV file\n",
        "        #new_csv_file_path = os.path.join(dataset_directory, 'new_sqli.csv')  # \"/content/dataset/new_sqli.csv\"\n",
        "        #df.to_csv(new_csv_file_path, index=False)\n",
        "\n",
        "      except Exception as e:\n",
        "        print('Can not Read File called : ', file)\n",
        "        print('File path: ', file_path)\n",
        "        print(\"Errpr Exception e : \", e)\n",
        "\n",
        "\n",
        "#\"\"\" ----------------- Apply Code ----------------- \"\"\"\n",
        "#dataset_directory = \"/content/datasets\"   #files_path\n",
        "#percent = 100\n",
        "#data = read_csv_files(dataset_directory,percent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVo6VxdQnc8j"
      },
      "source": [
        "# **pre-process data**\n",
        "\n",
        "* Input text String\n",
        "* Remove white spaces from start & end in text, using strip()\n",
        "* Convert all letters of the string to lower case\n",
        "* Remove accents (comment)\n",
        "* Check for NaN & infinit Values (switch to 0)\n",
        "* Tokenization\n",
        "* Remove stop words, and except some important words used in queries\n",
        "* Split data to Train/Test Data (70% / 30%)  --> (train_test_split)\n",
        "* TfidfVectorizer instead of OneHotEncoder\n",
        "* one_hot_encoder\n",
        "* target_encode\n",
        "* MinMaxScaler\n",
        "* PCA\n",
        "\n",
        "ref code: [https://www.geeksforgeeks.org/python-ai/?ref=shm](https://www.geeksforgeeks.org/python-ai/?ref=shm)\n",
        "\n",
        "\n",
        "***Functions to used here (Extraction Features)***\n",
        "*  filter_stop_words\n",
        "*  tokenization_text  -->  contain filter_stop_words func.\n",
        "*  Tfidf_Vectorizer\n",
        "*  one_hot_encoder\n",
        "*  target_encode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLjfD-UWeEXT"
      },
      "source": [
        "## **filter_stop_words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4no9zNRGdhzS"
      },
      "outputs": [],
      "source": [
        "## ----- filter_stop_words ------ ##\n",
        "\n",
        "\"\"\"\n",
        "Filters stop words from a list of tokens, keeping words in the exception list.\n",
        "Args:\n",
        "    word_tokens: A list of tokens (words).\n",
        "    except_stop_word: A set of stop words to be excluded from removal (optional).\n",
        "Returns:\n",
        "    A list of tokens with stop words removed, except for those in the exception list.\n",
        "\"\"\"\n",
        "def filter_stop_words(word_tokens):\n",
        "    stop_words = set(stopwords.words('english'))  # Load default stop words\n",
        "    except_stop_word = set(['and', 'or', 'where', 'from', 'over'])  # Exception list defined here\n",
        "\n",
        "    filtered_sentence = []\n",
        "    for w in word_tokens:\n",
        "        if w in except_stop_word or w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "\n",
        "    return filtered_sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAAobTgjeBnz"
      },
      "source": [
        "## **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kcD8_-hdpRR"
      },
      "outputs": [],
      "source": [
        "## ----- Tokenization ------ ##\n",
        "\n",
        "\"\"\"\n",
        "# Tokenization + stop word\n",
        "Handle Main column in dataset 'sentence'\n",
        "by encoding it using (Tokenization handlling OOV + stop_word  , TfidfVectorizer )\n",
        "OOV :\n",
        "    ensures any new words in the test data are mapped to a common OOV token.\n",
        "    ensure robustness and accuracy when dealing with new, unseen words.\n",
        "\"\"\"\n",
        "def tokenization_text(sentences, x_name = ''):      #sentences):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Tokenization + stop word (\" , x_name , f\") ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    #sentences = X['sentence']\n",
        "    print('sentences: ' , len(sentences) )\n",
        "\n",
        "    #define OOV token text to use it when new token appear and not in train tokens data\n",
        "    #oov_token='<OOV>'\n",
        "\n",
        "    tokenizer = WordPunctTokenizer()\n",
        "    #tokenizer.oov_token = '<UNK>'   # '<OOV>'\n",
        "    tokens = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        #word_tokens = word_tokenize(sentence)\n",
        "        word_tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "        # Replace OOV words with the specified token\n",
        "        #word_tokens_with_oov = [token if token in tokenizer.vocab else oov_token for token in word_tokens]\n",
        "\n",
        "        # stop_words  (filter tokeniz word by remove any stop word with except some word as from, where ... etc)\n",
        "        filtered_tokens = filter_stop_words(word_tokens)\n",
        "\n",
        "\n",
        "        ########tokens.extend(filtered_tokens)    ## for test 8:18 20-5-2024    #######\n",
        "        tokens.append(filtered_tokens)\n",
        "\n",
        "        # Join tokens back to string for TF-IDF\n",
        "        #tokens.append(' '.join(filtered_tokens))\n",
        "\n",
        "\n",
        "    #print('len tokens before unique: ', len(tokens))\n",
        "    #print('1 tokens: ', tokens )\n",
        "\n",
        "    # Get unique tokens ( Remove duplicates using set() and convert back to list )\n",
        "    ######tokens = list(set(tokens))    #######\n",
        "    #print('len tokens after unique: ', len(tokens))\n",
        "    #print('2 tokens: ', tokens )\n",
        "\n",
        "    print(\"tokens len:\", len(tokens) )\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    for token in tokens:\n",
        "      if i < 5:\n",
        "        print(\"tokens:\", tokens[i])\n",
        "        i = i + 1\n",
        "      else:\n",
        "        break\n",
        "    \"\"\"\n",
        "    return tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QbGTMgHd2fT"
      },
      "source": [
        "## **TF-IDF Vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or_DpAOjicTo"
      },
      "outputs": [],
      "source": [
        "## ----- TF-IDF Vectorizer ------ ##\n",
        "\n",
        "\"\"\"\n",
        "  Fit TF-IDF Vectorizer on the training data\n",
        "  # Tf-idf Vectorizer (Term Frequency-Inverse Document Frequency Vectorizer)\n",
        "  # Used to convert text data to numerical features\n",
        "  # and get each one weight\n",
        "  # care about frequency of a word within a sentence (term frequency)\n",
        "  # and its rarity across the entire dataset (inverse document frequency)\n",
        "  # It used for feature extraction in natural language processing (NLP).\n",
        "\"\"\"\n",
        "def Tfidf_Vectorizer(tokenized_sentence_joined, original_sentences, x_name):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ TfidfVectorizer (\" , x_name , f\") ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer()   #max_features=5000)\n",
        "\n",
        "    # Fit TF-IDF Vectorizer on the training data   #X_train_tfidf\n",
        "    #/if(x_name == 'x_train'):\n",
        "    X_tfidf = tfidf_vectorizer.fit_transform(tokenized_sentence_joined)     # X_train\n",
        "    #/else:\n",
        "    #/  X_tfidf = tfidf_vectorizer.transform(tokenized_sentence_joined)     # X_test\n",
        "\n",
        "    #print('x_name : ' , x_name)\n",
        "    #print('X_tfidf : ' , X_tfidf)\n",
        "\n",
        "    # Extract feature names\n",
        "    X_tfidf_selected_features = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Checking the shape of the output matrices\n",
        "    print('Shape of X_tfidf : ', X_tfidf.shape)\n",
        "\n",
        "    # Convert to DataFrame for better readability  #  df_X_train_tfidf\n",
        "    df_X_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=X_tfidf_selected_features)   # index = original_sentences,index = original_sentences,\n",
        "    print(\"Training data TF-IDF features: \\n\" , df_X_tfidf.head())\n",
        "\n",
        "    return X_tfidf, df_X_tfidf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVLPBl20yhyz"
      },
      "source": [
        "## **one_hot_encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75F-BFHMejYR"
      },
      "outputs": [],
      "source": [
        "## ----- one_hot_encoder_func ------ ##\n",
        "\n",
        "def one_hot_encoder_func(X, one_hot_cols):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Perform one-hot encoding ------------{TextStyle.RESET_ALL}\")\n",
        "    one_hot_encoded_df = X.copy()\n",
        "\n",
        "    # Perform one-hot encoding\n",
        "    one_hot_encoded_1 = pd.get_dummies(one_hot_encoded_df, columns=one_hot_cols, prefix=one_hot_cols)\n",
        "    #print('one_hot_encoded : ', one_hot_encoded_1)\n",
        "\n",
        "    # Get only columns that is original columns and needed as them + not neet to convert them from boolean to int\n",
        "    origin_needed_cols = one_hot_encoded_1[[col for col in one_hot_encoded_1.columns if\n",
        "                                            not any(col.startswith(col_name) for col_name in one_hot_cols)]]\n",
        "\n",
        "    # Get only columns that created new by one-hoe-encoder (get_dummies)\n",
        "    one_hot_cols_df = one_hot_encoded_1[[col for col in one_hot_encoded_1.columns if\n",
        "                                          any(col.startswith(col_name) for col_name in one_hot_cols)]]\n",
        "\n",
        "    # Convert one_hot_cols_df values from boolean to integers (1 and 0)\n",
        "    one_hot_cols_df = one_hot_cols_df.astype(int)\n",
        "\n",
        "    # Concatenate the one_hot_cols_df with the origin_needed_cols\n",
        "    X = pd.concat([origin_needed_cols, one_hot_cols_df], axis=1)\n",
        "    print('end one hot encoder : \\n', X.head())\n",
        "\n",
        "    \"\"\"\n",
        "      Ex of previous row:\n",
        "          i  color  size              i     color_blue   color_red   size_M   size_S\n",
        "          ---------------           ---------------------------------------------------\n",
        "          0   red    S                0         0            1          0       1\n",
        "          1   blue   M          ==>   1         1            0          1       0\n",
        "          2   blue   S                2         1            0          0       1\n",
        "    \"\"\"\n",
        "\n",
        "    return X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPnxOR1BeNlo"
      },
      "source": [
        "## **target_encode**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrO9TTujePsb"
      },
      "outputs": [],
      "source": [
        "## ----- target_encode ------ ##\n",
        "\n",
        "\"\"\"\n",
        "  # target encoding/mean encoding\n",
        "  # useful for encoding categorical features into numerical features based on their relationship with the target variable.\n",
        "  # target_column : deciding it will depend on the objective of your analysis or model training.\n",
        "  # It is typically the variable you're trying to predict in a supervised learning task.\n",
        "\n",
        "  NOTE::\n",
        "    Target-encode might not be ideal for SQL sentences\n",
        "      1. SQL sentences are unstructured text data with potentially high cardinality and no inherent ordering.\n",
        "      2. Target encoding works better for categorical variables with a clear relationship to the target,\n",
        "          rather than free-form text.\n",
        "\"\"\"\n",
        "def target_encode(X, y, target_encode_cols, target_column):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Perform target encoding ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    target_encoded_df = X.copy()\n",
        "    print('target_encoded_df: ', target_encoded_df.head())\n",
        "\n",
        "    # this colum choisen because it is needed to predict sentence is normal or sqli\n",
        "    #target_column = class_Col # y   #encoding_col\n",
        "    for col in target_encode_cols:\n",
        "\n",
        "        # Check if the column exists\n",
        "        if col not in X.columns:\n",
        "            raise KeyError(f\"Column '{col}' not found in DataFrame\")\n",
        "\n",
        "        # Concate y  --> class_Col to X now then next remove it\n",
        "        #class_Col_df = y.copy()\n",
        "        X = pd.concat([X, y], axis=1)\n",
        "\n",
        "        # Calculate the mean target value for each category  (the mean float no for each category in column)\n",
        "        mean_encoding = X.groupby(col)[target_column].mean()\n",
        "        print('mean_encoding: ', mean_encoding.head())\n",
        "\n",
        "        # Map the mean values back to the DataFrame\n",
        "        target_encoded_df[col + '_encoded'] = X[col].map(mean_encoding)\n",
        "        #print('target_encoded_df: ', target_encoded_df.head())\n",
        "\n",
        "        # Drop class_Col added to do this part\n",
        "        X = X.drop(target_column, axis=1)\n",
        "\n",
        "        # Drop the original categorical column\n",
        "        target_encoded_df = target_encoded_df.drop(col, axis=1)\n",
        "        print('target_encoded_df: ', target_encoded_df.head())\n",
        "\n",
        "        X = target_encoded_df\n",
        "\n",
        "    return X\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldBpIOkHpFLI"
      },
      "source": [
        "## **MinMaxScaler**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jsU2T_bpGil"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "def apply_scaler(X_data, x_name):\n",
        "  # Scaling Features :: Standardize the features (PCA works better on standardized data)\n",
        "\n",
        "  #print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Scaling Features using 'StandardScaler' ------------{TextStyle.RESET_ALL}\")\n",
        "  #scaler = StandardScaler()\n",
        "\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Scaling Features using 'MinMaxScaler' ------------{TextStyle.RESET_ALL}\")\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  #/if x_name == 'x_train':\n",
        "  X_scaled = scaler.fit_transform(X_data.drop(['sentence'], axis=1))\n",
        "  #/else:\n",
        "  #/  X_scaled = scaler.transform(X_data.drop(['sentence'], axis=1))\n",
        "  #/print('X_scaled : ' , X_scaled.head())\n",
        "\n",
        "  # Convert the scaled numerical features back to DataFrames\n",
        "  X_scaled_df = pd.DataFrame(X_scaled, columns=X_data.drop(['sentence'], axis=1).columns)\n",
        "  #/print('X_scaled_df : ' , X_scaled_df.head())\n",
        "\n",
        "  return X_scaled, X_scaled_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk5eOJg8bDNj"
      },
      "source": [
        "## **performs feature selection using PCA (not used yet)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og-1WONtTzDG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "performs feature selection using Principal Component Analysis (PCA)\n",
        "Args:\n",
        "  data: X , which is array value of (num_rows, features)\n",
        "  n_components : cols num. without dropd y column (column features)\n",
        "  dataset_name : just name of dataset file\n",
        "Returns:\n",
        "  selected features df  (X_train_final_df, X_test_final_df)\n",
        "\"\"\"\n",
        "def simple_perform_pca_new(n_components, x_name, X_scaled , y ):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ PCA ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    # Fit & Transform PCA on the training data\n",
        "    pca = PCA(n_components=n_components, random_state=453)\n",
        "    print('pca: ' , pca)\n",
        "\n",
        "\n",
        "    #/if(x_name == 'x_train'):\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "    #/else:\n",
        "    #/    X_pca = pca.transform(X_scaled)   # Only Transform the test data using the same PCA , to avoid data leakage\n",
        "\n",
        "    print('X_pca: ' , X_pca)\n",
        "    print(456)\n",
        "\n",
        "    # Convert PCA results back to DataFrames and concatenate with the original non-sentence columns\n",
        "    # Generate new feature names for PCA components\n",
        "    pca_feature_names = [f'pca_{i+1}' for i in range(X_pca.shape[1])]\n",
        "\n",
        "    # Create a DataFrame with the PCA-transformed data\n",
        "    X_pca_df = pd.DataFrame(data=X_pca, columns=pca_feature_names)\n",
        "\n",
        "    ##X_pca_df = pd.DataFrame(data=X_pca, columns=X_train_feature_names)\n",
        "    print('X_pca_df: ', X_pca_df)\n",
        "    print('X_pca_df shape: ', X_pca_df.shape)\n",
        "\n",
        "    print(789)\n",
        "    # Combine the PCA-transformed features with the target variable\n",
        "    X_final_df = pd.concat([X_pca_df, y.reset_index(drop=True)], axis=1)\n",
        "    print(12333)\n",
        "\n",
        "    return X_final_df\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    ## X_train_combined <- X_train_prepared\n",
        "    #change components number in case new columns add after preprossing\n",
        "    ##n_components = df_X_train_tfidf.shape[1]      # cols num.   #without dropd y column     ## 23-5-2024 approch 1\n",
        "    #n_components = X_train_prepared.shape[1]      # or any desired number of components\n",
        "    #print('n_components : ', n_components)\n",
        "\n",
        "    # selected_features_df\n",
        "    n_components = 0.97\n",
        "    X_train_selected_features_df = perform_pca_new_1(n_components, 'x_train', X_scaled_df) #X_train_prepared)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    # selected_features_df\n",
        "    #X_train_selected_features_df, X_test_selected_features_df = perform_pca(df_X_train_tfidf, df_X_test_tfidf, n_components=n_components, dataset_name= 'file csv' )\n",
        "    df_X_test_tfidf = []\n",
        "    X_train_final_df = perform_pca(df_X_train_tfidf, df_X_test_tfidf, n_components=n_components, dataset_name= 'file csv' )\n",
        "\n",
        "    columns_witout_target = X_train_selected_features_df.columns\n",
        "\n",
        "    #change components number in case new columns add after preprossing\n",
        "    n_components = X_train_selected_features_df.shape[1] - 1\n",
        "    \"\"\"\n",
        "\n",
        "def perform_pca_new(n_components, x_name, X_scaled ):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ PCA ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    # Fit & Transform PCA on the training data\n",
        "    pca = PCA(n_components=n_components, random_state=453)\n",
        "    print('pca: ' , pca)\n",
        "\n",
        "\n",
        "    if(x_name == 'x_train'):\n",
        "        X_pca = pca.fit_transform(X_scaled)\n",
        "    else:\n",
        "        X_pca = pca.transform(X_scaled)   # Only Transform the test data using the same PCA , to avoid data leakage\n",
        "\n",
        "    print('X_pca: ' , X_pca)\n",
        "\n",
        "\n",
        "    # variance ratio : show how much information (variance) can be attributed to each of the principal components.\n",
        "    # Calculate explained variance ratio and cumulative variance\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    cumulative_variance      = np.cumsum(explained_variance_ratio)    # Calculates the cumulative explained variance ratio for each component.\n",
        "    #print('explained_variance_ratio: ', explained_variance_ratio)\n",
        "    #print('cumulative_variance: ', cumulative_variance)\n",
        "\n",
        "\n",
        "    # Plot component variance and cumulative variance\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    colors_list = ['Red','Orange', 'Blue', 'Purple','Green','Pink','Gray','Tan','Lime','Cyan']\n",
        "\n",
        "    # Plot component variance with percentages\n",
        "    plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio * 100, label='Component Variance')\n",
        "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance * 100, marker='o', color='r', label='Cumulative Variance')\n",
        "\n",
        "    plt.title('Variance Explained by Principal Components\\n Dataset ')\n",
        "    plt.xlabel('Principal Component')\n",
        "    plt.ylabel('Percentage of Variance Explained (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Determine the number of components to keep 97%+ of the variance\n",
        "    number_of_cumulative_variance = 0     # 1   # init the number of components to keep\n",
        "    vsum = 0                                    # to sum variance for each component (sumcum)\n",
        "    for n in explained_variance_ratio:\n",
        "        if vsum < 0.96:\n",
        "            number_of_cumulative_variance += 1\n",
        "        vsum += n\n",
        "\n",
        "    print('number_of_cumulative_variance: ', number_of_cumulative_variance)\n",
        "\n",
        "    # Ensure PCA output is float32\n",
        "    X_pca = X_pca.astype(np.float32)\n",
        "\n",
        "    # Get the components matrix (loadings)\n",
        "    components = pca.components_\n",
        "\n",
        "    # Get the selected feature names (corresponding to the original columns with highest loadings)\n",
        "    X_train_feature_names = X_scaled.columns.tolist()\n",
        "    Xt_selected_features_order = pd.DataFrame(components, columns=X_train_feature_names)\n",
        "    Xt_selected_features_order = Xt_selected_features_order.abs().idxmax(axis=1)\n",
        "    Xt_selected_features_order = Xt_selected_features_order.drop_duplicates()\n",
        "\n",
        "    Xt_selected_features = Xt_selected_features_order[:number_of_cumulative_variance]\n",
        "    print(\"Keep 97%+ of the variance and thus keep \" + str(number_of_cumulative_variance) + \" Principal components from \" + str(n_components) + \".\\n\")\n",
        "\n",
        "    print(f\"{TextStyle.BOLD}X train Selected_features:{TextStyle.RESET_ALL} \\n\")\n",
        "    print(Xt_selected_features[:number_of_cumulative_variance +1])\n",
        "\n",
        "\n",
        "    # Create a DataFrame for PCA results\n",
        "    print('shap X_pca: ', X_pca.shape)\n",
        "    print('shap X_train_feature_names: ', X_train_feature_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Transform the data using selected components\n",
        "    if(x_name == 'x_train'):\n",
        "        #X_pca = pca.fit_transform(X_scaled.toarray())[:, :number_of_cumulative_variance]\n",
        "        X_pca = pca.fit_transform(X_scaled)[:, :number_of_cumulative_variance]\n",
        "    else:\n",
        "        # Only Transform the test data using the same PCA , to avoid data leakage\n",
        "        #X_pca = pca.transform(X_scaled.toarray())[:, :number_of_cumulative_variance]\n",
        "        X_pca = pca.transform(X_scaled)[:, :number_of_cumulative_variance]\n",
        "\n",
        "    print('X_pca 2: ' , X_pca)\n",
        "    print('X_pca 2 shape: ' , X_pca.shape)\n",
        "    print('X_train_feature_names len: ' , len(X_train_feature_names))\n",
        "\n",
        "\n",
        "\n",
        "    # Number of top features to identify\n",
        "    num_top_features = X_pca.shape[1]  #10\n",
        "\n",
        "    # Identify top contributing features for each principal component\n",
        "    top_features_per_component = []\n",
        "    for component in components:\n",
        "        top_feature_indices = np.argsort(np.abs(component))[-num_top_features:]\n",
        "        top_feature_names = [X_train_feature_names[i] for i in top_feature_indices]\n",
        "        top_features_per_component.append(top_feature_names)\n",
        "\n",
        "\n",
        "    # Create a DataFrame to display the top features for each component\n",
        "    # identifying and inspecting the most important original features for each principal component.\n",
        "    top_features_df = pd.DataFrame(top_features_per_component, columns=[f'Top_Feature_{i+1}' for i in range(num_top_features)])\n",
        "    top_features_df.index = [f'Component_{i+1}' for i in range(components.shape[0])]\n",
        "\n",
        "\n",
        "\n",
        "    # Convert PCA results back to DataFrames and concatenate with the original non-sentence columns\n",
        "    # Generate new feature names for PCA components\n",
        "    pca_feature_names = [f'pca_{i+1}' for i in range(X_pca.shape[1])]\n",
        "\n",
        "    # Create a DataFrame with the PCA-transformed data\n",
        "    X_pca_df = pd.DataFrame(data=X_pca, columns=pca_feature_names)\n",
        "\n",
        "    ##X_pca_df = pd.DataFrame(data=X_pca, columns=X_train_feature_names)\n",
        "    print('X_pca_df: ', X_pca_df)\n",
        "    print('X_pca_df shape: ', X_pca_df.shape)\n",
        "\n",
        "    # Combine the PCA-transformed features with the target variable\n",
        "    #/X_final_df = pd.concat([X_pca_df, y.reset_index(drop=True)], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "    # Define selected features (for example, selecting first 10 principal components)\n",
        "    Xt_selected_features = pca_feature_names[:X_pca_df.shape[1]]  # Modify this based on your criteria for feature selection\n",
        "\n",
        "    # Create final DataFrame with selected features\n",
        "    X_final_df = X_pca_df[Xt_selected_features]\n",
        "    X_final_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "\n",
        "    return X_final_df\n",
        "\n",
        "\n",
        "\n",
        "def perform_pca_new_1(n_components, x_name, X_scaled ):     #approch to use\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ PCA ------------{TextStyle.RESET_ALL}\")\n",
        "    #n_components = X_scaled.shape[1]\n",
        "    #n_components = min(X_scaled.shape[0], X_scaled.shape[1])  # ensure we have a valid number of components\n",
        "\n",
        "    no_x = X_scaled.shape[0]\n",
        "    no_y = X_scaled.shape[1]\n",
        "\n",
        "    # if (no_y > no_x) ? no_x : X_scaled.shape[1]\n",
        "    n_components = no_x if no_y > no_x else X_scaled.shape[1]\n",
        "    print('n_components: ' , n_components)\n",
        "\n",
        "\n",
        "    print('X_scaled: ' , X_scaled)\n",
        "\n",
        "    # Fit & Transform PCA on the training data\n",
        "    pca = PCA(n_components=n_components)      # Number of components initially set to all features\n",
        "    print('pca: ' , pca)\n",
        "\n",
        "\n",
        "    if(x_name == 'x_train'):\n",
        "        X_pca = pca.fit_transform(X_scaled)\n",
        "    else:\n",
        "        # Only Transform the test data using the same PCA , to avoid data leakage\n",
        "        X_pca = pca.transform(X_scaled)\n",
        "\n",
        "    print('X_pca: ' , X_pca)\n",
        "\n",
        "\n",
        "    # variance ratio : show how much information (variance) can be attributed to each of the principal components.\n",
        "    # Calculate explained variance ratio and cumulative variance\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    cumulative_variance      = np.cumsum(explained_variance_ratio)    # Calculates the cumulative explained variance ratio for each component.\n",
        "    print('explained_variance_ratio: ', explained_variance_ratio)\n",
        "    print('cumulative_variance: ', cumulative_variance)\n",
        "\n",
        "\n",
        "\n",
        "    # Plot component variance and cumulative variance\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    colors_list = ['Red','Orange', 'Blue', 'Purple','Green','Pink','Gray','Tan','Lime','Cyan']\n",
        "\n",
        "    # Plot component variance with percentages\n",
        "    plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio * 100, label='Component Variance')\n",
        "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance * 100, marker='o', color='r', label='Cumulative Variance')\n",
        "\n",
        "    plt.title('Variance Explained by Principal Components\\n Dataset ')\n",
        "    plt.xlabel('Principal Component')\n",
        "    plt.ylabel('Percentage of Variance Explained (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    # Get the selected feature names (corresponding to the original columns with highest loadings)\n",
        "    # pca.components_ :\n",
        "    X_train_feature_names = X_scaled.columns.tolist()\n",
        "    Xt_selected_features_order = pd.DataFrame(pca.components_, columns=X_train_feature_names)\n",
        "    Xt_selected_features_order = Xt_selected_features_order.abs().idxmax(axis=1)\n",
        "    Xt_selected_features_order = Xt_selected_features_order.drop_duplicates()\n",
        "\n",
        "    Xt_selected_features = Xt_selected_features_order[:cumulative_variance]\n",
        "    print(\"Keep 97%+ of the variance and thus keep \" + str(cumulative_variance) + \" Principal components from \" + str(n_components) + \".\\n\")\n",
        "\n",
        "    print(f\"{TextStyle.BOLD}X train Selected_features:{TextStyle.RESET_ALL} \\n\")\n",
        "    print(Xt_selected_features[:cumulative_variance +1])\n",
        "\n",
        "    # Transform the data using selected components\n",
        "    if(x_name == 'x_train'):\n",
        "        X_pca = pca.fit_transform(X_scaled.toarray())[:, :cumulative_variance]\n",
        "    else:\n",
        "        # Only Transform the test data using the same PCA , to avoid data leakage\n",
        "        X_pca = pca.transform(X_scaled.toarray())[:, :cumulative_variance]\n",
        "\n",
        "    print('X_pca: ' , X_pca)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    # Determine the number of components to keep 97%+ of the variance\n",
        "    number_of_cumulative_variance = 0     # 1   # init the number of components to keep\n",
        "    vsum = 0                                    # to sum variance for each component (sumcum)\n",
        "    for n in explained_variance_ratio:\n",
        "        if vsum < 0.98:\n",
        "            number_of_cumulative_variance += 1\n",
        "        vsum += n\n",
        "\n",
        "    #n_components = X_scaled.shape[1]\n",
        "\n",
        "    print('X_scaled.columns : ' , X_scaled.columns)\n",
        "\n",
        "    # Get the selected feature names (corresponding to the original columns with highest loadings)\n",
        "    X_train_feature_names = X_scaled.columns.tolist()\n",
        "    Xt_selected_features_order = pd.DataFrame(pca.components_, columns=X_train_feature_names)\n",
        "    Xt_selected_features_order = Xt_selected_features_order.abs().idxmax(axis=1)\n",
        "    Xt_selected_features_order = Xt_selected_features_order.drop_duplicates()\n",
        "\n",
        "    Xt_selected_features = Xt_selected_features_order[:number_of_cumulative_variance]\n",
        "    print(\"Keep 97%+ of the variance and thus keep \" + str(number_of_cumulative_variance) + \" Principal components from \" + str(n_components) + \".\\n\")\n",
        "\n",
        "    print(f\"{TextStyle.BOLD}X train Selected_features:{TextStyle.RESET_ALL} \\n\")\n",
        "    print(Xt_selected_features[:number_of_cumulative_variance +1])\n",
        "\n",
        "\n",
        "    # Transform the data using selected components\n",
        "    if(x_name == 'x_train'):\n",
        "        X_pca = pca.fit_transform(X_scaled.toarray())[:, :number_of_cumulative_variance]\n",
        "    else:\n",
        "        # Only Transform the test data using the same PCA , to avoid data leakage\n",
        "        X_pca = pca.transform(X_scaled.toarray())[:, :number_of_cumulative_variance]\n",
        "\n",
        "    print('X_pca: ' , X_pca)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a DataFrame for PCA results\n",
        "    X_pca_df = pd.DataFrame(data=X_pca, columns=X_train_feature_names)\n",
        "    X_final_df = X_pca_df[Xt_selected_features]\n",
        "    X_final_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    return X_final_df\n",
        "\n",
        "\n",
        "\n",
        "def perform_pca_old(df_X_train_tfidf, n_components, dataset_name):     #df_X_test_tfidf,\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}Principal Component Analysis (PCA){TextStyle.RESET_ALL}\")\n",
        "\n",
        "    # Extract feature names\n",
        "    X_train_feature_names = df_X_train_tfidf.columns.tolist()\n",
        "\n",
        "\n",
        "    # Fit & Transform PCA on the training data\n",
        "    pca = PCA(n_components=n_components)      # Number of components initially set to all features\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "    # Only Transform the test data using the same PCA , to avoid data leakage\n",
        "    #X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "    # Check the shapes of the transformed data\n",
        "    print(\"Original training data shape:\", df_X_train_tfidf.shape)\n",
        "    print(\"Transformed training data shape:\", X_train_pca.shape)\n",
        "    #print(\"Original testing data shape:\", df_X_test_tfidf.shape)\n",
        "    #print(\"Transformed testing data shape:\", X_test_pca.shape)\n",
        "\n",
        "\n",
        "    # variance ratio : show how much information (variance) can be attributed to each of the principal components.\n",
        "    # Calculate explained variance ratio and cumulative variance\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    cumulative_variance      = np.cumsum(explained_variance_ratio)    # Calculates the cumulative explained variance ratio for each component.\n",
        "\n",
        "\n",
        "\n",
        "    # Plot component variance and cumulative variance\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot component variance with percentages\n",
        "    plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio * 100, label='Component Variance')\n",
        "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance * 100, marker='o', color='r', label='Cumulative Variance')\n",
        "\n",
        "    plt.title('Variance Explained by Principal Components\\n Dataset: ' + dataset_name)\n",
        "    plt.xlabel('Principal Component')\n",
        "    plt.ylabel('Percentage of Variance Explained (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Determine the number of components to keep 97%+ of the variance\n",
        "    number_of_cumulative_variance = 0     # 1   # init the number of components to keep\n",
        "    vsum = 0                                    # to sum variance for each component (sumcum)\n",
        "    for n in explained_variance_ratio:\n",
        "        if vsum < 0.98:\n",
        "            number_of_cumulative_variance += 1\n",
        "        vsum += n\n",
        "\n",
        "\n",
        "    # Get the selected feature names (corresponding to the original columns with highest loadings)\n",
        "    Xt_selected_features_order = pd.DataFrame(pca.components_, columns=X_train_feature_names)\n",
        "    Xt_selected_features_order = Xt_selected_features_order.abs().idxmax(axis=1)\n",
        "    Xt_selected_features_order = Xt_selected_features_order.drop_duplicates()\n",
        "\n",
        "    Xt_selected_features = Xt_selected_features_order[:number_of_cumulative_variance]\n",
        "    print(\"Keep 97%+ of the variance and thus keep \" + str(number_of_cumulative_variance) + \" Principal components from \" + str(n_components) + \".\\n\")\n",
        "\n",
        "    print(f\"{TextStyle.BOLD}X train Selected_features:{TextStyle.RESET_ALL} \\n\")\n",
        "    print(Xt_selected_features[:number_of_cumulative_variance +1])\n",
        "\n",
        "\n",
        "    # Transform the data using selected components\n",
        "    #X_train_pca = pca.fit_transform(X_train_tfidf.toarray())[:, :number_of_cumulative_variance]\n",
        "    #X_test_pca = pca.transform(X_test_tfidf.toarray())[:, :number_of_cumulative_variance]\n",
        "\n",
        "\n",
        "    \"\"\"  Start net code \"\"\"\n",
        "    \"\"\"# Determine the number of components based on explained variance ratio\n",
        "    total_variance = np.sum(pca.explained_variance_)\n",
        "    explained_variance_ratio = cumulative_variance / total_variance\n",
        "    num_components = np.searchsorted(explained_variance_ratio, explained_variance_ratio) + 1\n",
        "\n",
        "    # Select features based on explained variance ratio threshold\n",
        "    selected_features = pca.components_[:, :num_components]\n",
        "    print(f\"{TextStyle.BOLD}Selected_features:{TextStyle.RESET_ALL} \\n\")\n",
        "\n",
        "    # Transform data using selected components\n",
        "    X_train_reduced = pca.transform(X_train_scaled)[:, :num_components]\n",
        "    print('X_train_reduced : ', X_train_reduced)\n",
        "    X_test_reduced = pca.transform(X_test_scaled)[:, :num_components]\n",
        "    print('X_test_reduced : ', X_test_reduced)\n",
        "    \"\"\"\n",
        "    \"\"\" End net code \"\"\"\n",
        "\n",
        "    # Create a DataFrame for PCA results\n",
        "    X_train_pca_df = pd.DataFrame(data=X_train_pca, columns=X_train_feature_names)\n",
        "    X_train_final_df = X_train_pca_df[Xt_selected_features]\n",
        "    X_train_final_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    #X_test_pca_df = pd.DataFrame(data=X_test_pca, columns=X_train_feature_names)\n",
        "    #X_test_final_df = X_test_pca_df[Xt_selected_features]\n",
        "    #X_test_final_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "\n",
        "    return X_train_final_df  #, X_test_final_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7msjiV9Gil15"
      },
      "source": [
        "## **Run Pre-process function (oll struct pre- in v1)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFBxITVnnikK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_data(df_data, class_Col, x_name, main_text_col = ''):\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Pre-Proccess Data ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "\n",
        "  ## Remove rows with missing target.\n",
        "  #//print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Handling Missing Target ------------{TextStyle.RESET_ALL}\")\n",
        "  #//print(\"Remove rows with missing target, separate target from predictors\")\n",
        "\n",
        "\n",
        "  # ----------- Check for NaN Values ------------- #\n",
        "  print(\"Checking for NaN vallues ...\")\n",
        "  if df_data.isnull().values.any():\n",
        "      print(\"NaN values found in the dataset. Handling missing values...\")\n",
        "      # Handling missing values by imputing with mean (you can choose other methods as well)\n",
        "      df_data = df_data.fillna(df_data.mean())\n",
        "\n",
        "  df_data.replace([np.inf, -np.inf,np.nan,np.NAN],0, inplace=True)\n",
        "\n",
        "  # Check again to ensure there are no NaN or Infinite values after handling\n",
        "  #print(df_data.isnull().sum().sum())  # Should print 0\n",
        "\n",
        "  if df_data.isnull().sum().sum() == 0 : # and np.isinf(df_data.values).sum() == 0 :\n",
        "      print('Done Handling missing values')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # ----------- lowercase + strip 'sentence' ------------- #\n",
        "  # Convert to lowercase & Remove spaces in start and end\n",
        "  df_data['sentence'] = df_data['sentence'].str.lower().str.strip()\n",
        "\n",
        "  # Remove accents (optional)\n",
        "  # df_data['sentence'] = ''.join(x for x in unicodedata.normalize('NFKD', df_data['sentence']) if x in string.ascii_letters or x == \" \")\n",
        "\n",
        "\n",
        "\n",
        "  # ------------------------------------  separate target from predictors ------------------------------------ #\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}separate target from predictors{TextStyle.RESET_ALL}\")\n",
        "\n",
        "  # no. classes/category in class_Col (attack_type)\n",
        "  n_classes = df_data[class_Col].nunique()\n",
        "  print(\"n_classes: \", n_classes)\n",
        "\n",
        "  # main col to classify at end (class_Col --> attack_type)\n",
        "  y = df_data[class_Col]\n",
        "  #y_attack_type = np.unique(y).tolist()   # no need now\n",
        "  print(\"y: \", y)\n",
        "\n",
        "  # remove y column (class_Col) from data columns list (df_data)\n",
        "  df_data.drop([class_Col], axis=1, inplace=True)           # comment prev. 10:00 - 19-5-2024\n",
        "  print(\"df_data: \", df_data)\n",
        "\n",
        "  X = df_data.copy()\n",
        "  X_columns = X.columns       #['sentence', 'len_payload', ....]\n",
        "  print(\"X_columns: \", X_columns)\n",
        "\n",
        "  print(\" X: \", X.shape)\n",
        "  print(\" y: \", y.shape)\n",
        "\n",
        "  #x_sentence_data = X['sentence']\n",
        "  #print('X_columns : ', X['sentence'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # ------------------------------------ Apply encoding for categorical columns if any ------------------------------------ #\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}Handling Categorical Features{TextStyle.RESET_ALL}\")\n",
        "  # all categorical columns (any column with object dataType ant its value not number)\n",
        "  categorical_cols = [cname for cname in X.columns if\n",
        "                      X[cname].dtype == \"object\"]\n",
        "  print(\"Categorical columns len: \", len(categorical_cols))\n",
        "  print(\"Categorical columns: \", categorical_cols)\n",
        "\n",
        "  # Columns that will be one-hot encoded\n",
        "  # X[cname].nunique() < 4   : less that 4 categories in the column\n",
        "  one_hot_cols = [cname for cname in X.columns if\n",
        "                    X[cname].nunique() < 4 and\n",
        "                    X[cname].dtype == \"object\"]\n",
        "  print('one_hot_cols : ', one_hot_cols)\n",
        "  print(\"No. Columns need to encoded using one-hot-encoding: \", len(one_hot_cols))\n",
        "\n",
        "  # Skip this columns because it needs to use TfidFVectorizer With it\n",
        "  skip_cols = [main_text_col]\n",
        "\n",
        "  # Columns that will be target encoded  (will get columns with more than 4 categories in it :: like 'sentence' column)\n",
        "  target_encode_cols = list(set(categorical_cols)-set(one_hot_cols) - set(skip_cols) )\n",
        "  print('target_encode_cols : ', target_encode_cols)\n",
        "  print(\"No. Columns need to encoded using target-encoding: \", len(target_encode_cols))\n",
        "\n",
        "\n",
        "  # Encode categorical columns if there is any\n",
        "  if(len(categorical_cols) > 0):\n",
        "\n",
        "    # to perform one-hot encoding on specified columns of a DataFrame.\n",
        "    if(len(one_hot_cols)> 0):\n",
        "      X = one_hot_encoder_func(X, one_hot_cols)\n",
        "\n",
        "    if(len(target_encode_cols) > 0):\n",
        "      target_column = class_Col\n",
        "      X = target_encode(X, y, target_encode_cols, target_column)\n",
        "\n",
        "  else:\n",
        "    print(\"No categorical columns, nothing todo\")\n",
        "\n",
        "\n",
        "\n",
        "  # ------------------------------------ train_test_split ------------------------------------ #\n",
        "  # Divide the dataset into training (70%) and testing (30%)\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}---------------- Divide the dataset into training (70%) and testing (30%) ----------------{TextStyle.RESET_ALL}\")\n",
        "  #### selected_features_df, y_encoder\n",
        "  selected_features_df = X   #df_data['sentence']  #x_sentence_data\n",
        "  y_encoder = y  #_attack_type\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(selected_features_df, y_encoder, test_size=0.3, random_state=42,stratify=y)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y_encoder, test_size=0.3, random_state=42,stratify=y)\n",
        "  print(f\"X_train shape: {X_train.shape}\")\n",
        "  print(f\"y_train shape: {y_train.shape}\")\n",
        "  print(f\"X_test shape : {X_test.shape}\")\n",
        "  print(f\"y_test shape : {y_test.shape}\")\n",
        "\n",
        "  X_train_copy  = X_train.copy()\n",
        "  X_test_copy   = X_test.copy()\n",
        "  y_train_copy  = y_train.copy()\n",
        "  y_test_copy   = y_test.copy()\n",
        "\n",
        "  print(f\"X_train : {X_train_copy}\")\n",
        "\n",
        "\n",
        "\n",
        "  # ------------------------------------ Tokenization train data ------------------------------------ #\n",
        "  # Tokenize sentences\n",
        "  X_train_tokenized = tokenization_text(X_train_copy['sentence'], 'x_train')\n",
        "  #X_test_tokenized = tokenization_text(X_test_copy['sentence'], 'x_test')\n",
        "\n",
        "  # Join tokens back into strings for each sentence\n",
        "  X_train_joined_sentences = [' '.join(tokens) for tokens in X_train_tokenized]     #X_train_copy['sentence_joined'] = ' '.join(X_train_tokenized)\n",
        "  #X_test_joined_sentences = [' '.join(tokens) for tokens in X_train_tokenized]\n",
        "  #/print('joined_sentences: ' , joined_sentences)\n",
        "\n",
        "\n",
        "\n",
        "  # ------------------------------------ TfidfVectorizer train data ------------------------------------ #\n",
        "  X_train_tfidf, df_X_train_tfidf = Tfidf_Vectorizer(X_train_joined_sentences, X_train_copy['sentence'], 'x_train')\n",
        "  #/print('X_train_tfidf: ' , X_train_tfidf)\n",
        "  #/print('df_X_train_tfidf: ' , df_X_train_tfidf)\n",
        "\n",
        "  print(f\"{TextStyle.BLUE}--- again ----{TextStyle.RESET_ALL}\")\n",
        "  print('X_train_copy shape : ', X_train_copy.shape)\n",
        "  #/print('X_train_tfidf shape : \\n', X_train_tfidf.shape)\n",
        "  print('df_X_train_tfidf shape : ', df_X_train_tfidf.shape)\n",
        "  print('123 : End TfidfVectorizer')\n",
        "\n",
        "\n",
        "  # ------------------------------------ Standardize other numerical columns ------------------------------------ #\n",
        "  # Standardize the Data (mean = 0 and variance = 1)\n",
        "\n",
        "  ### X_train data ###\n",
        "  # Ensure indices are unique\n",
        "  X_train_copy = X_train_copy.reset_index(drop=True)\n",
        "  df_X_train_tfidf = df_X_train_tfidf.reset_index(drop=True) #, inplace=True)\n",
        "\n",
        "  # Concatenate TF-IDF features with the original data excluding the sentence column\n",
        "  X_train_combined = pd.concat([X_train_copy, df_X_train_tfidf], axis=1)    #.drop(['sentence'], axis=1)\n",
        "  #/print('X_train_combined shape : ' , X_train_combined.shape)\n",
        "  #/print('X_train_combined : ' , X_train_combined)\n",
        "\n",
        "  # Apply Scaler\n",
        "  X_train_scaled, X_train_scaled_df = apply_scaler(X_train_combined, 'x_train')\n",
        "  #/print('X_scaled_df shape: ', X_scaled_df.shape)\n",
        "  #/print('X_scaled_df: ', X_scaled_df)\n",
        "\n",
        "  print(f\"{TextStyle.BLUE}--- again ----{TextStyle.RESET_ALL}\")\n",
        "  print('X_train_combined shape : ' , X_train_combined.shape)\n",
        "  print('X_train_scaled_df shape: ', X_train_scaled_df.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # ------------------------------------ PCA for feature selection ------------------------------------ #\n",
        "  \"\"\"\n",
        "  no_x = X_scaled.shape[0]\n",
        "  no_y = X_scaled.shape[1]    #n_components\n",
        "  n_components = no_x if no_y > no_x else X_scaled.shape[1]     # if (no_y > no_x) ? no_x : X_scaled.shape[1]\n",
        "  print('n_components: ' , n_components)\n",
        "  \"\"\"\n",
        "  n_components = 0.95\n",
        "  X_train_final_df = simple_perform_pca_new(n_components, 'x_train', X_train_scaled_df , y_train_copy )\n",
        "  X_final_df = X_train_final_df\n",
        "\n",
        "\n",
        "\n",
        "  return n_classes, X_train_copy, X_test_copy, y_train_copy, y_test_copy, n_components, X_final_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_data_test(X_test, y_test):\n",
        "\n",
        "  # ------------------------------------ Tokenization train data ------------------------------------ #\n",
        "  # Tokenize sentences\n",
        "  X_test_tokenized = tokenization_text(X_test['sentence'], 'x_test')\n",
        "\n",
        "  # Join tokens back into strings for each sentence\n",
        "  X_test_joined_sentences = [' '.join(tokens) for tokens in X_test_tokenized]\n",
        "\n",
        "\n",
        "\n",
        "  # ------------------------------------ TfidfVectorizer train data ------------------------------------ #\n",
        "  X_test_tfidf, df_X_test_tfidf = Tfidf_Vectorizer(X_test_joined_sentences, X_test['sentence'], 'x_test')\n",
        "\n",
        "  # ------------------------------------ Standardize other numerical columns ------------------------------------ #\n",
        "  # Standardize the Data (mean = 0 and variance = 1)\n",
        "  X_test = X_test.reset_index(drop=True)\n",
        "  df_X_test_tfidf = df_X_test_tfidf.reset_index(drop=True) #, inplace=True)\n",
        "\n",
        "  # Concatenate TF-IDF features with the original data excluding the sentence column\n",
        "  X_test_combined = pd.concat([X_test, df_X_test_tfidf], axis=1)\n",
        "\n",
        "  # Apply Scaler\n",
        "  X_test_scaled, X_test_scaled_df = apply_scaler(X_test_combined, 'x_test')\n",
        "\n",
        "\n",
        "\n",
        "  # ------------------------------------ PCA for feature selection ------------------------------------ #\n",
        "  n_components = 0.95\n",
        "  X_test_final_df = simple_perform_pca_new(n_components, 'x_test', X_test_scaled_df , y_test )\n",
        "  X_final_df = X_test_final_df\n",
        "  print('-- End PCA test')\n",
        "\n",
        "\n",
        "  return X_test, y_test, n_components, X_final_df\n",
        "\n",
        "\n",
        "  \"\"\" ---------------- Apply --------------------\n",
        "  # Read Data of CSV Files\n",
        "  dataset_directory = \"/content/datasets\"   #files_path\n",
        "  percent = 100\n",
        "  data = read_csv_files(dataset_directory,percent)\n",
        "  print('len data : ' , len(data))\n",
        "\n",
        "\n",
        "  # Apply pre-processing\n",
        "  main_text_col = 'sentence'\n",
        "  class_Col = 'attack_type'\n",
        "  n_classes, X_train_copy, X_test_copy, y_train_copy, y_test_copy, n_components, X_train_final_df = preprocess_data(data, class_Col, 'csv files', main_text_col)\n",
        "\n",
        "  #n_classes, X_train, X_test, y_train, y_test, X_train_tfidf, \\\n",
        "  #X_test_tfidf, df_X_train_tfidf, df_X_test_tfidf = preprocess_data(data, class_Col, 'csv files', main_text_col)\n",
        "  #n_classes, X_train_copy, X_test_copy, y_train_copy, y_test_copy,  \\\n",
        "  #X_train_tfidf, X_test_tfidf, df_X_train_tfidf, df_X_test_tfidf, \\\n",
        "  #X_train_selected_features_df, X_test_selected_features_df, n_components = preprocess_data(data, class_Col, 'csv files', main_text_col)\n",
        "\n",
        "  \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hVv5dbENMEO"
      },
      "source": [
        "***Train Part Pre-process***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s1tjt_uNU5u"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_part_preprocess():\n",
        "  # ------------------------------------ train_test_split ------------------------------------ #\n",
        "      # Divide the dataset into training (70%) and testing (30%)\n",
        "      print(f\"{TextStyle.BOLD}{TextStyle.BLACK}---------------- Divide the dataset into training (70%) and testing (30%) ----------------{TextStyle.RESET_ALL}\")\n",
        "      #### selected_features_df, y_encoder\n",
        "      selected_features_df = X   #df_data['sentence']  #x_sentence_data\n",
        "      y_encoder = y  #_attack_type\n",
        "      #X_train, X_test, y_train, y_test = train_test_split(selected_features_df, y_encoder, test_size=0.3, random_state=42,stratify=y)\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y_encoder, test_size=0.3, random_state=42,stratify=y)\n",
        "      print(f\"X_train shape: {X_train.shape}\")\n",
        "      print(f\"y_train shape: {y_train.shape}\")\n",
        "      print(f\"X_test shape : {X_test.shape}\")\n",
        "      print(f\"y_test shape : {y_test.shape}\")\n",
        "\n",
        "      X_train_copy  = X_train.copy()\n",
        "      X_test_copy   = X_test.copy()\n",
        "      y_train_copy  = y_train.copy()\n",
        "      y_test_copy   = y_test.copy()\n",
        "\n",
        "      print(f\"X_train : {X_train_copy}\")\n",
        "\n",
        "\n",
        "\n",
        "      # ------------------------------------ Tokenization train data ------------------------------------ #\n",
        "      # Tokenize sentences\n",
        "      X_train_tokenized = tokenization_text(X_train_copy['sentence'], 'x_train')\n",
        "      X_test_tokenized = tokenization_text(X_test_copy['sentence'], 'x_test')\n",
        "\n",
        "      # Join tokens back into strings for each sentence\n",
        "      X_train_joined_sentences = [' '.join(tokens) for tokens in X_train_tokenized]     #X_train_copy['sentence_joined'] = ' '.join(X_train_tokenized)\n",
        "      X_test_joined_sentences = [' '.join(X_train_tokenized)]\n",
        "      #/print('joined_sentences: ' , joined_sentences)\n",
        "\n",
        "\n",
        "\n",
        "      # ------------------------------------ TfidfVectorizer train data ------------------------------------ #\n",
        "      X_train_tfidf, df_X_train_tfidf = Tfidf_Vectorizer(X_train_joined_sentences, X_train_copy['sentence'], 'x_train')\n",
        "      #/print('X_train_tfidf: ' , X_train_tfidf)\n",
        "      #/print('df_X_train_tfidf: ' , df_X_train_tfidf)\n",
        "\n",
        "      print(f\"{TextStyle.BLUE}--- again ----{TextStyle.RESET_ALL}\")\n",
        "      print('X_train_copy shape : ', X_train_copy.shape)\n",
        "      #/print('X_train_tfidf shape : \\n', X_train_tfidf.shape)\n",
        "      print('df_X_train_tfidf shape : ', df_X_train_tfidf.shape)\n",
        "      print('123 : End TfidfVectorizer')\n",
        "\n",
        "\n",
        "      # ------------------------------------ Standardize other numerical columns ------------------------------------ #\n",
        "      # Standardize the Data (mean = 0 and variance = 1)\n",
        "\n",
        "      ### X_train data ###\n",
        "      # Ensure indices are unique\n",
        "      X_train_copy = X_train_copy.reset_index(drop=True)\n",
        "      df_X_train_tfidf = df_X_train_tfidf.reset_index(drop=True) #, inplace=True)\n",
        "\n",
        "      # Concatenate TF-IDF features with the original data excluding the sentence column\n",
        "      X_train_combined = pd.concat([X_train_copy, df_X_train_tfidf], axis=1)    #.drop(['sentence'], axis=1)\n",
        "      #/print('X_train_combined shape : ' , X_train_combined.shape)\n",
        "      #/print('X_train_combined : ' , X_train_combined)\n",
        "\n",
        "      # Apply Scaler\n",
        "      X_train_scaled, X_train_scaled_df = apply_scaler(X_train_combined, 'x_train')\n",
        "      #/print('X_scaled_df shape: ', X_scaled_df.shape)\n",
        "      #/print('X_scaled_df: ', X_scaled_df)\n",
        "\n",
        "      print(f\"{TextStyle.BLUE}--- again ----{TextStyle.RESET_ALL}\")\n",
        "      print('X_train_combined shape : ' , X_train_combined.shape)\n",
        "      print('X_train_scaled_df shape: ', X_train_scaled_df.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # ------------------------------------ PCA for feature selection ------------------------------------ #\n",
        "      \"\"\"\n",
        "      no_x = X_scaled.shape[0]\n",
        "      no_y = X_scaled.shape[1]    #n_components\n",
        "      n_components = no_x if no_y > no_x else X_scaled.shape[1]     # if (no_y > no_x) ? no_x : X_scaled.shape[1]\n",
        "      print('n_components: ' , n_components)\n",
        "      \"\"\"\n",
        "      n_components = 0.95\n",
        "      X_train_final_df = simple_perform_pca_new(n_components, 'x_train', X_train_scaled_df , y_train_copy )\n",
        "      X_final_df = X_train_final_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implement TDCGAN Models**"
      ],
      "metadata": {
        "id": "wMW4T3EmtT2E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23KIY8XptvUZ"
      },
      "source": [
        "## **build_generator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRJt_ebtzQAU"
      },
      "source": [
        "The model of the generator is a deep multi-layer perceptron (MLP) composed of an input layer, output layer and four hidden layers. Initially, the generator takes a point from the latent space to generate new data. The latent space is a multi-dimensional hypersphere normal distributed points, where each variable is drawn from the distribution of the data in the dataset. An embedded layer in the generator creates a vector representation for the generated point.\n",
        "The generator model has four hidden layers. The first one is composed of 256 neurons with a rectified linear unit (ReLU) activation function. An embedded layer is used between hidden layers to efficiently map input data from a high-dimension to lower-dimension space. This allows the neural network to learn the data relationship and process it efficiently. The second hidden layer consists of 128 neurons, the third has 64 neurons and the last one has 32 neurons, with the ReLU activation function used with them all, and a regularization dropout of 20% is added to avoid overfitting. The output layer is activated using the Softmax activation function with 14 neurons as the number of features in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH72rhdfttJX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Creates a generator model for text generation using a deep MLP.\n",
        "\n",
        "Args:\n",
        "    input_dim  : The dimension of the latent space.  (latent_dim)\n",
        "    num_classes: The size of the vocabulary (number of unique words).  ((get it by tokeniz))  (vocabulary_size)\n",
        "    GENERATOR_DROPOUT_RATE : it is 20% ==> 0.2\n",
        "\n",
        "Returns:\n",
        "    A TensorFlow Keras model representing the text generation GAN generator.\n",
        "\"\"\"\n",
        "def build_generator(input_dim, num_classes):   # vocabulary_size / num_class\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ build_generator ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "  GENERATOR_DROPOUT_RATE = 0.2\n",
        "\n",
        "  # Input layers for data (real or generated) and label\n",
        "  # our data is Conditional Generation so label is needed\n",
        "  noise = Input(shape=(input_dim,), name='noise')    # latent space noise (Features , ex: 100 samples, 10 features)\n",
        "  label = Input(shape=(num_classes,), name='label')  # no need for One-hot encoded label, its numeric\n",
        "  print('len noise: ', noise.shape[1])\n",
        "\n",
        "  #concatenated = Concatenate()([data, label])\n",
        "  # Concatenate features and label  (Concatenate along the last dimension)\n",
        "  \"\"\"  Concatenation: A Concatenate layer is used to combine the features and label along the last dimension (axis=-1).\n",
        "      This creates a single input for the subsequent hidden layers in the discriminator model.\"\"\"\n",
        "  concatenated = Concatenate()([noise, label])   #Concatenate(axis=-1) is defualt\n",
        "\n",
        "  # Embedded layer to map noise to lower-dimensional space\n",
        "  # It creates a vector representation for the generated point.\n",
        "  hidden = Embedding(input_dim, num_classes // 2)(concatenated)  # Adjust embedding dim as needed\n",
        "\n",
        "  # Hidden layers with ReLU activation and dropout\n",
        "  hidden = Dense(256, activation='relu')(concatenated)  #(hidden)\n",
        "  hidden = Dropout(GENERATOR_DROPOUT_RATE)(hidden)\n",
        "\n",
        "  hidden = Dense(128, activation='relu')(hidden)\n",
        "  hidden = Dropout(GENERATOR_DROPOUT_RATE)(hidden)\n",
        "\n",
        "  hidden = Dense(64, activation='relu')(hidden)\n",
        "  hidden = Dropout(GENERATOR_DROPOUT_RATE)(hidden)\n",
        "\n",
        "  hidden = Dense(32, activation='relu')(hidden)\n",
        "  hidden = Dropout(GENERATOR_DROPOUT_RATE)(hidden)\n",
        "\n",
        "  # Output layer with Softmax activation\n",
        "  generated_text = Dense(input_dim, activation='softmax', name='generated_text')(hidden)\n",
        "  print('generated_text shape: ', generated_text.shape)\n",
        "\n",
        "  # Create the generator model\n",
        "  generator_model = Model(inputs=[noise, label], outputs=generated_text, name='generator')\n",
        "\n",
        "  generator_model.compile(\n",
        "      loss='binary_crossentropy',\n",
        "      optimizer=Adam(learning_rate= 0.0002 , beta_1= 0.5),\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return generator_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq6EPGPqx6n5"
      },
      "source": [
        "## **build_discriminator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSg03D5g0m5W"
      },
      "source": [
        "In the TDCGAN architecture consists of three discriminators. Each discriminator is a deep neural network with different architecture and different parameter settings. Each discriminator’s role is to extract features from the output of the generator and classify the data with varying levels of accuracy for each them. The model aims to classify data into two groups: normal flows for the background traffic with 0 representation, and anomaly flows for the attack data with 1 representation. The discriminator distinguishes the new data generated by the generator from the true data distribution. It classifies them as either real or fake. Subsequently, the discriminator undergoes updates to improve its ability to distinguish between real and fake samples in the subsequent round. The discriminators try to classify the data into their corresponding class, which is done through a fully connected MLP network.\n",
        "\n",
        "Each discriminator is a MLP model with a different number of hidden layers, different number of neurons and different dropout percentage. The first discriminator is composed of 3 hidden layers with 100 neurons for each and 10% dropout regularization. The second has five hidden layers with 64, 128, 256, 512, and 1024 neurons for each layer, respectively. The dropout percentage is 40%. The last discriminator has 4 hidden layers with 512, 256, 128, and 64 neurons for each layer and 20% dropout percentage.\n",
        "The LeakyReLU(alpha = 0.2) is used as an activation function for the hidden layers in the discriminators. Two output layers are used for each discriminator with the Softmax function as an activation function for one output layer and the Sigmoid activation function for the second output layer.\n",
        "\n",
        "***Why 3 discriminator?***\n",
        "\n",
        "In this model, three discriminators are used and each discriminator haas different architecture. These considered a modified training strategy which helped to face challenge in detection tasks. So, it help the generator to reach its optimal state even when the discriminator converges quickly during the initial stages of training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_3ORAJU0pS9"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "\n",
        "\"\"\"\n",
        "Creates a discriminator model for TDCGAN with specified architecture.\n",
        "\n",
        "Args:\n",
        "    input_dim         : Dimensionality of the data features (based on your data size)\n",
        "    num_classes       : Number of classes in the dataset (maybe - num_attack_classes)\n",
        "    name              : Name for the discriminator model.\n",
        "    num_hidden_layers : Number of hidden layers in the discriminator.\n",
        "    neurons_per_layer : List of integers specifying the number of neurons in each hidden layer.\n",
        "    dropout_rate      : Dropout rate for regularization.\n",
        "\n",
        "Returns:\n",
        "    A TensorFlow Keras model representing the TDCGAN discriminator.\n",
        "\"\"\"\n",
        "\n",
        "def build_discriminator(input_dim, num_classes, name, num_hidden_layers, neurons_per_layer, dropout_rate):\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ build_discriminator (\", name , f\") ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "  # Input layers for data (real or generated) and label\n",
        "  # our data is Conditional Generation so label is needed\n",
        "  data  = Input(shape=(input_dim,), name='data')       # Features    (ex: 100 samples, 10 features)\n",
        "  label = Input(shape=(num_classes,), name='label')   # Tf-IDF Vectorizer not One-hot encoded label\n",
        "  print('len data: ', data.shape[1])\n",
        "  # Concatenate features and label  (Concatenate along the last dimension)\n",
        "  \"\"\"  Concatenation: A Concatenate layer is used to combine the features and label along the last dimension (axis=-1).\n",
        "      This creates a single input for the subsequent hidden layers in the discriminator model.\"\"\"\n",
        "  concatenated = Concatenate()([data, label])   #Concatenate(axis=-1) is defualt\n",
        "\n",
        "  # Hidden layers with LeakyReLU activation and dropout\n",
        "  # These layers extract the features\n",
        "  hidden = data\n",
        "  for i in range(num_hidden_layers):\n",
        "    if i == 0:\n",
        "      #hidden = Dense(neurons_per_layer[i], activation='leaky_relu', alpha=0.2)(concatenated)\n",
        "      hidden = Dense(neurons_per_layer[i])(concatenated)\n",
        "      hidden = LeakyReLU(alpha=0.2)(hidden)\n",
        "      hidden = Dropout(dropout_rate)(hidden)\n",
        "    else:\n",
        "      #hidden = Dense(neurons_per_layer[i], activation='leaky_relu', alpha=0.2)(hidden)\n",
        "      hidden = Dense(neurons_per_layer[i])(hidden)\n",
        "      hidden = LeakyReLU(alpha=0.2)(hidden)\n",
        "      hidden = Dropout(dropout_rate)(hidden)\n",
        "      # Consider adding for training stability (important) but this will make disc summary non-trainable params != 0\n",
        "      #hidden = BatchNormalization()(hidden)\n",
        "\n",
        "  ## Flatten ?!\n",
        "\n",
        "  # Output layer for real vs fake prediction (Real/fake classification)\n",
        "  validity_output = Dense(1, activation='sigmoid', name='validity_output')(hidden)\n",
        "\n",
        "  # Output layer for class label prediction (normal/sqli attack classification)   (Auxiliary classification / class_output)\n",
        "  auxiliary_output = Dense(num_classes, activation='softmax', name='auxiliary_output')(hidden)\n",
        "\n",
        "  print('ttsm - real/fake validity_output : ', validity_output)\n",
        "  print('ttsm - normal/sqli auxiliary_output : ', auxiliary_output)\n",
        "  print('ttsm - real/fake validity_output value : ', K.print_tensor(validity_output, message='K1 = '))\n",
        "  print('ttsm - normal/sqli auxiliary_output value : ', K.print_tensor(auxiliary_output, message='K2 = '))\n",
        "\n",
        "  # Build and compile the discriminator model\n",
        "  discriminator_model = Model(inputs=[data, label], outputs=[auxiliary_output, validity_output], name=name)    # inputs=data\n",
        "\n",
        "  # auxiliary_output (attack_type maybe) may binary_crossentropy  (0 --> normal / 1 --> SQLi)\n",
        "  #'auxiliary_output': 'categorical_crossentropy', // binary_crossentropy\n",
        "  discriminator_model.compile(\n",
        "      loss={'auxiliary_output': 'categorical_crossentropy', 'validity_output': 'binary_crossentropy'},\n",
        "      optimizer=Adam(learning_rate= 0.0002 , beta_1= 0.5),\n",
        "      metrics={'auxiliary_output': 'accuracy', 'validity_output': 'accuracy'})\n",
        "\n",
        "  return discriminator_model\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\" ----------------- Apply Discriminater with req. values ------------------ \"\"\"\n",
        "  # Define discriminator architectures\n",
        "  #discriminator_1 = create_discriminator(2, 'discriminator_1', 3, [100, 100, 100], 0.1)\n",
        "  #discriminator_2 = create_discriminator(2, 'discriminator_2', 5, [64, 128, 256, 512, 1024], 0.4)\n",
        "  #discriminator_3 = create_discriminator(2, 'discriminator_3', 4, [512, 256, 128, 64], 0.2)\n",
        "\n",
        "  \"\"\"\n",
        "  # May used in training as EX\n",
        "  # Example Data (assuming you have 3 classes and input_dim is 10)\n",
        "  real_data = tf.random.normal(shape=(100, 10))  # Sample real data (100 samples, 10 features)\n",
        "  label_one_hot = tf.one_hot(tf.random.uniform(shape=(100,), minval=0, maxval=num_classes, dtype=tf.int32), depth=num_classes)  # One-hot encoded labels (100 samples)\n",
        "  \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBcvbKLOfwbt"
      },
      "source": [
        "## **Election Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WnHaCNj9QC7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "# Election Layer\n",
        "def election_layer(discriminators_outputs):\n",
        "    def majority_vote(logits):\n",
        "        votes = tf.reduce_sum(logits, axis=0)\n",
        "        ### votes = tf.reduce_sum(logits)  #.numpy()      ## try this\n",
        "\n",
        "        # use argmax to finds the index of the maximum value in the summed logits vector.\n",
        "        # This index corresponds to the class with the highest combined vote.\n",
        "        #xx = tf.round(tf.argmax(votes, axis=-1))\n",
        "\n",
        "        return votes\n",
        "\n",
        "    print('election 0')\n",
        "    # Stack the discriminator outputs along the last axis\n",
        "    class_outputs_stack = Lambda(lambda x: tf.stack(x, axis=0))(discriminators_outputs['class'])\n",
        "    validity_outputs_stack = Lambda(lambda x: tf.stack(x, axis=0))(discriminators_outputs['validity'])\n",
        "\n",
        "    # Unpack the class and validity outputs from each discriminator\n",
        "    #class_outputs = [output[0] for output in discriminators_outputs]\n",
        "    #validity_outputs = [output[1] for output in discriminators_outputs]\n",
        "\n",
        "\n",
        "    # Ensure that we do not calculate mean on an empty tensor\n",
        "    if class_outputs_stack.shape[0] == 0:\n",
        "        raise ValueError(\"Class outputs stack is empty.\")\n",
        "    if validity_outputs_stack.shape[0] == 0:\n",
        "        raise ValueError(\"Validity outputs stack is empty.\")\n",
        "\n",
        "\n",
        "    # Perform majority voting\n",
        "    #class_output = Lambda(lambda x: majority_vote(x), name='comb_class_output')(class_outputs_stack)  # election_class_output\n",
        "    #validity_output = Lambda(lambda x: majority_vote(x), name='comb_validity_output')(validity_outputs_stack)  # election_validity_output\n",
        "\n",
        "    print('election 1')\n",
        "    # Perform majority voting by averaging the stacked outputs\n",
        "    class_output = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=0), name='comb_class_output')(class_outputs_stack)   # election_class_output\n",
        "    validity_output = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=0), name='comb_validity_output')(validity_outputs_stack)  # election_validity_output\n",
        "\n",
        "    # Calculate the majority vote for class outputs\n",
        "    #class_output = Lambda(lambda x: majority_vote(x), name='comb_class_output')(class_outputs_stack)\n",
        "\n",
        "    # Calculate the mean of the validity outputs\n",
        "    #validity_output = Lambda(lambda x: tf.reduce_mean(x, axis=0), name='comb_validity_output')(validity_outputs_stack)\n",
        "\n",
        "\n",
        "    # Return the rounded average as the final decision (0 or 1)\n",
        "    #class_output = tf.cast( tf.round(class_output) , tf.int64)\n",
        "    #validity_output = tf.cast( tf.round(validity_output) , tf.int64)\n",
        "    print('election 3')\n",
        "    return class_output, validity_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def election_layer_old(discriminators_outputs):\n",
        "    concatenated_class_outputs = Concatenate()(discriminators_outputs['class'])         # auxiliary_output\n",
        "    concatenated_validity_outputs = Concatenate()(discriminators_outputs['validity'])   # validity_output\n",
        "\n",
        "    class_output = Dense(discriminators_outputs['class'][0].shape[-1], activation='softmax', name='election_class_output')(concatenated_class_outputs)\n",
        "    validity_output = Dense(1, activation='sigmoid', name='election_validity_output')(concatenated_validity_outputs)\n",
        "\n",
        "    return class_output, validity_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0iIjtO-O4NU"
      },
      "source": [
        "***class***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K9eNV78O52A"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class ElectionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ElectionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Inputs will be a list of outputs from the three discriminators\n",
        "        d1_output, d2_output, d3_output = inputs\n",
        "\n",
        "        # Stack the discriminator outputs along the last axis\n",
        "        stacked_outputs = tf.stack([d1_output, d2_output, d3_output], axis=-1)\n",
        "\n",
        "        # Perform majority voting by averaging the stacked outputs\n",
        "        majority_vote = tf.reduce_mean(stacked_outputs, axis=-1)\n",
        "\n",
        "        # Return the rounded average as the final decision (0 or 1)\n",
        "        return tf.round(majority_vote)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0][:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neWHtdZt9K7M"
      },
      "source": [
        "## **create_combined_model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgaPAKHi9Q_G"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_combined_model(input_dim, num_classes, generator_model, discriminators_model):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ build_combined_model ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    print('comb 0')\n",
        "    # functional API model has its input shape, it is suited for more complex models, accepting branches, concatenations, etc.\n",
        "    #noise = Input(shape=(input_dim,))\n",
        "    noise_data  = Input(shape=(input_dim,), name='noise_data')\n",
        "    labels = Input(shape=(num_classes,), name='labels')\n",
        "    print('labels sm : ', labels)\n",
        "\n",
        "    # Generator output features for the discriminator\n",
        "    generated_data = generator_model([noise_data, labels])      # generated_features\n",
        "\n",
        "\n",
        "    print('comb 1')\n",
        "    \"\"\" With loop (num of Disc. (3 Disc)  ) \"\"\"\n",
        "\n",
        "    # Define list for Discriminator outputs for the generated data\n",
        "    discriminators_class_outputs =[]\n",
        "    discriminators_validity_outputs =[]\n",
        "\n",
        "    # another way (not in loop) see -->  ##### Comment 1 #####\n",
        "\n",
        "    for discriminator in discriminators_model:\n",
        "        # Freeze discriminator\n",
        "        # (Freeze Trainable Parameters) -> this make lots params non-trainable\n",
        "        # For the combined model, we only train the generator (disable discriminator training) for all 3 Discr.\n",
        "        discriminator.trainable = False\n",
        "\n",
        "        # Discriminator Output for (class label prediction) and (validity --> real vs fake prediction)\n",
        "        class_outputs , validity_outputs = discriminator([generated_data, labels])\n",
        "        discriminators_class_outputs.append(class_outputs)\n",
        "        discriminators_validity_outputs.append(validity_outputs)\n",
        "\n",
        "        #print('class_outputs ', class_outputs )\n",
        "        #print('shap disc class output (class[0] layers [1]): ', discriminators_outputs['class'][0].get_layer().output )\n",
        "\n",
        "\n",
        "    discriminators_outputs = {\n",
        "                                'class': discriminators_class_outputs,\n",
        "                                'validity': discriminators_validity_outputs\n",
        "                            }\n",
        "\n",
        "\n",
        "    \"\"\" ------- Election Layer ------- \"\"\"\n",
        "    # discriminators_outputs[class][0]  => discriminator 0 , [1] disc 1 , [2] disc 2\n",
        "    print('comb 2')\n",
        "    print('shap disc class output discriminators_outputs[class]: ', discriminators_outputs['class'] )\n",
        "    print('shap disc class output discriminators_outputs[class][0]: ', discriminators_outputs['class'][0] )\n",
        "    print('shap disc class output: ', discriminators_outputs['class'][0].shape[-1] )\n",
        "\n",
        "\n",
        "\n",
        "    print('discriminators_outputs : ', discriminators_outputs)\n",
        "    #class_output, validity_output = election_layer(discriminators_outputs)\n",
        "\n",
        "    #election_layer_class = ElectionLayer()(discriminators_class_outputs)\n",
        "    #election_layer_validity = ElectionLayer()(discriminators_validity_outputs)\n",
        "\n",
        "    \"\"\"\"\"\"\n",
        "    print('comb 3')\n",
        "    # Concatenate discriminators outputs\n",
        "    concatenated_discriminator_class_outputs = Concatenate()(discriminators_class_outputs)\n",
        "    concatenated_discriminator_validity_outputs = Concatenate()(discriminators_validity_outputs)\n",
        "\n",
        "    print('comb 4')\n",
        "    # Output for class label prediction\n",
        "    class_output = Dense(num_classes, activation='softmax', name='comb_class_output')(concatenated_discriminator_class_outputs)\n",
        "\n",
        "    # Output for real vs fake prediction\n",
        "    validity_output = Dense(1, activation='sigmoid', name='comb_validity_output')(concatenated_discriminator_validity_outputs)\n",
        "\n",
        "\n",
        "    # Apply Election Layer to get the final majority vote\n",
        "    #election_class_output = ElectionLayer(name='election_class_output')([class_outputs for class_outputs in discriminators_class_outputs])\n",
        "    #election_validity_output = ElectionLayer(name='election_validity_output')([validity_outputs for validity_outputs in discriminators_validity_outputs])\n",
        "\n",
        "\n",
        "\n",
        "    # Perform the election using tf.reduce_max (can also use other combining strategies)\n",
        "    ##$class_output = tf.reduce_max(tf.stack(discriminators_outputs['class'], axis=-1), axis=-1)\n",
        "    ##$validity_output = tf.reduce_max(tf.stack(discriminators_outputs['validity'], axis=-1), axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print('comb 5')\n",
        "    # Build the combined model\n",
        "    combined_model = Model(inputs=[noise_data, labels],outputs=[class_output,validity_output], name='Combined_Model_TDCGAN')\n",
        "    print('comb 6')\n",
        "\n",
        "    combined_model.compile(\n",
        "        loss={'comb_class_output': 'categorical_crossentropy', 'comb_validity_output': 'binary_crossentropy'},  # categorical_crossentropy\n",
        "        optimizer = Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "        metrics={'comb_class_output': 'accuracy', 'comb_validity_output': 'accuracy'}  #['accuracy']  #\n",
        "    )\n",
        "    print(77)\n",
        "\n",
        "    return combined_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    ##### Comment 1 #####\n",
        "\n",
        "    # --------> Disc 0 <------- #\n",
        "    # Freeze discriminator\n",
        "    # For the combined model, we only train the generator (disable discriminator training) for all 3 Discr.\n",
        "    discriminators_model[0].trainable = False\n",
        "\n",
        "    # create_discriminator(data_dim, num_classes, name, num_hidden_layers, neurons_per_layer, dropout_rate):\n",
        "    class_outputs_0 , validity_outputs_0 = discriminators_model[0]([generated_data, labels])\n",
        "    discriminators_class_outputs.append(class_outputs_0)\n",
        "    discriminators_validity_outputs.append(validity_outputs_0)\n",
        "    print('class_outputs 0 : ', class_outputs_0)\n",
        "    print('validity_outputs 0 : ', validity_outputs_0)\n",
        "\n",
        "\n",
        "    # --------> Disc 1 <------- #\n",
        "    # Freeze discriminator\n",
        "    # For the combined model, we only train the generator (disable discriminator training) for all 3 Discr.\n",
        "    discriminators_model[1].trainable = False\n",
        "\n",
        "    # create_discriminator(data_dim, num_classes, name, num_hidden_layers, neurons_per_layer, dropout_rate):\n",
        "    class_outputs_1 , validity_outputs_1 = discriminators_model[1]([generated_data, labels])\n",
        "    discriminators_class_outputs.append(class_outputs_1)\n",
        "    discriminators_validity_outputs.append(validity_outputs_1)\n",
        "    print('class_outputs 1 : ', class_outputs_1)\n",
        "    print('validity_outputs 1 : ', validity_outputs_1)\n",
        "\n",
        "    # --------> Disc 2 <------- #\n",
        "    # Freeze discriminator\n",
        "    # For the combined model, we only train the generator (disable discriminator training) for all 3 Discr.\n",
        "    discriminators_model[2].trainable = False\n",
        "\n",
        "    # create_discriminator(data_dim, num_classes, name, num_hidden_layers, neurons_per_layer, dropout_rate):\n",
        "    class_outputs_2 , validity_outputs_2 = discriminators_model[2]([generated_data, labels])\n",
        "    discriminators_class_outputs.append(class_outputs_2)\n",
        "    discriminators_validity_outputs.append(validity_outputs_2)\n",
        "    print('class_outputs 2 : ', class_outputs_2)\n",
        "    print('validity_outputs 2 : ', validity_outputs_2)\n",
        "\n",
        "    class_output = class_outputs_2\n",
        "    validity_output = validity_outputs_2\n",
        "\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMtU6unFqkOM"
      },
      "source": [
        "## **print_models_summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioyygMiOqmPU"
      },
      "outputs": [],
      "source": [
        "def print_models_summary(generator_model, combined_model, discriminators_model):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Print Summary ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLUE}\\nGenerator summary (TDCGAN): \\n{TextStyle.RESET_ALL}\")\n",
        "    generator_model.summary(show_trainable=True,expand_nested=True)\n",
        "\n",
        "\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLUE}\\nDiscriminators summary (TDCGAN): \\n{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    i = 1\n",
        "    for discriminator in discriminators_model:\n",
        "        print(f\"{TextStyle.BOLD}\\nDiscriminator \" , i , f\" summary (TDCGAN):{TextStyle.RESET_ALL} \")\n",
        "        discriminator.summary(show_trainable=True,expand_nested=True)\n",
        "        i = i + 1\n",
        "\n",
        "\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLUE}\\nCombined_model summary (TDCGAN):\\n{TextStyle.RESET_ALL} \")\n",
        "    combined_model.summary(show_trainable=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iBIrTvuwMHI"
      },
      "source": [
        "## **plot_architecture (not wok)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7Z5zAmfwOeK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_architecture(generator_model, discriminators_model, combined_model):\n",
        "\n",
        "    # Create a single row of 6 subplots\n",
        "    fig, axes = plt.subplots(6, 1, figsize=(10, 40))\n",
        "    path = '/content/datasets/assets/'\n",
        "\n",
        "    # Plot Generator Architecture\n",
        "    #print(path+'generator.png')\n",
        "    file_name = os.path.join(path, 'generator.png')\n",
        "    plot_model(generator_model, to_file=file_name, show_shapes=True, show_layer_names=True,show_trainable=True,expand_nested=True)\n",
        "    img = plt.imread('generator.png')\n",
        "    axes[0].imshow(img)\n",
        "    axes[0].axis('off')\n",
        "    axes[0].set_title('TDCGAN Generator Architecture')\n",
        "\n",
        "    # Plot Discriminator Architecture\n",
        "\n",
        "    i = 0\n",
        "    axes_idx = 1\n",
        "    for discriminator in discriminators_model:\n",
        "      disc_name = path + 'discriminator_' + i + '.png'\n",
        "      file_name = os.path.join(path, disc_name)\n",
        "      plot_model(discriminator, to_file=file_name, show_shapes=True, show_layer_names=True,show_trainable=True,expand_nested=True)\n",
        "      img = plt.imread(disc_name)\n",
        "      axes[axes_idx].imshow(img)\n",
        "      axes[axes_idx].axis('off')\n",
        "      i = i + 1\n",
        "      axes[axes_idx].set_title('TDCGAN Discriminator ' + i + ' Architecture')\n",
        "      axes_idx = axes_idx + 1\n",
        "\n",
        "    # Plot Combined Architecture\n",
        "    file_name = os.path.join(path, 'combined.png')\n",
        "    plot_model(combined_model, to_file=file_name, show_shapes=True, show_layer_names=True,show_trainable=True,expand_nested=True)\n",
        "    img = plt.imread('combined.png')\n",
        "    axes[4].imshow(img)\n",
        "    axes[4].axis('off')\n",
        "    axes[4].set_title('TDCGAN Combined Architecture')\n",
        "\n",
        "    # Plot Combined Architecture\n",
        "    file_name = os.path.join(path, 'combined2.png')\n",
        "    plot_model(combined_model, to_file=file_name, show_shapes=True, show_layer_names=True,show_trainable=True)\n",
        "    img = plt.imread('combined2.png')\n",
        "    axes[5].imshow(img)\n",
        "    axes[5].axis('off')\n",
        "    axes[5].set_title('TDCGAN Combined 2 Architecture')\n",
        "\n",
        "    #plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf78A8IV2T8g"
      },
      "source": [
        "## **Train**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRgTPUag_Vdx"
      },
      "source": [
        "Through training, the generator learns to map points from the latent space into specific output data, which are different each time the model is trained. Taken a step further, new data are then generated using random points in the latent space. So, these points are used to generate specific data. The discriminator distinguishes the new data generated by the generator from the true data distribution.\n",
        "Here both the generator and discriminator models are trained simultaneously and conditioned on the class label. This conditioning enables the generator model, when utilized independently, to generate minor class data within the domain that corresponds to a specific class label. Subsequently, the discriminator undergoes updates to improve its ability to distinguish between real and fake samples in the subsequent round. Additionally, the generator receives updates based on its success or failure in deceiving the discriminator with its generated samples.\n",
        "To Ensure a more balanced training process between the generator and discriminators by three discriminators with different architectures.\n",
        "In this manner, the two models engage in a competitive relationship, exhibiting adversarial behavior in the context of game theory. In this scenario, the concept of zero-sum implies that when the discriminator effectively distinguishes between real and fake samples, it receives a reward, or no adjustments are made to its model parameters. Simultaneously, the generator is penalized with significant updates to its model parameters. Alternatively, when the generator successfully deceives the discriminator, it receives a reward, or no modifications are made to its model parameters. Whereas, the discriminator is penalized. This is the generic GAN approach.\n",
        "The model is trained for 1000 epochs with a batch size of 128. The optimizer is Adam with a learning rate equal to 0.0001. The proposed model allows the generator to train until it produces a new set of data samples that resembles the real distribution of the original dataset.\n",
        "\n",
        "The discriminators undergo separate training, where each of the model weights are designated as non-trainable within the TDCGAN model. This ensures that solely the weights of the generator model are updated during the training process. This trainability modification specifically applies when training the TDCGAN model, not when training the discriminator independently. So, the TDCGAN model is employed to train the generator’s model weights by utilizing the output and error computed by the discriminator models.\n",
        "\n",
        "The primary objective of the training methodology employed in the GAN framework is for the generator to generate fake data that closely resemble real data, and for the discriminator to acquire sufficient knowledge to differentiate between real and fake samples. Both the generator and discriminator are trained until the discriminator can no longer distinguish real data from fake data. This mean that the generated network can estimate the data sample’s distribution and achieve Nash equilibrium.In order to assess the performance of our model with precision, it is customary to divide the data into training and test sets to produce accurate predictions on unseen data.\n",
        "The training set is utilized for model fitting, while the test set is employed to measure the predictive precision of the trained model. The dataset is split into 70% for training and validation and 30% for testing. The training set is divided into minor class data and other class data. The TDCGAN model uses the minor class to generate data. The generator is trained to model the distribution of the anomaly data (minor class), while fixing the discriminator. The output from the generator is fed as input to the discriminator to predict it.\n",
        "The error is estimated, and the generator’s weight is then updated. The training continues until the discriminator cannot distinguish if the input data come from the generator’s output or from the real anomaly dataset. In the training process, we make sure that all architectures undergo an equal number of epochs and that the weights from the final epoch are selected to generate artificial attack samples.\n",
        "We begin by adhering to this iterative training procedure and ultimately utilize the generator to produce attack samples. Eventually, we incorporate the generated attack samples into the training set. By this, we oversample minor classes in the dataset during the training phase. The test dataset is then used to test the model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtWb7eSFiEis"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# input_dim = latent_dim = noise_dim\n",
        "\n",
        "def train_22(batch_size, noise_dim, num_classes, generator_model, discriminators_model, combined_model,\n",
        "          X_train, y_train, NUM_EPOCHS):\n",
        "#def train_22(X_train, y_train, batch_size, noise_dim, num_classes,\n",
        "#               generator_model, discriminators_model, combined_model):\n",
        "\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ train_22 ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "  print('X_train shape 11 : ', X_train.shape)\n",
        "  print('y_train shape 11 : ', y_train.shape)\n",
        "\n",
        "  #print('x train 1', X_train)\n",
        "  X_train = X_train.to_numpy()\n",
        "  y_train = y_train.to_numpy()\n",
        "\n",
        "  # Save losses for plotting\n",
        "  d0_real_losses = []   # left discriminator losses   (disc 0)\n",
        "  d0_fake_losses = []   # left discriminator losses   (disc 0)\n",
        "  d0_losses      = []   # discriminator losses        (disc 0)\n",
        "\n",
        "  d1_real_losses = []   # Middle discriminator losses (disc 1)\n",
        "  d1_fake_losses = []   # Middle discriminator losses (disc 1)\n",
        "  d1_losses      = []   # discriminator losses        (disc 1)\n",
        "\n",
        "  d2_real_losses = []   # right discriminator losses  (disc 2)\n",
        "  d2_fake_losses = []   # right discriminator losses  (disc 2)\n",
        "  d2_losses      = []   # discriminator losses        (disc 2)\n",
        "\n",
        "  g_losses       = []   # generator losses\n",
        "  d_losses       = []   # discriminator losses\n",
        "\n",
        "\n",
        "  # -----------> Epochs <---------- #\n",
        "  e_idx = 1\n",
        "  for epoch in tqdm(range(NUM_EPOCHS), desc=\"Epoch\", unit=\"epoch\"):\n",
        "      epoch_i = 'Epoch_' + str(e_idx)\n",
        "      #/print(f\"{TextStyle.BOLD}{TextStyle.MAGENTA}------------ \", epoch_i , f\" ------------{TextStyle.RESET_ALL}\")\n",
        "      #print('Epoch: ', NUM_EPOCHS , 'no:' , i )\n",
        "      d_loss_list = []\n",
        "      g_loss_list = []\n",
        "      q_loss_list = []\n",
        "      start = time.time()\n",
        "\n",
        "\n",
        "      idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "      real_data   = X_train[idx]\n",
        "      real_labels = y_train[idx]\n",
        "\n",
        "      #/print('real_data shape: ' , real_data.shape)\n",
        "      #/print('real_labels shape: ' , real_labels.shape)\n",
        "\n",
        "      # Ensure real_labels has the correct shape\n",
        "      one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "      one_hot_labels = one_hot_encoder.fit_transform(np.array(y_train).reshape(-1, 1))\n",
        "      real_labels1 = one_hot_labels[idx]\n",
        "      #/print('real_labels1 shape: ' , real_labels1.shape)\n",
        "\n",
        "\n",
        "      # Sample a random batch of noise the latent space and concatenate the labels.\n",
        "      # NumPy (np) : to generate random noise following a normal (Gaussian) distribution with mean 0 and standard deviation 1.\n",
        "      #noise = tf.random.normal(shape=(batch_size, noise_dim))   # same np contain 0 , 1 but Tensorflow library\n",
        "      noise = np.random.normal(0, 1,size=(batch_size, noise_dim))\n",
        "      #/print('noise shape: ' , noise.shape)\n",
        "\n",
        "\n",
        "      # Generate fake data using the generator\n",
        "      generated_data = generator_model([noise, real_labels1])    # more direct and convenient during training\n",
        "      #generated_data = generator_model.predict([noise, real_labels] ,verbose='0')  # is useful for batch generation or inference tasks\n",
        "\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Discriminator\n",
        "      # ---------------------\n",
        "      # Train the discriminator on real and fake images, separately\n",
        "      # Research showed that separate training is more effective.\n",
        "\n",
        "      # train_on_batch  : is simpler and more straightforward\n",
        "      # tf.GradientTape : (with custom loss) provides greater flexibility for customizing the loss calculation process.\n",
        "\n",
        "      #/print(f\"{TextStyle.BOLD}{TextStyle.RED} *** Train Discriminator *** {TextStyle.RESET_ALL}\")\n",
        "\n",
        "      valid = np.ones((batch_size, 1))  #tf.ones((batch_size, 1), dtype=tf.float32)\n",
        "      fake  = np.zeros((batch_size, 1))  #tf.zeros((batch_size, 1), dtype=tf.float32)\n",
        "\n",
        "\n",
        "      # -------->   Disc 0   <--------- #\n",
        "      disc_loss_real_0 = discriminators_model[0].train_on_batch( [real_data, real_labels1], {'auxiliary_output': real_labels1, 'validity_output': valid })[0]\n",
        "      disc_loss_fake_0 = discriminators_model[0].train_on_batch( [generated_data, real_labels1], {'auxiliary_output': real_labels1, 'validity_output': fake })[0]\n",
        "      d_loss_0 = 0.5 * np.add(disc_loss_real_0, disc_loss_fake_0)   #take average loss from real and fake images.\n",
        "\n",
        "      #print('disc_loss_real_0 : ', disc_loss_real_0)\n",
        "      #print('disc_loss_real_0[0] : ', disc_loss_real_0[0])\n",
        "      #/print('disc_loss_fake_0 : ', disc_loss_fake_0)\n",
        "      #/print('d_loss_0 : ', d_loss_0)\n",
        "\n",
        "\n",
        "      # -------->   Disc 1   <--------- #\n",
        "      disc_loss_real_1 = discriminators_model[1].train_on_batch( [real_data, real_labels1], {'auxiliary_output': real_labels1, 'validity_output': valid })[0]\n",
        "      disc_loss_fake_1 = discriminators_model[1].train_on_batch( [generated_data, real_labels1], {'auxiliary_output': real_labels1, 'validity_output': fake })[0]\n",
        "      d_loss_1 = 0.5 * np.add(disc_loss_real_1, disc_loss_fake_1)   #take average loss from real and fake images.\n",
        "\n",
        "      #/print('disc_loss_real_1 : ', disc_loss_real_1)\n",
        "      #/print('disc_loss_fake_1 : ', disc_loss_fake_1)\n",
        "      #/print('d_loss_1 : ', d_loss_1)\n",
        "\n",
        "\n",
        "      # -------->   Disc 2   <--------- #\n",
        "      disc_loss_real_2 = discriminators_model[2].train_on_batch( [real_data, real_labels1], {'auxiliary_output': real_labels1, 'validity_output': valid })[0]\n",
        "      disc_loss_fake_2 = discriminators_model[2].train_on_batch( [generated_data, real_labels1], {'auxiliary_output': real_labels1, 'validity_output': fake })[0]\n",
        "      d_loss_2 = 0.5 * np.add(disc_loss_real_2, disc_loss_fake_2)   #take average loss from real and fake images.\n",
        "\n",
        "      #/print('disc_loss_real_2 : ', disc_loss_real_2)\n",
        "      #/print('disc_loss_fake_2 : ', disc_loss_fake_2)\n",
        "      #/print('d_loss_2 : ', d_loss_2)\n",
        "\n",
        "\n",
        "      d0_real_losses.append(disc_loss_real_0)\n",
        "      d1_real_losses.append(disc_loss_real_1)\n",
        "      d2_real_losses.append(disc_loss_real_2)\n",
        "\n",
        "      d0_fake_losses.append(disc_loss_fake_0)\n",
        "      d1_fake_losses.append(disc_loss_fake_1)\n",
        "      d2_fake_losses.append(disc_loss_fake_2)\n",
        "\n",
        "      d0_losses.append(d_loss_0)\n",
        "      d1_losses.append(d_loss_1)\n",
        "      d2_losses.append(d_loss_2)\n",
        "      #print('epoch_i ', epoch_i , ' epoch : ', epoch, )\n",
        "      #print('d_loss_0 : ', d_loss_0)\n",
        "      #print('d0_losses : ', d0_losses)\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Generator\n",
        "      # ---------------------\n",
        "\n",
        "      #/print(f\"{TextStyle.BOLD}{TextStyle.RED} *** Train Generator *** {TextStyle.RESET_ALL}\")\n",
        "\n",
        "      # Train the generator by fooling the discriminator\n",
        "      # Sample random points in the latent space.\n",
        "      #noise = np.random.normal(0, 1,size=(batch_size, noise_dim))\n",
        "\n",
        "      # The generator wants the discriminator to label the generated samples , as valid (ones)\n",
        "      # This is where the genrator is trying to trick discriminator into believing\n",
        "      # the generated image is true (hence value of 1 for y)\n",
        "\n",
        "      #Creates an array of all ones of size=batch size\n",
        "      # valid defined above or this valid_y = np.array([1] * batch_size)   [[ Both same]]\n",
        "\n",
        "      # Generator is part of combined where it got directly linked with the discriminator\n",
        "      # Train the generator with noise as x and 1 as y.\n",
        "      # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "      #job of folling the discriminator then the output would be 1 (true)\n",
        "      #g_loss = combined_model.train_on_batch(noise, valid)\n",
        "      g_loss = combined_model.train_on_batch([noise, real_labels1], [real_labels1, valid])\n",
        "      #print('g_loss: ', g_loss[0])\n",
        "      g_losses.append(g_loss[0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #show_samples(3, n_class, g_model)\n",
        "\n",
        "      # Print the loss every few epochs (Print progre)\n",
        "      # can remove it & print epoch all time OR  at finish (epoch % 100)\n",
        "      if epoch % 100 == 0 or epoch == NUM_EPOCHS:\n",
        "          # try this --->  direct g_loss_list not np.mean(g_loss_list)\n",
        "          #print (\n",
        "          tqdm.write(f'{TextStyle.BOLD}{TextStyle.BLUE}Epoch: {epoch}, \\n  \\\n",
        "                    Mean Generator Loss: {np.mean(g_losses)}, \\n \\\n",
        "                    Mean Discriminator Loss: {np.mean([d0_losses, d1_losses, d2_losses])}\\n  \\\n",
        "                    Generator Loss: {g_loss[0]:.3f}, \\n  \\\n",
        "                    Discriminators Loss: [D1 loss: {d_loss_0:.3f} | D2 loss: {d_loss_1:.3f} | D3 loss: {d_loss_2:.3f}  \\\n",
        "                    {TextStyle.RESET_ALL}' ,end=''\n",
        "                )\n",
        "          print (f'Took {time.time()-start} seconds. \\n\\n')\n",
        "\n",
        "      e_idx = e_idx + 1\n",
        "\n",
        "  print('----- End Epoch : -----', )\n",
        "\n",
        "\n",
        "  #print('NUM_EPOCHS ', NUM_EPOCHS)\n",
        "  #print('Generator Loss', len(g_losses), ' - ' , g_losses)\n",
        "  #print('Disc 0 Loss', len(d0_losses), ' - ' , d0_losses)\n",
        "  #print('Disc 1 Loss', len(d1_losses), ' - ' , d1_losses)\n",
        "  #print('Disc 2 Loss', len(d2_losses), ' - ' , d2_losses)\n",
        "\n",
        "\n",
        "\n",
        "  ## ------- Plot TDCGAN Training Losses Curves ------- ##\n",
        "  print('--- plot_losses ---')\n",
        "  # Define labels for each loss\n",
        "  labels      = ['Generator Loss' , 'Disc 0 Loss', 'Disc 1 Loss', 'Disc 2 Loss' ]\n",
        "  colors_list = ['Red'            , 'Orange'     , 'Blue'       , 'Green'       ]\n",
        "  epochs = list(range(NUM_EPOCHS))\n",
        "\n",
        "  plt.figure(num= 'normal_1', figsize=(12, 6))\n",
        "  plt.subplot(2, 1, 1)\n",
        "\n",
        "  plt.plot(epochs, g_losses, color=colors_list[0], label=labels[0])\n",
        "  plt.plot(epochs, d0_losses, color=colors_list[1], label=labels[1])\n",
        "  plt.plot(epochs, d1_losses, color=colors_list[2], label=labels[2])\n",
        "  plt.plot(epochs, d2_losses, color=colors_list[3], label=labels[3])\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('TDCGAN Training Loss Curves')\n",
        "  plt.legend()\n",
        "  #plt.grid(True)\n",
        "  #plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Optional: Save the plot as an image (empty content)\n",
        "  #plt.savefig('tdcgan_losses.png')\n",
        "\n",
        "\n",
        "\n",
        "  ## ------- Plot TDCGAN Training (3 Discriminator) Real-fake Loss Curves ------- ##\n",
        "  print('--- plot_3discs ---')\n",
        "  labels      = ['Disc 0 Real' , 'Disc 0 Fake',     'Disc 1 Real', 'Disc 1 Fake',     'Disc 2 Real', 'Disc 2 Fake']\n",
        "  colors_list = ['Red'         , 'pink'       ,     'Blue'       , 'Purple'     ,     'Green'      ,  'Cyan' ]\n",
        "  #colors_list = ['Red','Orange', 'Blue', 'Purple','Green','Pink','Gray','Tan','Lime','Cyan']\n",
        "  epochs = list(range(NUM_EPOCHS))\n",
        "\n",
        "  #plt.figure()\n",
        "  plt.figure(num= 'normal_21', figsize=(12, 6))\n",
        "  #plt.subplot(2, 1, 2)\n",
        "\n",
        "  plt.plot(epochs, d0_real_losses, color=colors_list[0], label=labels[0])\n",
        "  plt.plot(epochs, d0_fake_losses, color=colors_list[1], label=labels[1])\n",
        "  plt.plot(epochs, d1_real_losses, color=colors_list[2], label=labels[2])\n",
        "  plt.plot(epochs, d1_fake_losses, color=colors_list[3], label=labels[3])\n",
        "  plt.plot(epochs, d2_real_losses, color=colors_list[4], label=labels[4])\n",
        "  plt.plot(epochs, d2_fake_losses, color=colors_list[5], label=labels[5])\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('TDCGAN Training (3 Discriminator) Real-fake Loss Curves')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRFSJcisUH6f"
      },
      "source": [
        "# **Balance Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFyNalH4UYHh"
      },
      "source": [
        "Learning from imbalanced dataset is one of the main challenges that ML facing and it's a common problem. Because collecting balanced dataset cann't be guaranteed in many different fields. For example, in the website and Imbalanced datasets can lead to biased models that perform well on the majority class but poorly on minority classes. This is especially\n",
        "\n",
        "\n",
        "\n",
        "***Note this Why ned to balance datasdt??***\n",
        "\n",
        "application security field that software engineers face, more normal information is usually entered on the website than malicious attacks, which generates unbalanced data. This problem can be solved by balancing the data through reducing the majority classes and increasing the minority classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5tfTwlFBmNG"
      },
      "source": [
        "## ***Decrement rows of Dataset (classes_to_decrement)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoxnJAwIB0ST"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  # Decrement rows for classes of Dataset to reach the goal_rows_per_class\n",
        "    categories    : list of all values in column label that selected\n",
        "    class_label   : from classes_to_inc\n",
        "    class_df      : if equal then get all values in row index (all column not only selected label)\n",
        "\"\"\"\n",
        "def classes_to_decrement(df1, our_classes, categories):\n",
        "  print('===================')\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.RED} ------ Decrement Classes ------ {TextStyle.RESET_ALL}\")\n",
        "\n",
        "  df_after_dec = pd.DataFrame()\n",
        "\n",
        "  for class_label in our_classes:\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLUE}Class_label : \" , class_label , f\"{TextStyle.RESET_ALL}\")\n",
        "    class_df = df1[categories == class_label]\n",
        "    #print('class_df : ', class_df)\n",
        "\n",
        "    n_rows_real_data = len(class_df)        # count number of rows ef each category separatly in loop\n",
        "    size_needed = n_rows_real_data - goal_rows_per_class\n",
        "    print('need to minimum (remove) rows count : ', size_needed)\n",
        "\n",
        "\n",
        "    # Check if its rows greater to less -OR- less to add to it\n",
        "    sampled_df = class_df.sample(n=goal_rows_per_class, random_state=42)\n",
        "    print('sampled_df : ', sampled_df)\n",
        "\n",
        "    \"\"\"\n",
        "    minimize_rows = read_random_n_rows(class_df, goal_rows_per_class, percent)\n",
        "    new_rows_to_add = minimize_rows\n",
        "    print('greater : ', new_rows_to_add)\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenate the duplicated rows to the new DataFrame\n",
        "    df_after_dec = pd.concat([df_after_dec, sampled_df])\n",
        "    #df_after_dec = pd.concat([df_after_dec, sampled_df], ignore_index=True)\n",
        "\n",
        "\n",
        "  return df_after_dec\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKtXuWljB-7X"
      },
      "source": [
        "## ***Increment rows of Dataset (classes_to_increment)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AwvQMALCFdV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  # Duplicate rows for classes with fewer instances than the minimum to reach goal_rows_per_class\n",
        "    categories    : list of all values in column label that selected\n",
        "    class_label   : from classes_to_inc\n",
        "    class_df      : if equal then get all values in row index (all column not only selected label)\n",
        "\"\"\"\n",
        "def classes_to_increment(df1, our_classes, categories):\n",
        "  print('===================')\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.RED} ------ Increment Classes ------ {TextStyle.RESET_ALL}\")\n",
        "\n",
        "  df_after_inc = pd.DataFrame()\n",
        "\n",
        "  for class_label in our_classes:\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLUE}Class_label : \" , class_label , f\"{TextStyle.RESET_ALL}\")\n",
        "    class_df = df1[categories == class_label]\n",
        "    #print('class_df : ', class_df)\n",
        "\n",
        "    n_rows_real_data = len(class_df)        # count number of rows ef each category separatly in loop\n",
        "    size_needed = goal_rows_per_class - n_rows_real_data\n",
        "    print('need to add (duplicate) rows count : ', size_needed)\n",
        "\n",
        "    # add origin exist in data class\n",
        "    df_after_inc = pd.concat([df_after_inc, class_df])\n",
        "\n",
        "    # Duplicate rows using random sampling with replacement (duplicate class_df in num rows where num is param)\n",
        "    duplicate_indices = np.random.choice(class_df.index, size=size_needed, replace=True)\n",
        "    duplicated_rows = class_df.loc[duplicate_indices]\n",
        "    #duplicated_rows = class_df.sample(n=size_needed, replace=True, random_state=42)      # this line make result random but same for each time calling the function\n",
        "    new_rows_to_add = duplicated_rows\n",
        "    print('less : ', new_rows_to_add)\n",
        "\n",
        "\n",
        "    # Concatenate the duplicated rows to the new DataFrame\n",
        "    df_after_inc = pd.concat([df_after_inc, new_rows_to_add])\n",
        "\n",
        "  return df_after_inc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHUv0vDUCnAk"
      },
      "source": [
        "## ***Plot the Data Balanced***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DSLSdktCo54"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\"\n",
        "  This function used to plot our dataset depend on class_Col identify (attack_type)\n",
        "  And shows the count of data in each type\n",
        "  This will shhow 2 plot\n",
        "    1. For Dataset before balanced -->  (r1)\n",
        "    1. For Dataset after balanced -->  (r2)\n",
        "\"\"\"\n",
        "def plot_data_per_class(data_before_balance, data_after_balance, class_Col, goal_rows_per_class):\n",
        "    # set nrows\n",
        "    nrows = goal_rows_per_class\n",
        "\n",
        "    # Create a figure with subplots\n",
        "    # nrows & ncols : creates figure with 4 subplots arranged in 1 row and 4 columns\n",
        "    # ax : array of axes objects which can be indexed to access individual subplots ( ax[0] : first subplot , ax[1] , ax[2], ax[3] other subplots )\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n",
        "\n",
        "    # List of colors to be used for the bars in the plot\n",
        "    colors_list = ['Red', 'Orange', 'Blue', 'Purple', 'Green', 'Pink', 'Gray', 'Tan', 'Lime', 'Cyan']\n",
        "\n",
        "\n",
        "    ## ----------------------> data_before_balance <---------------------- ##\n",
        "\n",
        "    # This creates a bar plot of the value counts of the class_Col column in the data_before_balance DataFrame.\n",
        "    # kind='bar'  :  argument specifies a bar plot\n",
        "    # color       :  argument specifies the colors to use for the bars\n",
        "    # ax=ax[0]    :  argument specifies that this plot should be drawn on the first subplot (first axis in ax)\n",
        "    r1 = data_before_balance[class_Col].value_counts().plot(kind='bar', color=colors_list, ax=ax[0])\n",
        "\n",
        "    # Set the x-axis tick labels\n",
        "    r1.set_xticklabels(r1.get_xticklabels())\n",
        "\n",
        "    # Set the title of the plot\n",
        "    # The title includes the number of rows (nrows) and the shape of the dataset (data_before_balance.shape)\n",
        "    #r1.set_title(str(nrows) + \" of the dataset before balanced\\n\" + str(data_before_balance.shape))\n",
        "    r1.set_title(\"Dataset before balanced\\n\" + str(data_before_balance.shape))\n",
        "\n",
        "    # Rotate x-axis tick labels for  by N degrees better readability\n",
        "    for tick in r1.get_xticklabels():\n",
        "        tick.set_rotation(90)\n",
        "\n",
        "    # Annotate each bar with its height as a percentage of the total data (total number of rows in the dataset)\n",
        "    #  text function                                            :   used to place the computed percentage value above each bar with rotation (vertical), color (dark olive green), and font-size (7)\n",
        "    # rect.get_x() + rect.get_width() / 2                       :   calculates the x-coordinate for placing the text at the center of each bar\n",
        "    # rect.get_height() + 0.75                                  :   calculates the y-coordinate for placing the text just above each bar\n",
        "    # str(round(rect.get_height() / len(data_before_balance) * 100, 4)) + \"%\"  :   computes the height of each bar as a percentage of the total data and converts it to a string\n",
        "    for rect in r1.patches:\n",
        "        r1.text(rect.get_x() + rect.get_width() / 2,\n",
        "                rect.get_height() + 0.75,\n",
        "                \"  \" + str(round(rect.get_height() / len(data_before_balance) * 100, 4)) + \"%\",\n",
        "                rotation='vertical',\n",
        "                color='darkolivegreen',\n",
        "                fontsize=7)\n",
        "\n",
        "\n",
        "    ## ----------------------> data_after_balance <---------------------- ##\n",
        "\n",
        "    r2 = data_after_balance[class_Col].value_counts().plot(kind='bar', color=colors_list, ax=ax[1])\n",
        "\n",
        "    # Set the x-axis tick labels\n",
        "    r2.set_xticklabels(r2.get_xticklabels())\n",
        "\n",
        "    # Set the title of the plot\n",
        "    # The title includes the number of rows (nrows) and the shape of the dataset (data_after_balance.shape)\n",
        "    #r2.set_title(str(nrows) + \" of the dataset after balanced\\n\" + str(data_after_balance.shape))\n",
        "    r2.set_title(\"Dataset after balanced\\n\" + str(data_after_balance.shape))\n",
        "\n",
        "    # Rotate x-axis tick labels for  by N degrees better readability\n",
        "    for tick in r2.get_xticklabels():\n",
        "        tick.set_rotation(90)\n",
        "\n",
        "    # Annotate each bar with its height as a percentage of the total data (total number of rows in the dataset)\n",
        "    #  text function                                            :   used to place the computed percentage value above each bar with rotation (vertical), color (dark olive green), and font-size (7)\n",
        "    # rect.get_x() + rect.get_width() / 2                       :   calculates the x-coordinate for placing the text at the center of each bar\n",
        "    # rect.get_height() + 0.75                                  :   calculates the y-coordinate for placing the text just above each bar\n",
        "    # str(round(rect.get_height() / len(data_after_balance) * 100, 4)) + \"%\"  :   computes the height of each bar as a percentage of the total data and converts it to a string\n",
        "    for rect in r2.patches:\n",
        "        r2.text(rect.get_x() + rect.get_width() / 2,\n",
        "                rect.get_height() + 0.75,\n",
        "                \"  \" + str(round(rect.get_height() / len(data_after_balance) * 100, 4)) + \"%\",\n",
        "                rotation='vertical',\n",
        "                color='darkolivegreen',\n",
        "                fontsize=7)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkmECoyUAppX"
      },
      "source": [
        "## **Balance Data - befor used in TDCGAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ9Le-fVrbz-",
        "outputId": "495707b5-1fc6-4b6f-974f-48e05048ebb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndata = {\\n        \\'sentence\\' : [ \"\\' or 1=1\" , \\'hi soma\\', \\'29%\\', \\'--\\', \\'nice\\', \\'select * from user_data\\'],\\n        \\'attack_type\\' : [1, 0, 1, 1, 0, 1]\\n}\\n\\n\\n# Read Data of CSV Files\\ndataset_directory = \"/content/datasets\"   #files_path\\npercent = 100\\ndata = read_csv_files(dataset_directory,percent)\\nprint(\\'len data : \\' , len(data))\\n\\n\\n# Apply pre-processing\\nmain_text_col = \\'sentence\\'\\nclass_Col = \\'attack_type\\'\\n\\npercent = 100\\nclass_Col = \\'attack_type\\'\\ngoal_rows_per_class = 2000\\ndf_balanced = balance_data_before_tdcgan(data,class_Col,goal_rows_per_class)\\n\\npath=\\'/content/datasets/balanced/\\'\\ndf_balanced.to_csv(path + \\'before.csv\\', index=False)\\n\\nprint(\\'count data before Balance : \\', len(data[\\'sentence\\']))\\nprint(\\'count data after Balance: \\', len(df_balanced[\\'sentence\\']))\\n\\n\\nprint(\\'Count each category after balance :\\', df_balanced[class_Col].value_counts() )\\n\\n\\n#plot data_before_balance (data) & data_after_balance (df_balanced) in each type of class_Col\\ndata_before_balance = pd.DataFrame(data)\\ndata_after_balance = df_balanced\\nplot_data_per_class(data_before_balance, data_after_balance, class_Col, goal_rows_per_class)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "  This function used to balance dataset before using TDCGAN + add instances (rows) to reach min rows in specific class\n",
        "  It can be used to see the diff. res. in TDCGAN before balance and after balance\n",
        "  It works like:\n",
        "      calculate the occurrences of each class and identify classes with fewer instances than the specified minimum (goal_rows_per_class),\n",
        "      then duplicate rows for those classes using random sampling\n",
        "  Args:\n",
        "      data                : our Dataset that need to check if it is balanced or not and add to it if it is not balance\n",
        "      class_Col           : Column name that contain categories (here like attack_type)\n",
        "      goal_rows_per_class : minimum number of instances per class\n",
        "      #balance_type       : useful to use same function but in many time with change specific part. ('normal' -> by defualt balance befor use Model, 'byModel' -> after use Model)\n",
        "  Returns:\n",
        "      List of dataset (df_df_balanced) which length is greater than goal_rows_per_class\n",
        "\"\"\"\n",
        "def balance_data_before_tdcgan(data, class_Col, goal_rows_per_class):   #, balance_type = 'normal'):\n",
        "  df1 = pd.DataFrame(data)\n",
        "  categories = df1[class_Col]\n",
        "\n",
        "  # Count the occurrences of each value in class_Col\n",
        "  # count_per_value = Counter(cat_class)    # need to convert type to handler\n",
        "  class_counts = categories.value_counts()\n",
        "  print('Count # rows (instance) of each category in the class_Col: ' , class_counts)\n",
        "  \"\"\"\n",
        "    ex res:\n",
        "        class_counts :  attack_type\n",
        "        1    3\n",
        "        0    2\n",
        "        Name: count, dtype: int64\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Check if the counts of the unique values are equal\n",
        "  print('Number of unique classes: ' , len(set(class_counts.values)) )\n",
        "  are_counts_equal = len(set(class_counts.values)) == 1\n",
        "\n",
        "  if are_counts_equal:\n",
        "      print(\"The counts of values in class_Col are equal.\")\n",
        "  else:\n",
        "      print(\"The counts of values in class_Col are not equal.\")\n",
        "\n",
        "\n",
        "  # Create an empty DataFrame to store duplicated rows\n",
        "  new_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "  # (to decrement) Identify classes with greater instances than the (goal_rows_per_class) , means class need to get only (goal_rows_per_class) number of rows to reach our (goal_rows_per_class)\n",
        "  classes_to_dec = class_counts[class_counts > goal_rows_per_class].index\n",
        "  #print('classes_to_dec : ' , classes_to_dec)\n",
        "  df_after_dec = classes_to_decrement(df1, classes_to_dec, categories)\n",
        "  new_df = pd.concat([new_df, df_after_dec], ignore_index=True)\n",
        "  #print('new_df 1 : ', new_df)\n",
        "\n",
        "\n",
        "  # (to Increment) Identify classes with fewer instances than the (goal_rows_per_class) , means class need to add to it rows to reach (goal_rows_per_class)\n",
        "  classes_to_inc = class_counts[class_counts < goal_rows_per_class].index\n",
        "  #print('classes_to_inc : ' , classes_to_inc)\n",
        "  df_after_inc = classes_to_increment(df1, classes_to_inc, categories)\n",
        "  new_df = pd.concat([new_df, df_after_inc], ignore_index=True)\n",
        "  #print('new_df 2 : ', new_df)\n",
        "\n",
        "\n",
        "  print('-------========--------========-------')\n",
        "  print('New Data Balanced (new_df): ', new_df)\n",
        "\n",
        "  return new_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\" ----------------- Apply Balance Data Before TDCGAN (Up Class) Function ----------------- \"\"\"\n",
        "  \"\"\"\n",
        "  data = {\n",
        "          'sentence' : [ \"' or 1=1\" , 'hi soma', '29%', '--', 'nice', 'select * from user_data'],\n",
        "          'attack_type' : [1, 0, 1, 1, 0, 1]\n",
        "  }\n",
        "\n",
        "\n",
        "  # Read Data of CSV Files\n",
        "  dataset_directory = \"/content/datasets\"   #files_path\n",
        "  percent = 100\n",
        "  data = read_csv_files(dataset_directory,percent)\n",
        "  print('len data : ' , len(data))\n",
        "\n",
        "\n",
        "  # Apply pre-processing\n",
        "  main_text_col = 'sentence'\n",
        "  class_Col = 'attack_type'\n",
        "\n",
        "  percent = 100\n",
        "  class_Col = 'attack_type'\n",
        "  goal_rows_per_class = 2000\n",
        "  df_balanced = balance_data_before_tdcgan(data,class_Col,goal_rows_per_class)\n",
        "\n",
        "  path='/content/datasets/balanced/'\n",
        "  df_balanced.to_csv(path + 'before.csv', index=False)\n",
        "\n",
        "  print('count data before Balance : ', len(data['sentence']))\n",
        "  print('count data after Balance: ', len(df_balanced['sentence']))\n",
        "\n",
        "\n",
        "  print('Count each category after balance :', df_balanced[class_Col].value_counts() )\n",
        "\n",
        "\n",
        "  #plot data_before_balance (data) & data_after_balance (df_balanced) in each type of class_Col\n",
        "  data_before_balance = pd.DataFrame(data)\n",
        "  data_after_balance = df_balanced\n",
        "  plot_data_per_class(data_before_balance, data_after_balance, class_Col, goal_rows_per_class)\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ytx-CQdwBM"
      },
      "source": [
        "## **Balance Data - Using TDCGAN (need to test it after run model)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5HjrSxHoqEU",
        "outputId": "5bcde6fb-4578-49b9-b4bf-f9be532a12bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' ----------------- Apply generate_new_data Function ----------------- '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "def generate_synthetic_data(synthetic_dect, cols, class_Col, input_dim, generator_model):\n",
        "    # Generate synthetic features and labels\n",
        "    synthetic_features = []\n",
        "    synthetic_labels = []\n",
        "    # Generate random class labels\n",
        "\n",
        "    for label, num_samples in synthetic_dect.items():\n",
        "        # Repeat each unique label based on the specified number of samples\n",
        "        print(\"Generate synthetic \" + str(num_samples) + \" samples for label \" + str(label))\n",
        "        sampled_labels = np.repeat(label, num_samples).reshape(-1,1)\n",
        "        #encoded_labels = one_hot_encoder.transform(sampled_labels)\n",
        "\n",
        "        # Generate random noise for each class\n",
        "        noise = np.random.normal(0, 1, (num_samples, input_dim))\n",
        "\n",
        "        # Generate synthetic features using the generator\n",
        "        #generated_features = generator_model.predict([noise, sampled_labels])\n",
        "        generated_features = generator_model([noise, sampled_labels])\n",
        "\n",
        "        synthetic_features.append(generated_features)\n",
        "        synthetic_labels.append(sampled_labels)\n",
        "\n",
        "    # Concatenate the generated features and labels\n",
        "    synthetic_features = np.concatenate(synthetic_features)\n",
        "    synthetic_labels = np.concatenate(synthetic_labels).reshape(-1, 1)\n",
        "\n",
        "    # Create a DataFrame with synthetic features and labels\n",
        "    synthetic_data = pd.DataFrame(synthetic_features, columns=cols)\n",
        "    synthetic_data[class_Col] = synthetic_labels\n",
        "\n",
        "    return synthetic_data\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  Same part in train TDCGAN before training Discriminator\n",
        "\"\"\"\n",
        "def balance_data_using_tdcgan(X_t, y_t, batch_size, noise_dim):\n",
        "  #X_t = X_t.to_numpy()\n",
        "  #y_t = y_t.to_numpy()\n",
        "\n",
        "  idx = np.random.randint(0, X_t.shape[0], batch_size)\n",
        "  real_data   = X_t[idx]\n",
        "  real_labels = y_t[idx]\n",
        "\n",
        "  #print('real_data: ' , real_data.shape)\n",
        "  #print('real_labels: ' , real_labels.shape)\n",
        "\n",
        "  # Ensure real_labels has the correct shape\n",
        "  one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "  one_hot_labels = one_hot_encoder.fit_transform(np.array(y_t).reshape(-1, 1))\n",
        "  real_labels1 = one_hot_labels[idx]\n",
        "  #print('real_labels1: ' , real_labels1.shape)\n",
        "\n",
        "  # Sample a random batch of noise the latent space and concatenate the labels.\n",
        "  noise = np.random.normal(0, 1,size=(batch_size, noise_dim))\n",
        "  print('noise: ' , noise.shape)\n",
        "\n",
        "  # Generate fake data using the generator\n",
        "  generated_data = generator_model([noise, real_labels1])    # more direct and convenient during training\n",
        "\n",
        "  return generated_data\n",
        "\n",
        "\n",
        "\"\"\" ----------------- Apply generate_new_data Function ----------------- \"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "VWyvfsmldq1K",
        "outputId": "0528ad50-4bcd-47a5-dc0e-b92a54b315d6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "non-default argument follows default argument (<ipython-input-30-d86d576c7bcd>, line 18)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-d86d576c7bcd>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    def balance_data_using_generator(data, class_Col, goal_rows_per_class, balance_type ='normal', X_t='', y_t='', batch_size, noise_dim = input_dim):\u001b[0m\n\u001b[0m                                                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  This function used to balance dataset before using Generator of TDCGAN + add instances (rows) to reach min rows in specific class\n",
        "  It can be used to see the diff. res. in TDCGAN before balance and after balance\n",
        "  It works like:\n",
        "      calculate the occurrences of each class and identify classes with fewer instances than the specified minimum (goal_rows_per_class),\n",
        "      then duplicate rows for those classes using random sampling\n",
        "  Args:\n",
        "      data                : our Dataset that need to check if it is balanced or not and add to it if it is not balance\n",
        "      class_Col           : Column name that contain categories (here like attack_type)\n",
        "      goal_rows_per_class : minimum number of instances per class\n",
        "      #balance_type       : useful to use same function but in many time with change specific part. ('normal' -> by defualt balance befor use Model, 'byModel' -> after use Model)\n",
        "  Returns:\n",
        "      List of dataset (df_df_balanced) which length is greater than goal_rows_per_class\n",
        "\"\"\"\n",
        "def balance_data_using_generator(data, class_Col, goal_rows_per_class, balance_type ='normal', X_t='', y_t='', batch_size, noise_dim = input_dim):\n",
        "  df1 = pd.DataFrame(data)\n",
        "  categories = df1[class_Col]\n",
        "\n",
        "  # Count the occurrences of each value in class_Col\n",
        "  # count_per_value = Counter(cat_class)    # need to convert type to handler\n",
        "  class_counts = categories.value_counts()\n",
        "  #print('Count # rows (instance) of each category in the class_Col: ' , class_counts)\n",
        "  \"\"\"\n",
        "    ex res:\n",
        "        class_counts :  attack_type\n",
        "        1    3\n",
        "        0    2\n",
        "        Name: count, dtype: int64\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Check if the counts of the unique values are equal\n",
        "  print('Number of unique classes: ' , len(set(class_counts.values)) )\n",
        "  are_counts_equal = len(set(class_counts.values)) == 1\n",
        "\n",
        "  if are_counts_equal:\n",
        "      print(\"The counts of values in class_Col are equal.\")\n",
        "      size_to_add = goal_rows_per_class\n",
        "  else:\n",
        "      print(\"The counts of values in class_Col are not equal.\")\n",
        "      size_to_add = []\n",
        "\n",
        "\n",
        "  # Identify classes with fewer instances than the (goal_rows_per_class) , means class need to add to it rows to reach (goal_rows_per_class)\n",
        "  classes_to_duplicate = class_counts[class_counts < goal_rows_per_class].index\n",
        "  #print('classes_to_duplicate : ' , classes_to_duplicate)\n",
        "\n",
        "\n",
        "  # Create an empty DataFrame to store duplicated rows\n",
        "  new_df = ''\n",
        "  new_df = pd.DataFrame(columns=df1.columns)\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  # Duplicate rows for classes with fewer instances than the minimum\n",
        "    categories    : list of all values in column label that selected\n",
        "    class_label   : from classes_to_duplicate\n",
        "    class_df      : if equal then get all values in row index (all column not only selected label)\n",
        "  \"\"\"\n",
        "  for class_label in classes_to_duplicate:\n",
        "    class_df = df1[categories == class_label]\n",
        "    n_rows_real_data = len(class_df)        # count number of rows ef each category separatly in loop\n",
        "    size_needed = goal_rows_per_class - n_rows_real_data\n",
        "    print('need to add (duplicate) rows count : ', size_needed)\n",
        "\n",
        "    if(balance_type == 'normal'):\n",
        "        # Duplicate rows using random sampling with replacement (duplicate class_df in num rows where num is param)\n",
        "        duplicate_indices = np.random.choice(class_df.index, size=size_needed, replace=True)\n",
        "        duplicated_rows = class_df.loc[duplicate_indices]\n",
        "        #duplicated_rows = class_df.sample(n=size_needed, replace=True, random_state=42)      # this line make result random but same for each time calling the function\n",
        "    else:\n",
        "        generate_new_data(X_t, y_t, batch_size, noise_dim)\n",
        "\n",
        "    # Concatenate the duplicated rows to the new DataFrame\n",
        "    new_df = pd.concat([new_df, duplicated_rows])\n",
        "\n",
        "  print('===================')\n",
        "  print('row to duplicate for each category class - new_df: ', new_df)\n",
        "\n",
        "  # Concatenate the original DataFrame with the duplicated rows\n",
        "  df_balanced = pd.concat([df1, new_df], ignore_index=True)\n",
        "  return df_balanced\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" ----------------- Apply Up Class Function ----------------- \"\"\"\n",
        "\n",
        "data = {\n",
        "        'sentence' : [ \"' or 1=1\" , 'hi soma', '29%', '--', 'nice'],\n",
        "        'attack_type' : [1, 0, 1, 1, 0]\n",
        "}\n",
        "\n",
        "\n",
        "class_Col = 'attack_type'\n",
        "goal_rows_per_class = 10\n",
        "df_balanced = balance_data_using_generator(data, class_Col, goal_rows_per_class, 'byModel', X_t, y_t , batch_size, noise_dim):\n",
        "balance_data_befor_tdcgan(data,class_Col,goal_rows_per_class)\n",
        "\n",
        "print('count data before Balance : ', len(data['sentence']))\n",
        "print('count data after Balance: ', len(df_balanced['sentence']))\n",
        "\n",
        "\n",
        "print('Count each category after balance :', df_balanced[class_Col].value_counts() )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" ----------------- Apply Code ----------------- \"\"\"\n",
        "\n",
        "\n",
        "# ---------------------------> Read Data of CSV Files <--------------------------- #\n",
        "dataset_directory = \"/content/datasets\"   #files_path\n",
        "percent = 100\n",
        "data = read_csv_files(dataset_directory,percent)\n",
        "print('len data : ' , len(data))\n",
        "\n",
        "class_Col = 'attack_type'\n",
        "goal_rows_per_class = 50000\n",
        "\n",
        "\n",
        "# ---------------------------> Balance Data Before Start <--------------------------- #\n",
        "df_balanced = balance_data_befor_tdcgan(data, class_Col, goal_rows_per_class)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9uF6qY2HQHP"
      },
      "source": [
        "## **X_Test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZSB662nHTeC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#def tdcgan_train_eval_full_balanced(data, X_test, y_test, encoder, name, balanced_name, class_Col,\n",
        "#                                    num_classes, input_dim, X_train_final_df, X_test_final_df):  #features_dim, latent_dim):\n",
        "\n",
        "def tdcgan_train_eval_full_balanced(X_test, y_test, num_classes, input_dim, generator_model, discriminators_model, combined_model):  #features_dim, latent_dim):\n",
        "\n",
        "  # ---------------------------> Apply pre-processing <--------------------------- #\n",
        "  main_text_col = 'sentence'\n",
        "  #n_classes, X_train_copy, X_test_copy, y_train_copy, y_test_copy, \\\n",
        "  #n_components, X_test_final_df = preprocess_data(data, class_Col, 'x_test', main_text_col)\n",
        "  X_test, y_test, n_components, X_test_final_df = preprocess_data_test(X_test, y_test)\n",
        "\n",
        "\n",
        "  # ---------------------------> Run TDCGAN Models (Gen., Disc., Comb.) <--------------------------- #\n",
        "  #input_dim = features_dim\n",
        "  #/generator_model, discriminators_model, combined_model  = run_TDCGAN_models(input_dim, num_classes)\n",
        "\n",
        "\n",
        "  # ---------------------------> Train Model <--------------------------- #\n",
        "  #batch_size = BATCH_SIZE\n",
        "  #train_22(BATCH_SIZE, input_dim, num_classes, generator_model, discriminators_model, combined_model,\n",
        "  #        X_train_final_df, y_test, NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "  #evaluate_on_real(X_test, y_test, combined_model, X_test_final_df)\n",
        "  performance = evaluate_tdcgan(generator_model, discriminators_model, combined_model, X_test, y_test, input_dim, num_classes)\n",
        "\n",
        "  return performance\n",
        "\n",
        "\n",
        "\n",
        "# For evaluate model by using test data\n",
        "def evaluate_on_real(X_test, y_test, combined_model, X_test_final_df):\n",
        "    #num_samples = len(X_test)\n",
        "    num_samples = len(X_test_final_df)\n",
        "\n",
        "    # One-hot encode labels for evaluation\n",
        "    #one_hot_labels = one_hot_encoder.transform(np.array(y_test).reshape(-1, 1))\n",
        "\n",
        "    # Evaluate the combined model on real data\n",
        "    #y_pred_combined_real = combined_model.predict([X_test, y_test])\n",
        "    y_pred_combined_real = combined_model.predict([X_test_final_df, y_test])\n",
        "    predicted_labels_real = np.argmax(y_pred_combined_real[0], axis=1)\n",
        "    predicted_real_vs_fake_combined = y_pred_combined_real[1]\n",
        "\n",
        "    # Print the classification report for the combined model on real data\n",
        "    print(f\"{TextStyle.BOLD}\\nClassification Report (Combined Model - Real Data - Class Prediction):{TextStyle.RESET_ALL}\")\n",
        "    #print(classification_report(encoder.inverse_transform(y_test), encoder.inverse_transform(predicted_labels_real),zero_division=1))\n",
        "    print(classification_report(y_test, predicted_labels_real, zero_division=1))\n",
        "\n",
        "    print(f\"{TextStyle.BOLD}\\nClassification Report (Combined Model - Real Data - Real vs. Fake Prediction):{TextStyle.RESET_ALL}\")\n",
        "    print(classification_report(np.ones(num_samples), np.round(predicted_real_vs_fake_combined),zero_division=1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#test_data, test_labels,\n",
        "def evaluate_tdcgan(generator_model, discriminators_model, combined_model, X_test, y_test, input_dim, num_classes):\n",
        "\n",
        "    # Generate synthetic data\n",
        "    noise_data = np.random.normal(0, 1, (len(X_test), input_dim))   #Input(shape=(input_dim,), name='noise_data')\n",
        "    #synthetic_data = generator_model([noise_data, y_test])    # generated_data\n",
        "    synthetic_data = generator_model([noise_data, y_test])\n",
        "\n",
        "    \"\"\"\n",
        "    print('e1')\n",
        "    # Prepare real and synthetic data for evaluation\n",
        "    #/real_data_labels = Conca np.concatenate([X_test, y_test], axis=1)\n",
        "    #/synthetic_data_labels = np.concatenate([synthetic_data, y_test], axis=1)\n",
        "    real_data_labels = Concatenate()([X_test, y_test])\n",
        "    synthetic_data_labels = Concatenate()([synthetic_data, y_test])\n",
        "    print('e2')\n",
        "\n",
        "    # Discriminators' predictions on real and synthetic data\n",
        "    real_predictions = [discriminator.predict([X_test, y_test]) for discriminator in discriminators_model]\n",
        "    synthetic_predictions = [discriminator.predict([synthetic_data, y_test]) for discriminator in discriminators_model]\n",
        "    print('e3')\n",
        "\n",
        "    # synthetic_class_output, synthetic_validity_output = combined_model([noise_data, y_test])\n",
        "    # synthetic_class_output, synthetic_validity_output = combined_model([X_test, y_test])\n",
        "\n",
        "\n",
        "    # Majority voting for class predictions\n",
        "    real_class_predictions = [pred[0] for pred in real_predictions]\n",
        "    synthetic_class_predictions = [pred[0] for pred in synthetic_predictions]\n",
        "    print('e4')\n",
        "\n",
        "    def majority_vote(predictions):\n",
        "        stacked_preds = np.stack(predictions, axis=0)\n",
        "        voted_preds = np.argmax(np.sum(stacked_preds, axis=0), axis=1)\n",
        "        return voted_preds\n",
        "\n",
        "    real_voted_predictions = majority_vote(real_class_predictions)\n",
        "    synthetic_voted_predictions = majority_vote(synthetic_class_predictions)\n",
        "    print('e5')\n",
        "\n",
        "\n",
        "    # Calculate metrics\n",
        "    real_accuracy = accuracy_score(y_test, real_voted_predictions)\n",
        "    synthetic_accuracy = accuracy_score(y_test, synthetic_voted_predictions)\n",
        "\n",
        "    real_precision = precision_score(y_test, real_voted_predictions, average='weighted')\n",
        "    synthetic_precision = precision_score(y_test, synthetic_voted_predictions, average='weighted')\n",
        "\n",
        "    real_recall = recall_score(y_test, real_voted_predictions, average='weighted')\n",
        "    synthetic_recall = recall_score(y_test, synthetic_voted_predictions, average='weighted')\n",
        "\n",
        "    real_f1 = f1_score(y_test, real_voted_predictions, average='weighted')\n",
        "    synthetic_f1 = f1_score(y_test, synthetic_voted_predictions, average='weighted')\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Real Data - Accuracy: {real_accuracy}, Precision: {real_precision}, Recall: {real_recall}, F1-score: {real_f1}\")\n",
        "    print(f\"Synthetic Data - Accuracy: {synthetic_accuracy}, Precision: {synthetic_precision}, Recall: {synthetic_recall}, F1-score: {synthetic_f1}\")\n",
        "\n",
        "    return {\n",
        "        'real': {\n",
        "            'accuracy': real_accuracy,\n",
        "            'precision': real_precision,\n",
        "            'recall': real_recall,\n",
        "            'f1': real_f1\n",
        "        },\n",
        "        'synthetic': {\n",
        "            'accuracy': synthetic_accuracy,\n",
        "            'precision': synthetic_precision,\n",
        "            'recall': synthetic_recall,\n",
        "            'f1': synthetic_f1\n",
        "        }\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\"\"\"\n",
        "Update discriminator metrics based on true labels and predicted labels.\n",
        "Parameters:\n",
        "  - y_true: True labels (ground truth).\n",
        "  - y_pred: Predicted labels.\n",
        "Returns:\n",
        "  - metrics_dict: Dictionary containing updated discriminator metrics.\n",
        "\"\"\"\n",
        "def update_discriminator_metrics(y_true, y_pred):\n",
        "\n",
        "    metrics_dict = {}\n",
        "    metrics_dict['accuracy'] = accuracy_score(y_true, y_pred)           # Calculate accuracy\n",
        "    metrics_dict['precision'] = precision_score(y_true, y_pred)         # Calculate precision\n",
        "    metrics_dict['recall'] = recall_score(y_true, y_pred)               # Calculate recall\n",
        "    metrics_dict['f1'] = f1_score(y_true, y_pred)                       # Calculate F1 score\n",
        "\n",
        "    return metrics_dict\n",
        "\n",
        "\n",
        "\n",
        "#tdcgan_train_eval_full_balanced(data, X_test_copy, y_test_copy, encoder, name, \"TDCGAN\", class_Col, n_classes,\n",
        "#                                X_test_copy.shape[1], 100, X_train_final_df, X_test_final_df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pDxPIJgpXtu"
      },
      "source": [
        "# **Apply DL Model (TDCGAN Model)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmCCWEdqDbJX"
      },
      "source": [
        "### **Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYPd9Mi0DfdT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\"\"\"\n",
        "Update discriminator metrics based on true labels and predicted labels.\n",
        "Parameters:\n",
        "  - y_true: True labels (ground truth).\n",
        "  - y_pred: Predicted labels.\n",
        "Returns:\n",
        "  - metrics_dict: Dictionary containing updated discriminator metrics.\n",
        "\"\"\"\n",
        "def update_discriminator_metrics(y_true, y_pred):\n",
        "\n",
        "    metrics_dict = {}\n",
        "    metrics_dict['accuracy'] = accuracy_score(y_true, y_pred)           # Calculate accuracy\n",
        "    metrics_dict['precision'] = precision_score(y_true, y_pred)         # Calculate precision\n",
        "    metrics_dict['recall'] = recall_score(y_true, y_pred)               # Calculate recall\n",
        "    metrics_dict['f1'] = f1_score(y_true, y_pred)                       # Calculate F1 score\n",
        "\n",
        "    return metrics_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFuKtBHualXs"
      },
      "source": [
        "### **Classification ML**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiHWnm4Nak2Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pickle\n",
        "\n",
        "\n",
        "def classify(data,X_test,y_test,encoder,name,class_col,balanc_model):\n",
        "  y_t = data[class_col]\n",
        "  data.drop([class_col], axis=1, inplace=True)\n",
        "  X_t = data.copy()\n",
        "\n",
        "  print(f\"{TextStyle.BOLD}Train and tune models on [\" + name + \" ] dataset balanced with [ \" + balanc_model + f\" ]{TextStyle.RESET_ALL}\")\n",
        "  train_tune_models(X_t,y_t, name )\n",
        "\n",
        "  # Evaluate models\n",
        "  print(f\"{TextStyle.BOLD}\\nEvaluate models on dataset [ \" + name + \" ] Balanced with [ \" + balanc_model + f\" ]{TextStyle.RESET_ALL}\")\n",
        "\n",
        "  # Decision Tree\n",
        "  saved_model_name = name + '_decision_tree_model.pkl'\n",
        "  model = load_model_2(saved_model_name)\n",
        "  evaluate_model(model, encoder, X_test, y_test, 'Decision Tree (' + name + ') ')\n",
        "\n",
        "  # Random Forest\n",
        "  saved_model_name = name + '_random_forest_model.pkl'\n",
        "  model = load_model_2(saved_model_name)\n",
        "  evaluate_model(model, encoder, X_test, y_test, 'Random Forest (' + name + ') ')\n",
        "\n",
        "  # MLP\n",
        "  saved_model_name = name + '_mlp_model.pkl'\n",
        "  model = load_model_2(saved_model_name)\n",
        "  evaluate_model(model, encoder, X_test, y_test, 'MLP (' + name + ') ')\n",
        "\n",
        "  # Naive Bayes\n",
        "  saved_model_name = name + '_naive_bayes_model.pkl'\n",
        "  model = load_model_2(saved_model_name)\n",
        "  evaluate_model(model, encoder, X_test, y_test, 'Naive Bayes (' + name + ') ')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### ----------> evaluate_model <---------- ###\n",
        "def evaluate_model(model,encoder, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "    class_names = encoder.classes_\n",
        "    print(f\"{TextStyle.BOLD}{model_name} Classification Report:{TextStyle.RESET_ALL}\")\n",
        "    print(classification_report(y_test, y_pred,zero_division=1,target_names=class_names))\n",
        "\n",
        "\n",
        "\n",
        "### ----------> save & load model Classified ML <---------- ###\n",
        "def save_model_2(model, balaced_dataset_name , model_name):\n",
        "    # Save the model to a file\n",
        "    with open(balaced_dataset_name + '_' + model_name + '_model.pkl', 'wb') as file:\n",
        "      pickle.dump(model, file)\n",
        "\n",
        "def load_model_2(saved_model_name):\n",
        "    # Load the model from the file\n",
        "    model = ''\n",
        "    with open(saved_model_name, 'rb') as file:\n",
        "        model = pickle.load(file)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "### ----------> train ML <---------- ###\n",
        "def train_tune_models(X_t,y_t,balaced_dataset_name):\n",
        "  # Train and tune models\n",
        "  print('\\nTrain and tune Decision Tree model: ' + balaced_dataset_name)\n",
        "  final_dt_model = tune_and_train_decision_tree(X_t, y_t)\n",
        "  save_model_2(final_dt_model, balaced_dataset_name , 'decision_tree')\n",
        "\n",
        "  print('\\nTrain and tune Random Forest model: ' + balaced_dataset_name)\n",
        "  final_rf_model = tune_and_train_random_forest(X_t, y_t)\n",
        "  save_model_2(final_rf_model, balaced_dataset_name , 'random_forest')\n",
        "\n",
        "  print('\\nTrain and tune MLP model: ' + balaced_dataset_name)\n",
        "  final_mlp_model = tune_and_train_mlp(X_t, y_t)\n",
        "  save_model_2(final_mlp_model, balaced_dataset_name , 'mlp')\n",
        "\n",
        "  print('\\nTrain and tune Naive Bayes model: ' + balaced_dataset_name)\n",
        "  nb_classifier = train_naive_bayes(X_t, y_t)\n",
        "  save_model_2(nb_classifier, balaced_dataset_name , 'naive_bayes')\n",
        "\n",
        "\n",
        "# Decision Tree\n",
        "def tune_and_train_decision_tree(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'criterion': criterion,\n",
        "        'max_depth': max_depth,\n",
        "        'min_samples_split': min_samples_split,\n",
        "        'min_samples_leaf': min_samples_leaf\n",
        "    }\n",
        "    print(\"Training with the following parameters:\")\n",
        "    print(param_grid)\n",
        "    print(\"k_folds = \",k_folds)\n",
        "    dt_classifier = DecisionTreeClassifier()\n",
        "    grid_search = GridSearchCV(dt_classifier, param_grid, cv=k_folds, scoring='accuracy',verbose=2)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_params_dt = grid_search.best_params_\n",
        "    print(f\"{TextStyle.BOLD}Best Parameters for Decision Tree:{TextStyle.RESET_ALL}\", best_params_dt)\n",
        "    print(\"k_folds = \",k_folds)\n",
        "    print(\"\")\n",
        "    final_dt_model = DecisionTreeClassifier(**best_params_dt)\n",
        "    final_dt_model.fit(X_train, y_train)\n",
        "\n",
        "    return final_dt_model\n",
        "\n",
        "# Random Forest\n",
        "def tune_and_train_random_forest(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'n_estimators': n_estimators,\n",
        "        'criterion': criterion,\n",
        "        'max_depth': max_depth,\n",
        "        'min_samples_split': min_samples_split,\n",
        "        'min_samples_leaf': min_samples_leaf\n",
        "    }\n",
        "    print(\"Training with the following parameters:\")\n",
        "    print(param_grid)\n",
        "    print(\"k_folds = \",k_folds)\n",
        "    rf_classifier = RandomForestClassifier(verbose=2)\n",
        "    grid_search = GridSearchCV(rf_classifier, param_grid, cv=k_folds, scoring='accuracy',verbose=2)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_params_rf = grid_search.best_params_\n",
        "    print(f\"{TextStyle.BOLD}Best Parameters for Random Forest:{TextStyle.RESET_ALL}\", best_params_rf)\n",
        "    print(\"k_folds = \",k_folds)\n",
        "    print(\"\")\n",
        "    final_rf_model = RandomForestClassifier(**best_params_rf)\n",
        "    final_rf_model.fit(X_train, y_train)\n",
        "\n",
        "    return final_rf_model\n",
        "\n",
        "# MLP\n",
        "def tune_and_train_mlp(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'hidden_layer_sizes': hidden_layer_sizes,\n",
        "        'activation': activation,\n",
        "        'solver': solver,\n",
        "        'alpha': alpha,\n",
        "        'max_iter': max_iter\n",
        "    }\n",
        "    print(\"Training with the following parameters:\")\n",
        "    print(param_grid)\n",
        "    print(\"k_folds = \",k_folds)\n",
        "    print(\"\")\n",
        "    mlp_classifier = MLPClassifier(verbose=True)\n",
        "    grid_search = GridSearchCV(mlp_classifier, param_grid, cv=k_folds, scoring='accuracy',verbose=2)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_params_mlp = grid_search.best_params_\n",
        "    print(f\"{TextStyle.BOLD}Best Parameters for MLP:{TextStyle.RESET_ALL}\", best_params_mlp)\n",
        "    print(\"k_folds = \",k_folds)\n",
        "    print(\"\")\n",
        "    final_mlp_model = MLPClassifier(**best_params_mlp)\n",
        "    final_mlp_model.fit(X_train, y_train)\n",
        "    return final_mlp_model\n",
        "\n",
        "# NB\n",
        "def train_naive_bayes(X_train, y_train):\n",
        "    nb_classifier = GaussianNB()\n",
        "    nb_classifier.fit(X_train, y_train)\n",
        "    return nb_classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcJgWrpTKCkg"
      },
      "source": [
        "### **Run TDCGAN Models (Gen., Disc., Comb.)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp_iHwX5KZBA"
      },
      "outputs": [],
      "source": [
        "def run_TDCGAN_models(input_dim, num_classes):\n",
        "\n",
        "    # ---------------------------> Build the generator <--------------------------- #\n",
        "    generator_model = build_generator(input_dim, num_classes)   # random_noise_size / input_shape / input_dim\n",
        "    \"\"\"print(f\"{TextStyle.BOLD}{TextStyle.BLUE}generator_model output build{TextStyle.RESET_ALL}\")\n",
        "    print(generator_model)\n",
        "    generator_model.summary()\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------------------------> Build multiple discriminators (3 disc.)  <--------------------------- #\n",
        "    #with Defining different discriminator architectures\n",
        "    #build_discriminator(input_dim, num_classes, name, num_hidden_layers, neurons_per_layer, dropout_rate)\n",
        "    discriminators_model = []\n",
        "    discriminators_model.append(build_discriminator(input_dim, num_classes, 'discriminator_0', 3, [100, 100, 100], 0.1))\n",
        "    discriminators_model.append(build_discriminator(input_dim, num_classes, 'discriminator_1', 5, [64, 128, 256, 512, 1024], 0.4))\n",
        "    discriminators_model.append(build_discriminator(input_dim, num_classes, 'discriminator_2', 4, [512, 256, 128, 64], 0.2))\n",
        "    \"\"\"\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLUE}discriminators_model output build{TextStyle.RESET_ALL}\")\n",
        "    # Iterate over each discriminator model and print its summary\n",
        "    print(discriminators_model)\n",
        "    for discriminator in discriminators_model:\n",
        "        discriminator.summary()\n",
        "    print(discriminators_model)\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------------------------> Build the combined model (GAN Model) <--------------------------- #\n",
        "    combined_model = build_combined_model(input_dim, num_classes, generator_model, discriminators_model)\n",
        "    \"\"\"print(f\"{TextStyle.BOLD}{TextStyle.BLUE}combined_model output build{TextStyle.RESET_ALL}\")\n",
        "    print(combined_model)\n",
        "    combined_model.summary()\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------------------------> print_models_summary + plot_architecture <--------------------------- #\n",
        "    print_models_summary(generator_model, combined_model, discriminators_model)\n",
        "    #plot_architecture(generator_model, discriminators_model, combined_model)       #need to work\n",
        "\n",
        "    return generator_model, discriminators_model, combined_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPkmd_UFwh5"
      },
      "source": [
        "### **Run Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvDhPR5xHl-y"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_on_real1(X_test_copy, y_test_copy, combined_model):\n",
        "\n",
        "\n",
        "    X_test = np.array(X_test_copy)\n",
        "    y_test = np.array(y_test_copy)\n",
        "\n",
        "    # Function to evaluate the model on test data\n",
        "    def evaluate_model(X_test, y_test, combined_model):\n",
        "        # Check the input shapes\n",
        "        print(\"X_test shape:\", X_test.shape)\n",
        "        print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "        # Evaluate the model\n",
        "        results = combined_model.evaluate([X_test, y_test], verbose=1)\n",
        "\n",
        "        # Print the results\n",
        "        print(\"Evaluation Results:\", results)\n",
        "        return results\n",
        "\n",
        "    # Call the evaluation function\n",
        "    results = evaluate_model(X_test, y_test, combined_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print('--99 0 --')\n",
        "    num_samples = len(X_test_copy)\n",
        "    print('--99 1 --')\n",
        "\n",
        "    X_test = X_test_copy.to_numpy()\n",
        "    y_test = y_test_copy.to_numpy()\n",
        "\n",
        "    idx1 = np.random.randint(0, X_test.shape[0], num_samples)\n",
        "    real_data   = X_test[idx1]\n",
        "    real_labels = y_test[idx1]\n",
        "\n",
        "    # Ensure real_labels has the correct shape\n",
        "    # One-hot encode labels for evaluation\n",
        "    one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "    one_hot_labels = one_hot_encoder.fit_transform(np.array(y_test).reshape(-1, 1))\n",
        "    real_labels1 = one_hot_labels[idx1]\n",
        "\n",
        "    # Evaluate the combined model on real data\n",
        "    y_pred_combined_real = combined_model([X_test, real_labels1])\n",
        "    print('--99--')\n",
        "    predicted_labels_real = np.argmax(y_pred_combined_real[0], axis=1)\n",
        "    predicted_real_vs_fake_combined = y_pred_combined_real[1]\n",
        "\n",
        "    print('--88--')\n",
        "    # Print the classification report for the combined model on real data\n",
        "    print(f\"{TextStyle.BOLD}\\nClassification Report (Combined Model - Real Data - Class Prediction):{TextStyle.RESET_ALL}\")\n",
        "    #print(classification_report(encoder.inverse_transform(y_test), encoder.inverse_transform(predicted_labels_real),zero_division=1))\n",
        "    print(classification_report(y_test, predicted_labels_real, zero_division=1))\n",
        "\n",
        "    print(f\"{TextStyle.BOLD}\\nClassification Report (Combined Model - Real Data - Real vs. Fake Prediction):{TextStyle.RESET_ALL}\")\n",
        "    print(classification_report(np.ones(num_samples), np.round(predicted_real_vs_fake_combined),zero_division=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "077c7c42c0474d6d8570ac4cbf2230db",
            "f8dd98fcc4ee426d8e7f548de2497bc2",
            "83c82b0b7760403eb511f5e6dee4d493",
            "60c7ad9804614ce0b1e287ea872e031a",
            "595563bbeba6455cb5bbb7d79c4e15f2",
            "4b2f00016575428ea01d04ca848d6068",
            "a9fe77e4dd474c4f8f4f26fe1b3fe486",
            "da07496c1ed14c199f28ce2efa45b6c6",
            "ea5bdf7654bd437190556eeb628b21da",
            "a3ca3127db4a41cf8d6abc560d2701af",
            "4f043ad058514f16bcd704d44ec457ea"
          ]
        },
        "id": "5s_SYsmIpfPz",
        "outputId": "49f448f7-7988-48ce-d8ba-a6887434b48f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[30m------------ Handling Read CSV Files ------------\u001b[0m\n",
            "Your files are:  ['before_only_4000.csv']\n",
            "File Path:  /content/datasets/before_only_4000.csv\n",
            "Total rows in df/file:  4000\n",
            "Total rows in df/file 100%:  4000\n",
            "                                               sentence  attack_type  \\\n",
            "1197  1 )  as svkf where 6503 = 6503 or char ( 75 ) ...            1   \n",
            "3813                                obolensky@etnics.hr            0   \n",
            "3607                                        tonche meja            0   \n",
            "607   ' )  AND 7767 = CAST ( CHR ( 58 ) ||CHR ( 99 )...            1   \n",
            "1423  1 )  as mnyy where 4409 = 4409 union all selec...            1   \n",
            "...                                                 ...          ...   \n",
            "2159                                               7098            0   \n",
            "3141  SELECT * FROM Product WHERE category = 'Gadgets';            0   \n",
            "2529  INSERT INTO E ( ENO,EName,BossENO )  VALUES  (...            0   \n",
            "509   ' )  UNION ALL SELECT NULL, NULL, NULL, NULL, ...            1   \n",
            "2025                                               foss            0   \n",
            "\n",
            "      len_payload  \n",
            "1197          214  \n",
            "3813           19  \n",
            "3607           11  \n",
            "607           266  \n",
            "1423           69  \n",
            "...           ...  \n",
            "2159            4  \n",
            "3141           49  \n",
            "2529           69  \n",
            "509           130  \n",
            "2025            4  \n",
            "\n",
            "[4000 rows x 3 columns]\n",
            "len data :  4000\n",
            "\u001b[1m\u001b[30m------------ Pre-Proccess Data ------------\u001b[0m\n",
            "Checking for NaN vallues ...\n",
            "Done Handling missing values\n",
            "\u001b[1m\u001b[30mseparate target from predictors\u001b[0m\n",
            "n_classes:  2\n",
            "y:  1197    1\n",
            "3813    0\n",
            "3607    0\n",
            "607     1\n",
            "1423    1\n",
            "       ..\n",
            "2159    0\n",
            "3141    0\n",
            "2529    0\n",
            "509     1\n",
            "2025    0\n",
            "Name: attack_type, Length: 4000, dtype: int64\n",
            "df_data:                                                 sentence  len_payload\n",
            "1197  1 )  as svkf where 6503 = 6503 or char ( 75 ) ...          214\n",
            "3813                                obolensky@etnics.hr           19\n",
            "3607                                        tonche meja           11\n",
            "607   ' )  and 7767 = cast ( chr ( 58 ) ||chr ( 99 )...          266\n",
            "1423  1 )  as mnyy where 4409 = 4409 union all selec...           69\n",
            "...                                                 ...          ...\n",
            "2159                                               7098            4\n",
            "3141  select * from product where category = 'gadgets';           49\n",
            "2529  insert into e ( eno,ename,bosseno )  values  (...           69\n",
            "509   ' )  union all select null, null, null, null, ...          130\n",
            "2025                                               foss            4\n",
            "\n",
            "[4000 rows x 2 columns]\n",
            "X_columns:  Index(['sentence', 'len_payload'], dtype='object')\n",
            " X:  (4000, 2)\n",
            " y:  (4000,)\n",
            "\u001b[1m\u001b[30mHandling Categorical Features\u001b[0m\n",
            "Categorical columns len:  1\n",
            "Categorical columns:  ['sentence']\n",
            "one_hot_cols :  []\n",
            "No. Columns need to encoded using one-hot-encoding:  0\n",
            "target_encode_cols :  []\n",
            "No. Columns need to encoded using target-encoding:  0\n",
            "\u001b[1m\u001b[30m---------------- Divide the dataset into training (70%) and testing (30%) ----------------\u001b[0m\n",
            "X_train shape: (2800, 2)\n",
            "y_train shape: (2800,)\n",
            "X_test shape : (1200, 2)\n",
            "y_test shape : (1200,)\n",
            "X_train :                                                sentence  len_payload\n",
            "2611                                            ratbodo            7\n",
            "808   1'  )  )   and 3754 =  ( select upper ( xmltyp...          351\n",
            "2331                                           estropeo            8\n",
            "1250                   -3637\" or  ( 8459 = 8459 ) *4906           32\n",
            "1036           -7903 where 6699 = 6699 or 8571 = 8571--           40\n",
            "...                                                 ...          ...\n",
            "1879                          iif ( 8770 = 9611,1,1/0 )           25\n",
            "272   and 5650=convert(int,(union all selectchar(88)...           50\n",
            "8     '  )  )   )  union all select null, null, null...          171\n",
            "1734  ' )  union all select null, null, null, null, ...          259\n",
            "458   '  )  )   )  union all select null, null, null...          296\n",
            "\n",
            "[2800 rows x 2 columns]\n",
            "\u001b[1m\u001b[30m------------ Tokenization + stop word ( x_train ) ------------\u001b[0m\n",
            "sentences:  2800\n",
            "tokens len: 2800\n",
            "\u001b[1m\u001b[30m------------ TfidfVectorizer ( x_train ) ------------\u001b[0m\n",
            "Shape of X_tfidf :  (2800, 5387)\n",
            "Training data TF-IDF features: \n",
            "     00  000  00001491528420366  00001491529916387  00001491531111591   01  \\\n",
            "0  0.0  0.0                0.0                0.0                0.0  0.0   \n",
            "1  0.0  0.0                0.0                0.0                0.0  0.0   \n",
            "2  0.0  0.0                0.0                0.0                0.0  0.0   \n",
            "3  0.0  0.0                0.0                0.0                0.0  0.0   \n",
            "4  0.0  0.0                0.0                0.0                0.0  0.0   \n",
            "\n",
            "   010   04   05   06  ...  zul  zumave  zunuzi  zutm   zw  zwgu  zwha  zwov  \\\n",
            "0  0.0  0.0  0.0  0.0  ...  0.0     0.0     0.0   0.0  0.0   0.0   0.0   0.0   \n",
            "1  0.0  0.0  0.0  0.0  ...  0.0     0.0     0.0   0.0  0.0   0.0   0.0   0.0   \n",
            "2  0.0  0.0  0.0  0.0  ...  0.0     0.0     0.0   0.0  0.0   0.0   0.0   0.0   \n",
            "3  0.0  0.0  0.0  0.0  ...  0.0     0.0     0.0   0.0  0.0   0.0   0.0   0.0   \n",
            "4  0.0  0.0  0.0  0.0  ...  0.0     0.0     0.0   0.0  0.0   0.0   0.0   0.0   \n",
            "\n",
            "   zzcd  â¼ckner  \n",
            "0   0.0      0.0  \n",
            "1   0.0      0.0  \n",
            "2   0.0      0.0  \n",
            "3   0.0      0.0  \n",
            "4   0.0      0.0  \n",
            "\n",
            "[5 rows x 5387 columns]\n",
            "\u001b[34m--- again ----\u001b[0m\n",
            "X_train_copy shape :  (2800, 2)\n",
            "df_X_train_tfidf shape :  (2800, 5387)\n",
            "123 : End TfidfVectorizer\n",
            "\u001b[1m\u001b[30m------------ Scaling Features using 'MinMaxScaler' ------------\u001b[0m\n",
            "\u001b[34m--- again ----\u001b[0m\n",
            "X_train_combined shape :  (2800, 5389)\n",
            "X_train_scaled_df shape:  (2800, 5388)\n",
            "\u001b[1m\u001b[30m------------ PCA ------------\u001b[0m\n",
            "pca:  PCA(n_components=0.95, random_state=453)\n",
            "X_pca:  [[-2.40893924e-01 -1.45515014e-01  1.72911846e-02 ...  1.52466130e-16\n",
            "   5.73708259e-17 -6.25869441e-17]\n",
            " [ 3.41680652e-01  8.68970974e-01 -9.83926875e-01 ...  2.48098888e-03\n",
            "  -2.18866469e-03 -2.52463446e-04]\n",
            " [-2.39409944e-01 -1.44468738e-01  1.67637689e-02 ... -9.80046449e-03\n",
            "   1.95712412e-03 -1.89801285e-03]\n",
            " ...\n",
            " [ 8.04373276e-01 -1.73752357e-01  1.15830748e-01 ... -5.39499881e-04\n",
            "   2.34186645e-02 -1.36247139e-02]\n",
            " [ 9.63162339e-01 -1.03117401e-01  7.18160780e-02 ... -6.74102701e-17\n",
            "  -3.67070198e-17  3.15096256e-17]\n",
            " [ 1.02370476e+00 -6.28680010e-02  5.54263810e-02 ... -7.14732754e-17\n",
            "   4.02899692e-17  2.54516460e-17]]\n",
            "456\n",
            "X_pca_df:           pca_1     pca_2     pca_3     pca_4     pca_5     pca_6     pca_7  \\\n",
            "0    -0.240894 -0.145515  0.017291 -0.042642  0.005463 -0.014416 -0.014007   \n",
            "1     0.341681  0.868971 -0.983927 -0.629964 -0.420362  0.195220 -0.597427   \n",
            "2    -0.239410 -0.144469  0.016764 -0.042866  0.005667 -0.014356 -0.013722   \n",
            "3    -0.210761 -0.108855  0.004159 -0.024017 -0.002468 -0.006542 -0.006642   \n",
            "4    -0.211758 -0.047466  0.000643  0.107428 -0.074617  0.036774  0.000909   \n",
            "...        ...       ...       ...       ...       ...       ...       ...   \n",
            "2795 -0.216833 -0.129712  0.007957 -0.048325  0.009570 -0.013907 -0.009423   \n",
            "2796 -0.167435 -0.074352 -0.039908 -0.067203 -0.009397 -0.004526  0.085318   \n",
            "2797  0.804373 -0.173752  0.115831  0.043035 -0.020876 -0.008534 -0.023038   \n",
            "2798  0.963162 -0.103117  0.071816  0.022526 -0.008697  0.002390 -0.018059   \n",
            "2799  1.023705 -0.062868  0.055426  0.011722  0.001173  0.002166  0.001229   \n",
            "\n",
            "         pca_8     pca_9    pca_10  ...      pca_2051      pca_2052  \\\n",
            "0     0.011753  0.010422 -0.017519  ... -2.494731e-17  5.144592e-17   \n",
            "1    -0.047109 -0.159563  0.191796  ... -1.170222e-03 -9.278657e-04   \n",
            "2     0.011488  0.010212 -0.017329  ...  4.758917e-03  1.052899e-03   \n",
            "3    -0.006446 -0.009228 -0.006054  ...  4.003078e-16 -6.038760e-16   \n",
            "4    -0.038628 -0.039634 -0.019573  ... -1.441481e-16  1.205734e-16   \n",
            "...        ...       ...       ...  ...           ...           ...   \n",
            "2795  0.007583  0.007354 -0.015340  ... -1.582935e-17 -5.052382e-17   \n",
            "2796 -0.038596 -0.008311 -0.112111  ...  9.831545e-16 -2.304038e-15   \n",
            "2797  0.018966 -0.013367  0.040315  ... -1.188869e-02  2.685739e-02   \n",
            "2798  0.003748  0.011026 -0.017445  ... -9.385125e-17  3.630383e-18   \n",
            "2799 -0.009831 -0.017567  0.025277  ...  3.236343e-17  1.798082e-17   \n",
            "\n",
            "          pca_2053      pca_2054      pca_2055      pca_2056      pca_2057  \\\n",
            "0    -7.379359e-17 -2.275020e-17 -1.240329e-16  4.507835e-17  2.335246e-17   \n",
            "1    -4.260210e-03 -5.596129e-03 -1.027444e-03  1.400949e-03  2.745722e-03   \n",
            "2    -2.269720e-03 -4.854264e-04  1.115621e-02  1.713659e-03 -1.105328e-04   \n",
            "3     4.418463e-16 -9.285514e-17 -1.708974e-16  1.788493e-16 -2.901156e-16   \n",
            "4     1.937605e-16 -4.617482e-16 -1.609769e-16 -3.033496e-16  3.972923e-16   \n",
            "...            ...           ...           ...           ...           ...   \n",
            "2795  6.722053e-18 -2.537033e-17  1.497283e-16  5.225854e-17 -1.317306e-16   \n",
            "2796  1.460854e-15 -1.532845e-15 -1.351024e-15  5.198749e-16  3.413068e-16   \n",
            "2797 -9.729076e-03  2.274649e-02  2.482392e-02  1.371227e-02 -9.783075e-03   \n",
            "2798 -8.613986e-17 -3.529078e-17  5.101171e-17  1.332891e-17  7.508100e-18   \n",
            "2799 -6.602791e-17 -4.643096e-17  5.044251e-17  5.255670e-17  1.867538e-17   \n",
            "\n",
            "          pca_2058      pca_2059      pca_2060  \n",
            "0     1.524661e-16  5.737083e-17 -6.258694e-17  \n",
            "1     2.480989e-03 -2.188665e-03 -2.524634e-04  \n",
            "2    -9.800464e-03  1.957124e-03 -1.898013e-03  \n",
            "3     2.851994e-16 -3.676631e-17  5.521842e-16  \n",
            "4    -2.107079e-16  2.790431e-16  6.687495e-17  \n",
            "...            ...           ...           ...  \n",
            "2795 -5.312591e-18 -8.136937e-17 -1.808449e-16  \n",
            "2796 -6.917210e-17 -6.027080e-16  7.272828e-16  \n",
            "2797 -5.394999e-04  2.341866e-02 -1.362471e-02  \n",
            "2798 -6.741027e-17 -3.670702e-17  3.150963e-17  \n",
            "2799 -7.147328e-17  4.028997e-17  2.545165e-17  \n",
            "\n",
            "[2800 rows x 2060 columns]\n",
            "X_pca_df shape:  (2800, 2060)\n",
            "789\n",
            "12333\n",
            "\u001b[1m\u001b[30m------------ build_generator ------------\u001b[0m\n",
            "len noise:  2061\n",
            "\u001b[1m\u001b[30m------------ build_discriminator ( discriminator_0 ) ------------\u001b[0m\n",
            "len data:  2061\n",
            "ttsm - real/fake validity_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='validity_output/Sigmoid:0', description=\"created by layer 'validity_output'\")\n",
            "ttsm - normal/sqli auxiliary_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='auxiliary_output/Softmax:0', description=\"created by layer 'auxiliary_output'\")\n",
            "ttsm - real/fake validity_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_186/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_186'\")\n",
            "ttsm - normal/sqli auxiliary_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_187/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_187'\")\n",
            "\u001b[1m\u001b[30m------------ build_discriminator ( discriminator_1 ) ------------\u001b[0m\n",
            "len data:  2061\n",
            "ttsm - real/fake validity_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='validity_output/Sigmoid:0', description=\"created by layer 'validity_output'\")\n",
            "ttsm - normal/sqli auxiliary_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='auxiliary_output/Softmax:0', description=\"created by layer 'auxiliary_output'\")\n",
            "ttsm - real/fake validity_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_188/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_188'\")\n",
            "ttsm - normal/sqli auxiliary_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_189/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_189'\")\n",
            "\u001b[1m\u001b[30m------------ build_discriminator ( discriminator_2 ) ------------\u001b[0m\n",
            "len data:  2061\n",
            "ttsm - real/fake validity_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='validity_output/Sigmoid:0', description=\"created by layer 'validity_output'\")\n",
            "ttsm - normal/sqli auxiliary_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='auxiliary_output/Softmax:0', description=\"created by layer 'auxiliary_output'\")\n",
            "ttsm - real/fake validity_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_190/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_190'\")\n",
            "ttsm - normal/sqli auxiliary_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_191/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_191'\")\n",
            "\u001b[1m\u001b[30m------------ build_combined_model ------------\u001b[0m\n",
            "comb 0\n",
            "labels sm :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name='labels'), name='labels', description=\"created by layer 'labels'\")\n",
            "comb 1\n",
            "comb 2\n",
            "shap disc class output discriminators_outputs[class]:  [<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_0')>, <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_1')>, <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_2')>]\n",
            "shap disc class output discriminators_outputs[class][0]:  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='discriminator_0/auxiliary_output/Softmax:0', description=\"created by layer 'discriminator_0'\")\n",
            "shap disc class output:  2\n",
            "discriminators_outputs :  {'class': [<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_0')>, <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_1')>, <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_2')>], 'validity': [<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'discriminator_0')>, <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'discriminator_1')>, <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'discriminator_2')>]}\n",
            "comb 3\n",
            "comb 4\n",
            "comb 5\n",
            "comb 6\n",
            "77\n",
            "\u001b[1m\u001b[30m------------ Print Summary ------------\u001b[0m\n",
            "\u001b[1m\u001b[34m\n",
            "Generator summary (TDCGAN): \n",
            "\u001b[0m\n",
            "Model: \"generator\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " noise (InputLayer)          [(None, 2061)]               0         []                            Y          \n",
            "                                                                                                             \n",
            " label (InputLayer)          [(None, 2)]                  0         []                            Y          \n",
            "                                                                                                             \n",
            " concatenate_168 (Concatena  (None, 2063)                 0         ['noise[0][0]',               Y          \n",
            " te)                                                                 'label[0][0]']                          \n",
            "                                                                                                             \n",
            " dense_496 (Dense)           (None, 256)                  528384    ['concatenate_168[0][0]']     Y          \n",
            "                                                                                                             \n",
            " dropout_496 (Dropout)       (None, 256)                  0         ['dense_496[0][0]']           Y          \n",
            "                                                                                                             \n",
            " dense_497 (Dense)           (None, 128)                  32896     ['dropout_496[0][0]']         Y          \n",
            "                                                                                                             \n",
            " dropout_497 (Dropout)       (None, 128)                  0         ['dense_497[0][0]']           Y          \n",
            "                                                                                                             \n",
            " dense_498 (Dense)           (None, 64)                   8256      ['dropout_497[0][0]']         Y          \n",
            "                                                                                                             \n",
            " dropout_498 (Dropout)       (None, 64)                   0         ['dense_498[0][0]']           Y          \n",
            "                                                                                                             \n",
            " dense_499 (Dense)           (None, 32)                   2080      ['dropout_498[0][0]']         Y          \n",
            "                                                                                                             \n",
            " dropout_499 (Dropout)       (None, 32)                   0         ['dense_499[0][0]']           Y          \n",
            "                                                                                                             \n",
            " generated_text (Dense)      (None, 2061)                 68013     ['dropout_499[0][0]']         Y          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 639629 (2.44 MB)\n",
            "Trainable params: 639629 (2.44 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_____________________________________________________________________________________________________________\n",
            "\u001b[1m\u001b[34m\n",
            "Discriminators summary (TDCGAN): \n",
            "\u001b[0m\n",
            "\u001b[1m\n",
            "Discriminator  1  summary (TDCGAN):\u001b[0m \n",
            "Model: \"discriminator_0\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " data (InputLayer)           [(None, 2061)]               0         []                            N          \n",
            "                                                                                                             \n",
            " label (InputLayer)          [(None, 2)]                  0         []                            N          \n",
            "                                                                                                             \n",
            " concatenate_169 (Concatena  (None, 2063)                 0         ['data[0][0]',                N          \n",
            " te)                                                                 'label[0][0]']                          \n",
            "                                                                                                             \n",
            " dense_500 (Dense)           (None, 100)                  206400    ['concatenate_169[0][0]']     N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_372 (LeakyReLU  (None, 100)                  0         ['dense_500[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_500 (Dropout)       (None, 100)                  0         ['leaky_re_lu_372[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_501 (Dense)           (None, 100)                  10100     ['dropout_500[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_373 (LeakyReLU  (None, 100)                  0         ['dense_501[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_501 (Dropout)       (None, 100)                  0         ['leaky_re_lu_373[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_502 (Dense)           (None, 100)                  10100     ['dropout_501[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_374 (LeakyReLU  (None, 100)                  0         ['dense_502[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_502 (Dropout)       (None, 100)                  0         ['leaky_re_lu_374[0][0]']     N          \n",
            "                                                                                                             \n",
            " auxiliary_output (Dense)    (None, 2)                    202       ['dropout_502[0][0]']         N          \n",
            "                                                                                                             \n",
            " validity_output (Dense)     (None, 1)                    101       ['dropout_502[0][0]']         N          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 226903 (886.34 KB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 226903 (886.34 KB)\n",
            "_____________________________________________________________________________________________________________\n",
            "\u001b[1m\n",
            "Discriminator  2  summary (TDCGAN):\u001b[0m \n",
            "Model: \"discriminator_1\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " data (InputLayer)           [(None, 2061)]               0         []                            N          \n",
            "                                                                                                             \n",
            " label (InputLayer)          [(None, 2)]                  0         []                            N          \n",
            "                                                                                                             \n",
            " concatenate_170 (Concatena  (None, 2063)                 0         ['data[0][0]',                N          \n",
            " te)                                                                 'label[0][0]']                          \n",
            "                                                                                                             \n",
            " dense_503 (Dense)           (None, 64)                   132096    ['concatenate_170[0][0]']     N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_375 (LeakyReLU  (None, 64)                   0         ['dense_503[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_503 (Dropout)       (None, 64)                   0         ['leaky_re_lu_375[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_504 (Dense)           (None, 128)                  8320      ['dropout_503[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_376 (LeakyReLU  (None, 128)                  0         ['dense_504[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_504 (Dropout)       (None, 128)                  0         ['leaky_re_lu_376[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_505 (Dense)           (None, 256)                  33024     ['dropout_504[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_377 (LeakyReLU  (None, 256)                  0         ['dense_505[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_505 (Dropout)       (None, 256)                  0         ['leaky_re_lu_377[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_506 (Dense)           (None, 512)                  131584    ['dropout_505[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_378 (LeakyReLU  (None, 512)                  0         ['dense_506[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_506 (Dropout)       (None, 512)                  0         ['leaky_re_lu_378[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_507 (Dense)           (None, 1024)                 525312    ['dropout_506[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_379 (LeakyReLU  (None, 1024)                 0         ['dense_507[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_507 (Dropout)       (None, 1024)                 0         ['leaky_re_lu_379[0][0]']     N          \n",
            "                                                                                                             \n",
            " auxiliary_output (Dense)    (None, 2)                    2050      ['dropout_507[0][0]']         N          \n",
            "                                                                                                             \n",
            " validity_output (Dense)     (None, 1)                    1025      ['dropout_507[0][0]']         N          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 833411 (3.18 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 833411 (3.18 MB)\n",
            "_____________________________________________________________________________________________________________\n",
            "\u001b[1m\n",
            "Discriminator  3  summary (TDCGAN):\u001b[0m \n",
            "Model: \"discriminator_2\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " data (InputLayer)           [(None, 2061)]               0         []                            N          \n",
            "                                                                                                             \n",
            " label (InputLayer)          [(None, 2)]                  0         []                            N          \n",
            "                                                                                                             \n",
            " concatenate_171 (Concatena  (None, 2063)                 0         ['data[0][0]',                N          \n",
            " te)                                                                 'label[0][0]']                          \n",
            "                                                                                                             \n",
            " dense_508 (Dense)           (None, 512)                  1056768   ['concatenate_171[0][0]']     N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_380 (LeakyReLU  (None, 512)                  0         ['dense_508[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_508 (Dropout)       (None, 512)                  0         ['leaky_re_lu_380[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_509 (Dense)           (None, 256)                  131328    ['dropout_508[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_381 (LeakyReLU  (None, 256)                  0         ['dense_509[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_509 (Dropout)       (None, 256)                  0         ['leaky_re_lu_381[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_510 (Dense)           (None, 128)                  32896     ['dropout_509[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_382 (LeakyReLU  (None, 128)                  0         ['dense_510[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_510 (Dropout)       (None, 128)                  0         ['leaky_re_lu_382[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_511 (Dense)           (None, 64)                   8256      ['dropout_510[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_383 (LeakyReLU  (None, 64)                   0         ['dense_511[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_511 (Dropout)       (None, 64)                   0         ['leaky_re_lu_383[0][0]']     N          \n",
            "                                                                                                             \n",
            " auxiliary_output (Dense)    (None, 2)                    130       ['dropout_511[0][0]']         N          \n",
            "                                                                                                             \n",
            " validity_output (Dense)     (None, 1)                    65        ['dropout_511[0][0]']         N          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 1229443 (4.69 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 1229443 (4.69 MB)\n",
            "_____________________________________________________________________________________________________________\n",
            "\u001b[1m\u001b[34m\n",
            "Combined_model summary (TDCGAN):\n",
            "\u001b[0m \n",
            "Model: \"Combined_Model_TDCGAN\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " noise_data (InputLayer)     [(None, 2061)]               0         []                            Y          \n",
            "                                                                                                             \n",
            " labels (InputLayer)         [(None, 2)]                  0         []                            Y          \n",
            "                                                                                                             \n",
            " generator (Functional)      (None, 2061)                 639629    ['noise_data[0][0]',          Y          \n",
            "                                                                     'labels[0][0]']                         \n",
            "                                                                                                             \n",
            " discriminator_0 (Functiona  [(None, 2),                  226903    ['generator[0][0]',           N          \n",
            " l)                           (None, 1)]                             'labels[0][0]']                         \n",
            "                                                                                                             \n",
            " discriminator_1 (Functiona  [(None, 2),                  833411    ['generator[0][0]',           N          \n",
            " l)                           (None, 1)]                             'labels[0][0]']                         \n",
            "                                                                                                             \n",
            " discriminator_2 (Functiona  [(None, 2),                  1229443   ['generator[0][0]',           N          \n",
            " l)                           (None, 1)]                             'labels[0][0]']                         \n",
            "                                                                                                             \n",
            " concatenate_172 (Concatena  (None, 6)                    0         ['discriminator_0[0][0]',     Y          \n",
            " te)                                                                 'discriminator_1[0][0]',                \n",
            "                                                                     'discriminator_2[0][0]']                \n",
            "                                                                                                             \n",
            " concatenate_173 (Concatena  (None, 3)                    0         ['discriminator_0[0][1]',     Y          \n",
            " te)                                                                 'discriminator_1[0][1]',                \n",
            "                                                                     'discriminator_2[0][1]']                \n",
            "                                                                                                             \n",
            " comb_class_output (Dense)   (None, 2)                    14        ['concatenate_172[0][0]']     Y          \n",
            "                                                                                                             \n",
            " comb_validity_output (Dens  (None, 1)                    4         ['concatenate_173[0][0]']     Y          \n",
            " e)                                                                                                          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 2929404 (11.17 MB)\n",
            "Trainable params: 639647 (2.44 MB)\n",
            "Non-trainable params: 2289757 (8.73 MB)\n",
            "_____________________________________________________________________________________________________________\n",
            "\u001b[1m\u001b[30m------------ train_22 ------------\u001b[0m\n",
            "X_train shape 11 :  (2800, 2061)\n",
            "y_train shape 11 :  (2800,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch:   0%|          | 0/500 [00:00<?, ?epoch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "077c7c42c0474d6d8570ac4cbf2230db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[34mEpoch: 0, \n",
            "                      Mean Generator Loss: 1.320608139038086, \n",
            "                     Mean Discriminator Loss: 1.387717028458913\n",
            "                      Generator Loss: 1.321, \n",
            "                      Discriminators Loss: [D1 loss: 1.385 | D2 loss: 1.394 | D3 loss: 1.385                      \u001b[0mTook 8.979578971862793 seconds. \n",
            "\n",
            "\n",
            "\u001b[1m\u001b[34mEpoch: 100, \n",
            "                      Mean Generator Loss: 1.5194322071453132, \n",
            "                     Mean Discriminator Loss: 0.6571848796121161\n",
            "                      Generator Loss: 1.583, \n",
            "                      Discriminators Loss: [D1 loss: 0.496 | D2 loss: 0.102 | D3 loss: 0.022                      \u001b[0mTook 0.29611730575561523 seconds. \n",
            "\n",
            "\n",
            "\u001b[1m\u001b[34mEpoch: 200, \n",
            "                      Mean Generator Loss: 1.5589658051580932, \n",
            "                     Mean Discriminator Loss: 0.3900394119091786\n",
            "                      Generator Loss: 1.606, \n",
            "                      Discriminators Loss: [D1 loss: 0.187 | D2 loss: 0.038 | D3 loss: 0.012                      \u001b[0mTook 0.5157990455627441 seconds. \n",
            "\n",
            "\n",
            "\u001b[1m\u001b[34mEpoch: 300, \n",
            "                      Mean Generator Loss: 1.5471231022546457, \n",
            "                     Mean Discriminator Loss: 0.35033559796222713\n",
            "                      Generator Loss: 1.467, \n",
            "                      Discriminators Loss: [D1 loss: 0.410 | D2 loss: 0.128 | D3 loss: 0.062                      \u001b[0mTook 0.2801513671875 seconds. \n",
            "\n",
            "\n",
            "\u001b[1m\u001b[34mEpoch: 400, \n",
            "                      Mean Generator Loss: 1.5248106289980121, \n",
            "                     Mean Discriminator Loss: 0.2915745029848279\n",
            "                      Generator Loss: 1.436, \n",
            "                      Discriminators Loss: [D1 loss: 0.141 | D2 loss: 0.024 | D3 loss: 0.007                      \u001b[0mTook 0.28405022621154785 seconds. \n",
            "\n",
            "\n",
            "----- End Epoch : -----\n",
            "--- plot_losses ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- plot_3discs ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[30m------------ Tokenization + stop word ( x_test ) ------------\u001b[0m\n",
            "sentences:  1200\n",
            "tokens len: 1200\n",
            "\u001b[1m\u001b[30m------------ TfidfVectorizer ( x_test ) ------------\u001b[0m\n",
            "Shape of X_tfidf :  (1200, 2755)\n",
            "Training data TF-IDF features: \n",
            "     00  000   01  02169  03079   04   05  0esh6red32  0x28  0x2e  ...  zrgf  \\\n",
            "0  0.0  0.0  0.0    0.0    0.0  0.0  0.0         0.0   0.0   0.0  ...   0.0   \n",
            "1  0.0  0.0  0.0    0.0    0.0  0.0  0.0         0.0   0.0   0.0  ...   0.0   \n",
            "2  0.0  0.0  0.0    0.0    0.0  0.0  0.0         0.0   0.0   0.0  ...   0.0   \n",
            "3  0.0  0.0  0.0    0.0    0.0  0.0  0.0         0.0   0.0   0.0  ...   0.0   \n",
            "4  0.0  0.0  0.0    0.0    0.0  0.0  0.0         0.0   0.0   0.0  ...   0.0   \n",
            "\n",
            "   ztkr  zurera  zuta  zxig  zxon  zxyw  zymq  zyvk  â¼n  \n",
            "0   0.0     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
            "1   0.0     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
            "2   0.0     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
            "3   0.0     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
            "4   0.0     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  \n",
            "\n",
            "[5 rows x 2755 columns]\n",
            "\u001b[1m\u001b[30m------------ Scaling Features using 'MinMaxScaler' ------------\u001b[0m\n",
            "\u001b[1m\u001b[30m------------ PCA ------------\u001b[0m\n",
            "pca:  PCA(n_components=0.95, random_state=453)\n",
            "X_pca:  [[-1.86030745e-01 -1.22006731e-01 -9.67127067e-02 ... -4.59362237e-17\n",
            "   1.09979506e-16 -1.92832348e-16]\n",
            " [-2.02366019e-01  2.31569012e-01 -8.38068220e-02 ...  4.78008425e-16\n",
            "  -4.66302835e-16  4.64356668e-16]\n",
            " [-4.72030281e-02 -7.77541945e-02  1.31866442e+00 ...  9.80374282e-03\n",
            "   4.03581146e-02 -4.39304722e-02]\n",
            " ...\n",
            " [-1.87078784e-01 -1.26773856e-01 -9.83998691e-02 ... -1.97377858e-03\n",
            "  -1.45173392e-02 -3.31736485e-02]\n",
            " [ 9.06136613e-01  4.75300826e-03  3.15271153e-03 ...  1.86960805e-02\n",
            "   1.66238768e-03 -9.16284303e-03]\n",
            " [ 9.02121668e-01  5.50361076e-03 -2.51774054e-03 ... -1.17658060e-01\n",
            "  -4.15727411e-02  4.49679836e-03]]\n",
            "456\n",
            "X_pca_df:           pca_1     pca_2     pca_3     pca_4     pca_5     pca_6     pca_7  \\\n",
            "0    -0.186031 -0.122007 -0.096713 -0.052103 -0.014848 -0.020752  0.007932   \n",
            "1    -0.202366  0.231569 -0.083807 -0.102091 -0.015340 -0.000507 -0.015879   \n",
            "2    -0.047203 -0.077754  1.318664 -0.028332 -0.158092 -0.107182  0.033358   \n",
            "3    -0.186585 -0.122171 -0.097335 -0.052507 -0.014720 -0.020751  0.007983   \n",
            "4     0.798439  0.003310 -0.070471 -0.009216  0.046821  0.063025 -0.016912   \n",
            "...        ...       ...       ...       ...       ...       ...       ...   \n",
            "1195 -0.190801  0.131807  0.030643  0.775821  0.786753 -0.623490 -0.250422   \n",
            "1196 -0.187140 -0.122335 -0.097957 -0.052911 -0.014592 -0.020751  0.008035   \n",
            "1197 -0.187079 -0.126774 -0.098400 -0.053673 -0.016990 -0.022631  0.008341   \n",
            "1198  0.906137  0.004753  0.003153  0.024012  0.003952  0.011275 -0.024487   \n",
            "1199  0.902122  0.005504 -0.002518  0.012148 -0.015496 -0.027291 -0.017666   \n",
            "\n",
            "         pca_8     pca_9    pca_10  ...       pca_882       pca_883  \\\n",
            "0    -0.010800 -0.007866 -0.012883  ... -1.773458e-16 -1.400015e-16   \n",
            "1    -0.012397 -0.004672  0.000974  ... -4.603971e-16 -5.480766e-17   \n",
            "2     0.009237 -1.000584 -0.142759  ... -1.005151e-02 -3.922982e-02   \n",
            "3    -0.010937 -0.007758 -0.013224  ... -5.851222e-03  1.631369e-02   \n",
            "4    -0.012484  0.009408 -0.033358  ... -1.441784e-02  1.078721e-02   \n",
            "...        ...       ...       ...  ...           ...           ...   \n",
            "1195 -0.121689 -0.005724 -0.020659  ... -2.537845e-02 -6.086194e-02   \n",
            "1196 -0.011075 -0.007651 -0.013565  ...  1.233210e-02  4.368581e-02   \n",
            "1197 -0.011045 -0.009518 -0.012276  ... -1.048587e-02  2.775386e-02   \n",
            "1198  0.007667 -0.008576  0.021154  ... -8.991356e-03  3.537445e-02   \n",
            "1199  0.008392 -0.006447  0.021941  ...  3.524930e-03  2.186728e-02   \n",
            "\n",
            "           pca_884       pca_885       pca_886       pca_887       pca_888  \\\n",
            "0     1.593675e-16  5.952508e-18 -3.414391e-17  1.558679e-16  8.318310e-17   \n",
            "1    -7.902276e-16  2.358301e-16  1.679055e-17 -1.258316e-16  1.119675e-15   \n",
            "2     2.564547e-02  1.974919e-03 -3.061287e-02  7.130283e-03 -1.062186e-02   \n",
            "3     1.103576e-02 -1.877778e-02 -2.341462e-02  2.120668e-02  1.977928e-02   \n",
            "4    -1.128147e-02 -1.182435e-02  8.436736e-03 -6.140260e-03 -5.163818e-03   \n",
            "...            ...           ...           ...           ...           ...   \n",
            "1195 -1.777144e-02  7.764517e-04  1.735185e-02  3.897889e-02  3.897983e-02   \n",
            "1196  4.988779e-02  4.325176e-04  1.076614e-02  4.331014e-02  5.031170e-02   \n",
            "1197  2.899214e-02 -2.023805e-03 -4.061358e-02  4.375760e-02 -7.050063e-03   \n",
            "1198 -4.100891e-03  3.073927e-04 -1.689121e-02  5.450585e-03  3.789082e-03   \n",
            "1199  8.067405e-02  5.952809e-03  2.952683e-02  3.141447e-02 -2.048335e-02   \n",
            "\n",
            "           pca_889       pca_890       pca_891  \n",
            "0    -4.593622e-17  1.099795e-16 -1.928323e-16  \n",
            "1     4.780084e-16 -4.663028e-16  4.643567e-16  \n",
            "2     9.803743e-03  4.035811e-02 -4.393047e-02  \n",
            "3     1.245745e-03 -9.308702e-03 -1.540929e-02  \n",
            "4     2.083320e-02 -7.339317e-05 -1.683177e-02  \n",
            "...            ...           ...           ...  \n",
            "1195 -1.108338e-02  4.415892e-02 -1.282709e-02  \n",
            "1196  1.485406e-01  6.278982e-03  6.915855e-02  \n",
            "1197 -1.973779e-03 -1.451734e-02 -3.317365e-02  \n",
            "1198  1.869608e-02  1.662388e-03 -9.162843e-03  \n",
            "1199 -1.176581e-01 -4.157274e-02  4.496798e-03  \n",
            "\n",
            "[1200 rows x 891 columns]\n",
            "X_pca_df shape:  (1200, 891)\n",
            "789\n",
            "12333\n",
            "-- End PCA test\n",
            "\u001b[1m\u001b[30m------------ build_generator ------------\u001b[0m\n",
            "len noise:  892\n",
            "\u001b[1m\u001b[30m------------ build_discriminator ( discriminator_0 ) ------------\u001b[0m\n",
            "len data:  892\n",
            "ttsm - real/fake validity_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='validity_output/Sigmoid:0', description=\"created by layer 'validity_output'\")\n",
            "ttsm - normal/sqli auxiliary_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='auxiliary_output/Softmax:0', description=\"created by layer 'auxiliary_output'\")\n",
            "ttsm - real/fake validity_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_192/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_192'\")\n",
            "ttsm - normal/sqli auxiliary_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_193/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_193'\")\n",
            "\u001b[1m\u001b[30m------------ build_discriminator ( discriminator_1 ) ------------\u001b[0m\n",
            "len data:  892\n",
            "ttsm - real/fake validity_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='validity_output/Sigmoid:0', description=\"created by layer 'validity_output'\")\n",
            "ttsm - normal/sqli auxiliary_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='auxiliary_output/Softmax:0', description=\"created by layer 'auxiliary_output'\")\n",
            "ttsm - real/fake validity_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_194/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_194'\")\n",
            "ttsm - normal/sqli auxiliary_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_195/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_195'\")\n",
            "\u001b[1m\u001b[30m------------ build_discriminator ( discriminator_2 ) ------------\u001b[0m\n",
            "len data:  892\n",
            "ttsm - real/fake validity_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='validity_output/Sigmoid:0', description=\"created by layer 'validity_output'\")\n",
            "ttsm - normal/sqli auxiliary_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='auxiliary_output/Softmax:0', description=\"created by layer 'auxiliary_output'\")\n",
            "ttsm - real/fake validity_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_196/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_196'\")\n",
            "ttsm - normal/sqli auxiliary_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_197/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_197'\")\n",
            "\u001b[1m\u001b[30m------------ build_combined_model ------------\u001b[0m\n",
            "comb 0\n",
            "labels sm :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name='labels'), name='labels', description=\"created by layer 'labels'\")\n",
            "comb 1\n",
            "comb 2\n",
            "shap disc class output discriminators_outputs[class]:  [<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_0')>, <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_1')>, <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_2')>]\n",
            "shap disc class output discriminators_outputs[class][0]:  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='discriminator_0/auxiliary_output/Softmax:0', description=\"created by layer 'discriminator_0'\")\n",
            "shap disc class output:  2\n",
            "discriminators_outputs :  {'class': [<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_0')>, <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_1')>, <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_2')>], 'validity': [<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'discriminator_0')>, <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'discriminator_1')>, <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'discriminator_2')>]}\n",
            "comb 3\n",
            "comb 4\n",
            "comb 5\n",
            "comb 6\n",
            "77\n",
            "\u001b[1m\u001b[30m------------ Print Summary ------------\u001b[0m\n",
            "\u001b[1m\u001b[34m\n",
            "Generator summary (TDCGAN): \n",
            "\u001b[0m\n",
            "Model: \"generator\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " noise (InputLayer)          [(None, 892)]                0         []                            Y          \n",
            "                                                                                                             \n",
            " label (InputLayer)          [(None, 2)]                  0         []                            Y          \n",
            "                                                                                                             \n",
            " concatenate_174 (Concatena  (None, 894)                  0         ['noise[0][0]',               Y          \n",
            " te)                                                                 'label[0][0]']                          \n",
            "                                                                                                             \n",
            " dense_512 (Dense)           (None, 256)                  229120    ['concatenate_174[0][0]']     Y          \n",
            "                                                                                                             \n",
            " dropout_512 (Dropout)       (None, 256)                  0         ['dense_512[0][0]']           Y          \n",
            "                                                                                                             \n",
            " dense_513 (Dense)           (None, 128)                  32896     ['dropout_512[0][0]']         Y          \n",
            "                                                                                                             \n",
            " dropout_513 (Dropout)       (None, 128)                  0         ['dense_513[0][0]']           Y          \n",
            "                                                                                                             \n",
            " dense_514 (Dense)           (None, 64)                   8256      ['dropout_513[0][0]']         Y          \n",
            "                                                                                                             \n",
            " dropout_514 (Dropout)       (None, 64)                   0         ['dense_514[0][0]']           Y          \n",
            "                                                                                                             \n",
            " dense_515 (Dense)           (None, 32)                   2080      ['dropout_514[0][0]']         Y          \n",
            "                                                                                                             \n",
            " dropout_515 (Dropout)       (None, 32)                   0         ['dense_515[0][0]']           Y          \n",
            "                                                                                                             \n",
            " generated_text (Dense)      (None, 892)                  29436     ['dropout_515[0][0]']         Y          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 301788 (1.15 MB)\n",
            "Trainable params: 301788 (1.15 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_____________________________________________________________________________________________________________\n",
            "\u001b[1m\u001b[34m\n",
            "Discriminators summary (TDCGAN): \n",
            "\u001b[0m\n",
            "\u001b[1m\n",
            "Discriminator  1  summary (TDCGAN):\u001b[0m \n",
            "Model: \"discriminator_0\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " data (InputLayer)           [(None, 892)]                0         []                            N          \n",
            "                                                                                                             \n",
            " label (InputLayer)          [(None, 2)]                  0         []                            N          \n",
            "                                                                                                             \n",
            " concatenate_175 (Concatena  (None, 894)                  0         ['data[0][0]',                N          \n",
            " te)                                                                 'label[0][0]']                          \n",
            "                                                                                                             \n",
            " dense_516 (Dense)           (None, 100)                  89500     ['concatenate_175[0][0]']     N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_384 (LeakyReLU  (None, 100)                  0         ['dense_516[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_516 (Dropout)       (None, 100)                  0         ['leaky_re_lu_384[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_517 (Dense)           (None, 100)                  10100     ['dropout_516[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_385 (LeakyReLU  (None, 100)                  0         ['dense_517[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_517 (Dropout)       (None, 100)                  0         ['leaky_re_lu_385[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_518 (Dense)           (None, 100)                  10100     ['dropout_517[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_386 (LeakyReLU  (None, 100)                  0         ['dense_518[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_518 (Dropout)       (None, 100)                  0         ['leaky_re_lu_386[0][0]']     N          \n",
            "                                                                                                             \n",
            " auxiliary_output (Dense)    (None, 2)                    202       ['dropout_518[0][0]']         N          \n",
            "                                                                                                             \n",
            " validity_output (Dense)     (None, 1)                    101       ['dropout_518[0][0]']         N          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 110003 (429.70 KB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 110003 (429.70 KB)\n",
            "_____________________________________________________________________________________________________________\n",
            "\u001b[1m\n",
            "Discriminator  2  summary (TDCGAN):\u001b[0m \n",
            "Model: \"discriminator_1\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " data (InputLayer)           [(None, 892)]                0         []                            N          \n",
            "                                                                                                             \n",
            " label (InputLayer)          [(None, 2)]                  0         []                            N          \n",
            "                                                                                                             \n",
            " concatenate_176 (Concatena  (None, 894)                  0         ['data[0][0]',                N          \n",
            " te)                                                                 'label[0][0]']                          \n",
            "                                                                                                             \n",
            " dense_519 (Dense)           (None, 64)                   57280     ['concatenate_176[0][0]']     N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_387 (LeakyReLU  (None, 64)                   0         ['dense_519[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_519 (Dropout)       (None, 64)                   0         ['leaky_re_lu_387[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_520 (Dense)           (None, 128)                  8320      ['dropout_519[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_388 (LeakyReLU  (None, 128)                  0         ['dense_520[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_520 (Dropout)       (None, 128)                  0         ['leaky_re_lu_388[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_521 (Dense)           (None, 256)                  33024     ['dropout_520[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_389 (LeakyReLU  (None, 256)                  0         ['dense_521[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_521 (Dropout)       (None, 256)                  0         ['leaky_re_lu_389[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_522 (Dense)           (None, 512)                  131584    ['dropout_521[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_390 (LeakyReLU  (None, 512)                  0         ['dense_522[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_522 (Dropout)       (None, 512)                  0         ['leaky_re_lu_390[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_523 (Dense)           (None, 1024)                 525312    ['dropout_522[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_391 (LeakyReLU  (None, 1024)                 0         ['dense_523[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_523 (Dropout)       (None, 1024)                 0         ['leaky_re_lu_391[0][0]']     N          \n",
            "                                                                                                             \n",
            " auxiliary_output (Dense)    (None, 2)                    2050      ['dropout_523[0][0]']         N          \n",
            "                                                                                                             \n",
            " validity_output (Dense)     (None, 1)                    1025      ['dropout_523[0][0]']         N          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 758595 (2.89 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 758595 (2.89 MB)\n",
            "_____________________________________________________________________________________________________________\n",
            "\u001b[1m\n",
            "Discriminator  3  summary (TDCGAN):\u001b[0m \n",
            "Model: \"discriminator_2\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " data (InputLayer)           [(None, 892)]                0         []                            N          \n",
            "                                                                                                             \n",
            " label (InputLayer)          [(None, 2)]                  0         []                            N          \n",
            "                                                                                                             \n",
            " concatenate_177 (Concatena  (None, 894)                  0         ['data[0][0]',                N          \n",
            " te)                                                                 'label[0][0]']                          \n",
            "                                                                                                             \n",
            " dense_524 (Dense)           (None, 512)                  458240    ['concatenate_177[0][0]']     N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_392 (LeakyReLU  (None, 512)                  0         ['dense_524[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_524 (Dropout)       (None, 512)                  0         ['leaky_re_lu_392[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_525 (Dense)           (None, 256)                  131328    ['dropout_524[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_393 (LeakyReLU  (None, 256)                  0         ['dense_525[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_525 (Dropout)       (None, 256)                  0         ['leaky_re_lu_393[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_526 (Dense)           (None, 128)                  32896     ['dropout_525[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_394 (LeakyReLU  (None, 128)                  0         ['dense_526[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_526 (Dropout)       (None, 128)                  0         ['leaky_re_lu_394[0][0]']     N          \n",
            "                                                                                                             \n",
            " dense_527 (Dense)           (None, 64)                   8256      ['dropout_526[0][0]']         N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_395 (LeakyReLU  (None, 64)                   0         ['dense_527[0][0]']           N          \n",
            " )                                                                                                           \n",
            "                                                                                                             \n",
            " dropout_527 (Dropout)       (None, 64)                   0         ['leaky_re_lu_395[0][0]']     N          \n",
            "                                                                                                             \n",
            " auxiliary_output (Dense)    (None, 2)                    130       ['dropout_527[0][0]']         N          \n",
            "                                                                                                             \n",
            " validity_output (Dense)     (None, 1)                    65        ['dropout_527[0][0]']         N          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 630915 (2.41 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 630915 (2.41 MB)\n",
            "_____________________________________________________________________________________________________________\n",
            "\u001b[1m\u001b[34m\n",
            "Combined_model summary (TDCGAN):\n",
            "\u001b[0m \n",
            "Model: \"Combined_Model_TDCGAN\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " noise_data (InputLayer)     [(None, 892)]                0         []                            Y          \n",
            "                                                                                                             \n",
            " labels (InputLayer)         [(None, 2)]                  0         []                            Y          \n",
            "                                                                                                             \n",
            " generator (Functional)      (None, 892)                  301788    ['noise_data[0][0]',          Y          \n",
            "                                                                     'labels[0][0]']                         \n",
            "                                                                                                             \n",
            " discriminator_0 (Functiona  [(None, 2),                  110003    ['generator[0][0]',           N          \n",
            " l)                           (None, 1)]                             'labels[0][0]']                         \n",
            "                                                                                                             \n",
            " discriminator_1 (Functiona  [(None, 2),                  758595    ['generator[0][0]',           N          \n",
            " l)                           (None, 1)]                             'labels[0][0]']                         \n",
            "                                                                                                             \n",
            " discriminator_2 (Functiona  [(None, 2),                  630915    ['generator[0][0]',           N          \n",
            " l)                           (None, 1)]                             'labels[0][0]']                         \n",
            "                                                                                                             \n",
            " concatenate_178 (Concatena  (None, 6)                    0         ['discriminator_0[0][0]',     Y          \n",
            " te)                                                                 'discriminator_1[0][0]',                \n",
            "                                                                     'discriminator_2[0][0]']                \n",
            "                                                                                                             \n",
            " concatenate_179 (Concatena  (None, 3)                    0         ['discriminator_0[0][1]',     Y          \n",
            " te)                                                                 'discriminator_1[0][1]',                \n",
            "                                                                     'discriminator_2[0][1]']                \n",
            "                                                                                                             \n",
            " comb_class_output (Dense)   (None, 2)                    14        ['concatenate_178[0][0]']     Y          \n",
            "                                                                                                             \n",
            " comb_validity_output (Dens  (None, 1)                    4         ['concatenate_179[0][0]']     Y          \n",
            " e)                                                                                                          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 1801319 (6.87 MB)\n",
            "Trainable params: 301806 (1.15 MB)\n",
            "Non-trainable params: 1499513 (5.72 MB)\n",
            "_____________________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'generator' (type Functional).\n    \n    Input 0 of layer \"dense_512\" is incompatible with the layer: expected axis -1 of input shape to have value 894, but received input with shape (None, 893)\n    \n    Call arguments received by layer 'generator' (type Functional):\n      • inputs=['tf.Tensor(shape=(None, 892), dtype=float32)', 'tf.Tensor(shape=(None, 1), dtype=float32)']\n      • training=False\n      • mask=None\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-02cbea5b15dc>\u001b[0m in \u001b[0;36m<cell line: 174>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_Col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0my_pred_combined_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m  \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--99 99--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'generator' (type Functional).\n    \n    Input 0 of layer \"dense_512\" is incompatible with the layer: expected axis -1 of input shape to have value 894, but received input with shape (None, 893)\n    \n    Call arguments received by layer 'generator' (type Functional):\n      • inputs=['tf.Tensor(shape=(None, 892), dtype=float32)', 'tf.Tensor(shape=(None, 1), dtype=float32)']\n      • training=False\n      • mask=None\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\" ----------------- Apply Code ----------------- \"\"\"\n",
        "\n",
        "\n",
        "# ---------------------------> Read Data of CSV Files <--------------------------- #\n",
        "dataset_directory = \"/content/datasets\"   #files_path\n",
        "percent = 100\n",
        "data = read_csv_files(dataset_directory,percent)\n",
        "print('len data : ' , len(data))\n",
        "\n",
        "class_Col = 'attack_type'\n",
        "min_rows_per_class = 2000  #50000\n",
        "origin_data = data\n",
        "\n",
        "# >> ---------------------------> Balance Data Before Start <--------------------------- #\n",
        "#df_balanced_before = balance_data_before_tdcgan(data, class_Col, min_rows_per_class)\n",
        "#data = df_balanced_before\n",
        "\n",
        "\n",
        "# ---------------------------> Apply pre-processing <--------------------------- #\n",
        "main_text_col = 'sentence'\n",
        "n_classes, X_train_copy, X_test_copy, y_train_copy, y_test_copy, \\\n",
        "n_components, X_train_final_df = preprocess_data(data, class_Col, 'x_train', main_text_col)\n",
        "\n",
        "\n",
        "num_classes = n_classes # - 1\n",
        "input_dim = X_train_final_df.shape[1]     # (features_dim) Dimensionality of TF-IDF vectors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------> Run TDCGAN Models (Gen., Disc., Comb.) <--------------------------- #\n",
        "generator_model, discriminators_model, combined_model  = run_TDCGAN_models(input_dim, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------> Train Model <--------------------------- #\n",
        "batch_size = BATCH_SIZE       # 128\n",
        "\n",
        "X_train = X_train_final_df\n",
        "#print('ppp: ', X_train.shape)\n",
        "\n",
        "train_22(BATCH_SIZE, input_dim, num_classes, generator_model, discriminators_model, combined_model,\n",
        "        X_train_final_df, y_train_copy, NUM_EPOCHS)\n",
        "\n",
        "\"\"\"\n",
        "print('X_train shape' , X_train.shape)\n",
        "print('y_train_copy shape' , y_train_copy.shape)\n",
        "\n",
        "print('X_train head' , X_train.head)\n",
        "print('y_train_copy head' , y_train_copy.head)\n",
        "\n",
        "y_pred_combined_real = combined_model.predict([X_train, y_train_copy])\n",
        "num_samples = 128 #len(y_train_copy)\n",
        "\n",
        "print('--99 99--')\n",
        "predicted_labels_real = np.argmax(y_pred_combined_real[0], axis=1)\n",
        "predicted_real_vs_fake_combined = y_pred_combined_real[1]\n",
        "print('--99 99 888--')\n",
        "\n",
        "\n",
        "print('--88--')\n",
        "# Print the classification report for the combined model on real data\n",
        "print(f\"{TextStyle.BOLD}\\nClassification Report (Combined Model - Real Data - Class Prediction):{TextStyle.RESET_ALL}\")\n",
        "#print(classification_report(encoder.inverse_transform(y_test), encoder.inverse_transform(predicted_labels_real),zero_division=1))\n",
        "print(classification_report(y_train_copy, predicted_labels_real, zero_division=1))\n",
        "\n",
        "print(f\"{TextStyle.BOLD}\\nClassification Report (Combined Model - Real Data - Real vs. Fake Prediction):{TextStyle.RESET_ALL}\")\n",
        "print(classification_report(np.ones(num_samples), np.round(predicted_real_vs_fake_combined),zero_division=1))\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# ---------------------------> Balance Data Using Generator of TDCGAN <--------------------------- #\n",
        "df_origin_data = origin_data\n",
        "\n",
        "df_2 = pd.DataFrame(X_train_copy)\n",
        "cols = df_2.columns\n",
        "df_2[class_Col] = y_train_copy\n",
        "print('X_train_copy : ' , X_train_copy.head)\n",
        "print('df_2 : ', df_2.head)\n",
        "\n",
        "y_df = pd.DataFrame(y_train_copy)\n",
        "print('y_train_copy : ' , y_train_copy.head)\n",
        "\n",
        "\n",
        "all_df = pd.concat([df_2, y_train_copy])\n",
        "\n",
        "\n",
        "input_dim = X_train_copy.shape[1]\n",
        "\n",
        "# Identify the majority class and count\n",
        "majority_class = df_2[class_Col].value_counts().idxmax()\n",
        "majority_count = len(df_2[df_2[class_Col] == majority_class])\n",
        "\n",
        "# Select only the minority classes data for training TDCGAN\n",
        "minority_data = df_2[~df_2[class_Col].isin([majority_class])]\n",
        "print('minority_data 1: ', minority_data.head)\n",
        "\n",
        "labels_dic = (minority_data[class_Col].value_counts()).to_dict()\n",
        "\n",
        "print('majority_class : ', majority_class)\n",
        "print('majority_count : ', majority_count)\n",
        "print('minority_data: ', minority_data)\n",
        "print('labels_dic : ', labels_dic)\n",
        "print('labels_dic len : ', len(labels_dic))\n",
        "\n",
        "#Generate synthetic samples for the minority classes\n",
        "print(f\"{TextStyle.BOLD}Generate synthetic features and labels{TextStyle.RESET_ALL}\")\n",
        "for key, value in labels_dic.items():\n",
        "  labels_dic[key]= majority_count - value\n",
        "print('labels_dic 2 len : ', len(labels_dic))\n",
        "\n",
        "synthetic_data = generate_synthetic_data(labels_dic, cols, class_Col, input_dim, generator_model)\n",
        "print('synthetic_data : ', synthetic_data.head())\n",
        "\n",
        "# Concatenate synthetic samples with the original dataset\n",
        "new_data = pd.concat([df_2, synthetic_data], ignore_index=True, sort=False)\n",
        "print('new_data shape' , new_data.shape)\n",
        "\n",
        "#df_balanced_after = balance_data_using_generator(data, class_Col, min_rows_per_class, 'byModel',X_train, y_train, batch_size, input_dim)\n",
        "#df_balanced_after = generate_new_data(X_train, y_train, batch_size, noise_dim)\n",
        "\n",
        "\n",
        "# Concatenate synthetic samples with the original dataset\n",
        "#synthetic_data = df_balanced_after\n",
        "#new_data = pd.concat([df, synthetic_data], ignore_index=True, sort=False)\n",
        "\"\"\"\n",
        "\n",
        "# ---------------------------> X_test dataset in TDCGAN <--------------------------- #\n",
        "\n",
        "#from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize OneHotEncoder\n",
        "#encoder = LabelEncoder()\n",
        "# Fit and transform the target column 'y'\n",
        "#y_encoded = encoder.fit_transform(y)\n",
        "\n",
        "#print('--99 7 s --')\n",
        "#evaluate_on_real1(X_train_copy, y_train_copy, combined_model)  #encoder)\n",
        "#print('--99 7 e --')\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------> X_test dataset in TDCGAN <--------------------------- #\n",
        "#import torch\n",
        "#import torchvision\n",
        "\n",
        "### ----- Apply pre-processing ----- ###\n",
        "main_text_col = 'sentence'\n",
        "X_test, y_test, n_components, X_test_final_df = preprocess_data_test(X_test_copy, y_test_copy)\n",
        "\n",
        "# no. classes/category in class_Col (attack_type)\n",
        "n_classes = y_test.nunique()\n",
        "input_dim = X_test_final_df.shape[1]\n",
        "num_samples = len(y_test)\n",
        "\n",
        "\n",
        "generator_model, discriminators_model, combined_model  = run_TDCGAN_models(input_dim, n_classes)\n",
        "\n",
        "\n",
        "batch_size = BATCH_SIZE       # 128\n",
        "X_test = X_test_final_df\n",
        "#train_22(BATCH_SIZE, input_dim, n_classes, generator_model, discriminators_model, combined_model,\n",
        "#        X_test_final_df, y_test_copy, NUM_EPOCHS)\n",
        "\n",
        "#y_test = pd.DataFrame(y_test, columns=[class_Col])\n",
        "\n",
        "y_pred_combined_real = combined_model.predict([  X_test, y_test ])\n",
        "\n",
        "print('--99 99--')\n",
        "predicted_labels_real = np.argmax(y_pred_combined_real[0], axis=1)\n",
        "predicted_real_vs_fake_combined = y_pred_combined_real[1]\n",
        "print('--99 99 888--')\n",
        "\n",
        "# Evaluate the combined model on real data\n",
        "#X_test = torch.from_numpy(X_test)\n",
        "#y_test = torch.from_numpy(y_test)\n",
        "\n",
        "#import tensor as tf\n",
        "#Considering y variable holds numpy array\n",
        "#/y_test = tf.convert_to_tensor(y_test, dtype=tf.float64)\n",
        "\n",
        "\n",
        "idx = np.random.randint(0, X_test.shape[0], batch_size)\n",
        "X_test1 = X_test[idx]\n",
        "y_test1 = y_test[idx]\n",
        "\n",
        "print('x shape' , X_test.shape)\n",
        "print('y shape' , y_test.shape)\n",
        "\n",
        "\"\"\"\n",
        "X_test_np = X_test.to_numpy()\n",
        "y_test_np = y_test.to_numpy()\n",
        "\n",
        "print(f\"X_test shape: {X_test_np.shape}\")\n",
        "print(f\"y_test shape: {y_test_np.shape}\")\n",
        "\"\"\"\n",
        "y_pred_combined_real = combined_model.predict([  X_test1, y_test1 ])\n",
        "print('--99--')\n",
        "predicted_labels_real = np.argmax(y_pred_combined_real[0], axis=1)\n",
        "predicted_real_vs_fake_combined = y_pred_combined_real[1]\n",
        "\n",
        "print('--88--')\n",
        "# Print the classification report for the combined model on real data\n",
        "print(f\"{TextStyle.BOLD}\\nClassification Report (Combined Model - Real Data - Class Prediction):{TextStyle.RESET_ALL}\")\n",
        "#print(classification_report(encoder.inverse_transform(y_test), encoder.inverse_transform(predicted_labels_real),zero_division=1))\n",
        "print(classification_report(y_test, predicted_labels_real, zero_division=1))\n",
        "\n",
        "print(f\"{TextStyle.BOLD}\\nClassification Report (Combined Model - Real Data - Real vs. Fake Prediction):{TextStyle.RESET_ALL}\")\n",
        "print(classification_report(np.ones(num_samples), np.round(predicted_real_vs_fake_combined),zero_division=1))\n",
        "\n",
        "\n",
        "\n",
        "#performance = tdcgan_train_eval_full_balanced(data, X_test_copy, y_test_copy, encoder, name, \"TDCGAN\", class_Col, n_classes,\n",
        "#                                X_test_copy.shape[1], 100, X_train_final_df, X_test_final_df)\n",
        "\n",
        "#num_classes = X_test_copy.shape[1]\n",
        "performance = tdcgan_train_eval_full_balanced(X_test_final_df, y_test, num_classes, input_dim, generator_model, discriminators_model, combined_model)\n",
        "\n",
        "name_dataset = 'our_dataset'\n",
        "classify(data, X_test_copy, y_test_copy, encoder, name_dataset, class_Col, \"TDCGAN\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Other Models**"
      ],
      "metadata": {
        "id": "Lbn9WwSDFNIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stacking**"
      ],
      "metadata": {
        "id": "Gp27KKt4FSI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\"\"\"\n",
        "def load_and_preprocess_data(pd_frames):\n",
        "    tfidf_vectorizer = TfidfVectorizer(dtype='float32')\n",
        "    tfidf_values = tfidf_vectorizer.fit_transform(pd_frames['sentence'])\n",
        "    pd_frames.drop(columns=['sentence'], inplace=True)\n",
        "    pd_frames['success attack (impact/effect)'] = pd_frames['success attack (impact/effect)'].astype(bool)\n",
        "    pd_frames['type / status'] = pd_frames['type / status'].astype(int)\n",
        "    return pd_frames['type / status'], tfidf_values\n",
        "\"\"\"\n",
        "\n",
        "def stacking_method(X_train, X_test, y_train, y_test):\n",
        "    #Y, X = load_and_preprocess_data(pd_frames=frames)\n",
        "    #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "    base_models = [\n",
        "        ('decision_tree', DecisionTreeClassifier()),\n",
        "    ]\n",
        "    stacking_model = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())\n",
        "    stacking_model.fit(X_train, y_train)\n",
        "    y_pred = stacking_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    print(\"Mean Absolute Error:\", mae)\n",
        "    # Calculate Precision score\n",
        "    precision = precision_score(y_test, y_pred, average='macro')\n",
        "    print(\"Precision:\", precision)\n",
        "    recall = recall_score(y_test, y_pred, average='macro')\n",
        "    print(\"Recall:\", recall)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    TN = conf_matrix[0, 0]\n",
        "    FP = conf_matrix[0, 1]\n",
        "    FN = conf_matrix[1, 0]\n",
        "    TP = conf_matrix[1, 1]\n",
        "    print(\"True Positives (TP):\", TP)\n",
        "    print(\"True Negatives (TN):\", TN)\n",
        "    print(\"False Positives (FP):\", FP)\n",
        "    print(\"False Negatives (FN):\", FN)"
      ],
      "metadata": {
        "id": "tV972777FTH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Boosting**"
      ],
      "metadata": {
        "id": "xPHRALUrIg0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#from stacking import load_and_preprocess_data\n",
        "\n",
        "\n",
        "def boosting_method(X_train, X_test, y_train, y_test): #(frames):\n",
        "    #Y, X = load_and_preprocess_data(frames)\n",
        "    #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "    base_model = DecisionTreeClassifier()\n",
        "    boosting_model = AdaBoostClassifier(base_model, n_estimators=10)\n",
        "    boosting_model.fit(X_train, y_train)\n",
        "    y_pred = boosting_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    print(\"Mean Absolute Error:\", mae)\n",
        "    # Calculate Precision score\n",
        "    precision = precision_score(y_test, y_pred, average='macro')\n",
        "    print(\"Precision:\", precision)\n",
        "    recall = recall_score(y_test, y_pred, average='macro')\n",
        "    print(\"Recall:\", recall)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    TN = conf_matrix[0, 0]\n",
        "    FP = conf_matrix[0, 1]\n",
        "    FN = conf_matrix[1, 0]\n",
        "    TP = conf_matrix[1, 1]\n",
        "    print(\"True Positives (TP):\", TP)\n",
        "    print(\"True Negatives (TN):\", TN)\n",
        "    print(\"False Positives (FP):\", FP)\n",
        "    print(\"False Negatives (FN):\", FN)"
      ],
      "metadata": {
        "id": "gcTCf3OdIhNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bagging**"
      ],
      "metadata": {
        "id": "WQDd0w8yIvh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#from stacking import load_and_preprocess_data\n",
        "\n",
        "\n",
        "def bagging_method(X_train, X_test, y_train, y_test):\n",
        "    #Y, X = load_and_preprocess_data(frames)\n",
        "    #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "    base_model = DecisionTreeClassifier()\n",
        "    bagging_model = BaggingClassifier(base_model, n_estimators=10)\n",
        "    bagging_model.fit(X_train, y_train)\n",
        "    y_pred = bagging_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    print(\"Mean Absolute Error:\", mae)\n",
        "    # Calculate Precision score\n",
        "    precision = precision_score(y_test, y_pred, average='macro')\n",
        "    print(\"Precision:\", precision)\n",
        "    recall = recall_score(y_test, y_pred, average='macro')\n",
        "    print(\"Recall:\", recall)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    TN = conf_matrix[0, 0]\n",
        "    FP = conf_matrix[0, 1]\n",
        "    FN = conf_matrix[1, 0]\n",
        "    TP = conf_matrix[1, 1]\n",
        "    print(\"True Positives (TP):\", TP)\n",
        "    print(\"True Negatives (TN):\", TN)\n",
        "    print(\"False Positives (FP):\", FP)\n",
        "    print(\"False Negatives (FN):\", FN)"
      ],
      "metadata": {
        "id": "NoXv9iG7IwLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ML Performance (Stacking)**\n",
        "\n",
        "this with simple preprocess (Tfidf + split data 70/30)"
      ],
      "metadata": {
        "id": "ki5YaBkUwgsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(pd_frames):\n",
        "    tfidf_vectorizer = TfidfVectorizer(dtype='float32')\n",
        "    tfidf_values = tfidf_vectorizer.fit_transform(pd_frames['sentence'])\n",
        "    pd_frames.drop(columns=['sentence'], inplace=True)\n",
        "    pd_frames['len_payload'] = pd_frames['len_payload'].astype(int)\n",
        "    pd_frames['attack_type'] = pd_frames['attack_type'].astype(int)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(tfidf_values, pd_frames['attack_type'], test_size=0.3, random_state=42)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def stacking_method(frames):\n",
        "    X_train, X_test, y_train, y_test = load_and_preprocess_data(pd_frames=frames)\n",
        "    #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "    base_models = [\n",
        "        ('decision_tree', DecisionTreeClassifier()),\n",
        "    ]\n",
        "    stacking_model = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())\n",
        "    stacking_model.fit(X_train, y_train)\n",
        "    y_pred = stacking_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    print(\"Mean Absolute Error:\", mae)\n",
        "    # Calculate Precision score\n",
        "    precision = precision_score(y_test, y_pred, average='macro')\n",
        "    print(\"Precision:\", precision)\n",
        "    recall = recall_score(y_test, y_pred, average='macro')\n",
        "    print(\"Recall:\", recall)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    TN = conf_matrix[0, 0]\n",
        "    FP = conf_matrix[0, 1]\n",
        "    FN = conf_matrix[1, 0]\n",
        "    TP = conf_matrix[1, 1]\n",
        "    print(\"True Positives (TP):\", TP)\n",
        "    print(\"True Negatives (TN):\", TN)\n",
        "    print(\"False Positives (FP):\", FP)\n",
        "    print(\"False Negatives (FN):\", FN)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dataset_directory = \"/content/datasets\"   #files_path\n",
        "    percent = 100\n",
        "    pd_frames = read_csv_files(dataset_directory,percent)\n",
        "    print('len pd_frames : ' , len(pd_frames))\n",
        "\n",
        "    #pd_frames = pd.read_csv(\"our_Dataset.csv\")\n",
        "    stacking_method(pd_frames)\n",
        "    # pd_frames.sample(frac=1)\n",
        "    # pd_frames.to_csv('newDataSet.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "1hOJAzsCwhgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TDCGAN (v2)**"
      ],
      "metadata": {
        "id": "79VPja3vuucs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIQ10x9sx7ki"
      },
      "source": [
        "## **libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b80956-c1f4-4a21-b166-d4e3a1938035",
        "id": "xAhwCwkZx7lC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#for read csv file\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "# pre-process\n",
        "##for stop word\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "## not used now\n",
        "#import unicodedata     # Remove accents\n",
        "#import string\n",
        "\n",
        "import sklearn\n",
        "\n",
        "## for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer      ## WordPunctTokenizer --> splits words based on punctuation boundaries.\n",
        "\n",
        "## for divide data to (train / test/ validate)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# for one-hor encode (sentence to 2D)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# for TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "\n",
        "# for TDCGAN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Concatenate,\n",
        "    Embedding,\n",
        "    Dense,\n",
        "    LeakyReLU,\n",
        "    BatchNormalization,\n",
        "    Dropout,\n",
        "    Reshape,\n",
        ")\n",
        "\n",
        "import keras\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# taqadum in arabic , progress/process in english\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(tf. __version__)      # tensorflow version\n",
        "\n",
        "!python --version           # python version\n",
        "\n",
        "print(sklearn.__version__)         # scikit-learn version\n",
        "\n",
        "print(keras.__version__)           # keras version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "678dc663-c8c2-4a03-d3d7-a6faa1909e73",
        "id": "CV6-_f64x7lD"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "Python 3.10.12\n",
            "1.2.2\n",
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk0A1I0Ax7lE"
      },
      "source": [
        "## **definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "16Eb0oBrx7lF"
      },
      "outputs": [],
      "source": [
        "NUM_DISCRIMINATORS = 3\n",
        "GENERATOR_DROPOUT_RATE = 0.2  #0.1\n",
        "DISCRIMINATOR_DROPOUT_RATE = 0.3      #Adjust the dropout rate to prevent overfitting during training.\n",
        "LEAKY_RELU_ALPA = 0.2\n",
        "\n",
        "NUM_EPOCHS = 500   #1000\n",
        "BATCH_SIZE = 128\n",
        "OPTIMIZER_LR = 0.0001                 # learning rate\n",
        "OPTIMIZER_BETAS = (0.5, 0.999)\n",
        "\n",
        "# Save losses for plotting\n",
        "d0_real_losses = []   # left discriminator losses   (disc 0)\n",
        "d0_fake_losses = []   # left discriminator losses   (disc 0)\n",
        "d0_losses      = []   # discriminator losses        (disc 0)\n",
        "\n",
        "d1_real_losses = []   # Middle discriminator losses (disc 1)\n",
        "d1_fake_losses = []   # Middle discriminator losses (disc 1)\n",
        "d1_losses      = []   # discriminator losses        (disc 1)\n",
        "\n",
        "d2_real_losses = []   # right discriminator losses  (disc 2)\n",
        "d2_fake_losses = []   # right discriminator losses  (disc 2)\n",
        "d2_losses      = []   # discriminator losses        (disc 2)\n",
        "\n",
        "g_losses       = []   # generator losses\n",
        "d_losses       = []   # discriminator losses\n",
        "\n",
        "\n",
        "\n",
        "# Suppress warnings from numpy\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaUlQjsCx7lF"
      },
      "source": [
        "## **Color list + TextStyle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VDgKTkHyx7lG"
      },
      "outputs": [],
      "source": [
        "colors_list = [\n",
        "    'Red', 'Green', 'Blue', 'Purple', 'Orange', 'Pink', 'Brown', 'Yellow',\n",
        "    'Cyan', 'Magenta', 'Lime', 'Teal', 'Lavender', 'Maroon', 'Navy', 'Olive', 'Silver', 'Gold',\n",
        "    'Indigo', 'Turquoise', 'Beige', 'Crimson', 'Salmon','Tan','Lime', 'Fuchsia', 'Plum',\n",
        "    'Tomato', 'Violet']\n",
        "\n",
        "class TextStyle:\n",
        "    # Font Styles\n",
        "    BOLD = '\\033[1m'\n",
        "    DIM = '\\033[2m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "    BLINK = '\\033[5m'\n",
        "    REVERSE = '\\033[7m'\n",
        "    RESET_ALL = '\\033[0m'\n",
        "\n",
        "    # Font Colors\n",
        "    BLACK = '\\033[30m'\n",
        "    RED = '\\033[31m'\n",
        "    GREEN = '\\033[32m'\n",
        "    YELLOW = '\\033[33m'\n",
        "    BLUE = '\\033[34m'\n",
        "    MAGENTA = '\\033[35m'\n",
        "    CYAN = '\\033[36m'\n",
        "    WHITE = '\\033[37m'\n",
        "\n",
        "    # Background Colors\n",
        "    BG_BLACK = '\\033[40m'\n",
        "    BG_RED = '\\033[41m'\n",
        "    BG_GREEN = '\\033[42m'\n",
        "    BG_YELLOW = '\\033[43m'\n",
        "    BG_BLUE = '\\033[44m'\n",
        "    BG_MAGENTA = '\\033[45m'\n",
        "    BG_CYAN = '\\033[46m'\n",
        "    BG_WHITE = '\\033[47m'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn8YhYkbx7lG"
      },
      "source": [
        "## **Read Multi Files csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5y3qtqg8x7lH"
      },
      "outputs": [],
      "source": [
        "\"\"\" ----------------- Read CSV File Function ----------------- \"\"\"\n",
        "def read_csv_files(dataset_directory, percent):\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Handling Read CSV Files ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "  files = [f for f in os.listdir(dataset_directory) if f.endswith('.csv')]\n",
        "\n",
        "  if files == []:\n",
        "    print('Not found any csv files')\n",
        "  else:\n",
        "    print('Your files are: ', files)\n",
        "\n",
        "    np_array_values = []\n",
        "    data_df = pd.DataFrame()\n",
        "    firstFile = True\n",
        "\n",
        "    for file in files:\n",
        "      file_path = os.path.join(dataset_directory, file)   # csv_file_path\n",
        "      print('File Path: ', file_path)\n",
        "\n",
        "      try:\n",
        "        df = ''\n",
        "        df = pd.read_csv(file_path, encoding = \"ISO-8859-1\")  #.head()   #,low_memory=False   ISO-8859-1\n",
        "        total_rows = len(df)\n",
        "        print('Total rows in df/file: ', total_rows)\n",
        "\n",
        "        num_rows = int(total_rows * (percent / 100))\n",
        "        print('Total rows in df/file 100%: ', num_rows)\n",
        "\n",
        "\n",
        "        \"\"\" Start From Teacher Code \"\"\"\n",
        "        # Generate a list of random indices\n",
        "        random_indices = random.sample(range(total_rows), num_rows)\n",
        "        #print('random_indices: ' , random_indices)\n",
        "\n",
        "        # Select the random rows from the DataFrame\n",
        "        temp_df = df.iloc[random_indices]\n",
        "        if(firstFile):\n",
        "          # Concatenate all DataFrames into one\n",
        "          data_df = temp_df.copy()\n",
        "          firstFile = False\n",
        "        else:\n",
        "          # Concatenate all DataFrames into one\n",
        "          data_df = pd.concat([data_df,temp_df], ignore_index=True)\n",
        "\n",
        "        print(data_df)\n",
        "        return data_df\n",
        "        \"\"\" End From Teacher Code \"\"\"\n",
        "\n",
        "        ## Add DataFrame to new CSV file\n",
        "        #new_csv_file_path = os.path.join(dataset_directory, 'new_sqli.csv')  # \"/content/dataset/new_sqli.csv\"\n",
        "        #df.to_csv(new_csv_file_path, index=False)\n",
        "\n",
        "      except Exception as e:\n",
        "        print('Can not Read File called : ', file)\n",
        "        print('File path: ', file_path)\n",
        "        print(\"Errpr Exception e : \", e)\n",
        "\n",
        "\n",
        "#\"\"\" ----------------- Apply Code ----------------- \"\"\"\n",
        "#dataset_directory = \"/content/datasets\"   #files_path\n",
        "#percent = 100\n",
        "#data = read_csv_files(dataset_directory,percent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ynaJX5-wME"
      },
      "source": [
        "## **one_hot_encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "uJe--rzm-wMk"
      },
      "outputs": [],
      "source": [
        "## ----- one_hot_encoder_func ------ ##\n",
        "\n",
        "def one_hot_encoder_func(X, one_hot_cols):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Perform one-hot encoding ------------{TextStyle.RESET_ALL}\")\n",
        "    one_hot_encoded_df = X.copy()\n",
        "\n",
        "    # Perform one-hot encoding\n",
        "    one_hot_encoded_1 = pd.get_dummies(one_hot_encoded_df, columns=one_hot_cols, prefix=one_hot_cols)\n",
        "    #print('one_hot_encoded : ', one_hot_encoded_1)\n",
        "\n",
        "    # Get only columns that is original columns and needed as them + not neet to convert them from boolean to int\n",
        "    origin_needed_cols = one_hot_encoded_1[[col for col in one_hot_encoded_1.columns if\n",
        "                                            not any(col.startswith(col_name) for col_name in one_hot_cols)]]\n",
        "\n",
        "    # Get only columns that created new by one-hoe-encoder (get_dummies)\n",
        "    one_hot_cols_df = one_hot_encoded_1[[col for col in one_hot_encoded_1.columns if\n",
        "                                          any(col.startswith(col_name) for col_name in one_hot_cols)]]\n",
        "\n",
        "    # Convert one_hot_cols_df values from boolean to integers (1 and 0)\n",
        "    one_hot_cols_df = one_hot_cols_df.astype(int)\n",
        "\n",
        "    # Concatenate the one_hot_cols_df with the origin_needed_cols\n",
        "    X = pd.concat([origin_needed_cols, one_hot_cols_df], axis=1)\n",
        "    print('end one hot encoder : \\n', X.head())\n",
        "\n",
        "    \"\"\"\n",
        "      Ex of previous row:\n",
        "          i  color  size              i     color_blue   color_red   size_M   size_S\n",
        "          ---------------           ---------------------------------------------------\n",
        "          0   red    S                0         0            1          0       1\n",
        "          1   blue   M          ==>   1         1            0          1       0\n",
        "          2   blue   S                2         1            0          0       1\n",
        "    \"\"\"\n",
        "\n",
        "    return X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sidzQ5A-wMl"
      },
      "source": [
        "## **target_encode**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "uIJdmSc6-wMm"
      },
      "outputs": [],
      "source": [
        "## ----- target_encode ------ ##\n",
        "\n",
        "\"\"\"\n",
        "  # target encoding/mean encoding\n",
        "  # useful for encoding categorical features into numerical features based on their relationship with the target variable.\n",
        "  # target_column : deciding it will depend on the objective of your analysis or model training.\n",
        "  # It is typically the variable you're trying to predict in a supervised learning task.\n",
        "\n",
        "  NOTE::\n",
        "    Target-encode might not be ideal for SQL sentences\n",
        "      1. SQL sentences are unstructured text data with potentially high cardinality and no inherent ordering.\n",
        "      2. Target encoding works better for categorical variables with a clear relationship to the target,\n",
        "          rather than free-form text.\n",
        "\"\"\"\n",
        "def target_encode(X, y, target_encode_cols, target_column):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Perform target encoding ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    target_encoded_df = X.copy()\n",
        "    print('target_encoded_df: ', target_encoded_df.head())\n",
        "\n",
        "    # this colum choisen because it is needed to predict sentence is normal or sqli\n",
        "    #target_column = class_Col # y   #encoding_col\n",
        "    for col in target_encode_cols:\n",
        "\n",
        "        # Check if the column exists\n",
        "        if col not in X.columns:\n",
        "            raise KeyError(f\"Column '{col}' not found in DataFrame\")\n",
        "\n",
        "        # Concate y  --> class_Col to X now then next remove it\n",
        "        #class_Col_df = y.copy()\n",
        "        X = pd.concat([X, y], axis=1)\n",
        "\n",
        "        # Calculate the mean target value for each category  (the mean float no for each category in column)\n",
        "        mean_encoding = X.groupby(col)[target_column].mean()\n",
        "        print('mean_encoding: ', mean_encoding.head())\n",
        "\n",
        "        # Map the mean values back to the DataFrame\n",
        "        target_encoded_df[col + '_encoded'] = X[col].map(mean_encoding)\n",
        "        #print('target_encoded_df: ', target_encoded_df.head())\n",
        "\n",
        "        # Drop class_Col added to do this part\n",
        "        X = X.drop(target_column, axis=1)\n",
        "\n",
        "        # Drop the original categorical column\n",
        "        target_encoded_df = target_encoded_df.drop(col, axis=1)\n",
        "        print('target_encoded_df: ', target_encoded_df.head())\n",
        "\n",
        "        X = target_encoded_df\n",
        "\n",
        "    return X\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-process**"
      ],
      "metadata": {
        "id": "ew7WAnelv4mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "##### work result 1 in png images\n",
        "def load_and_preprocess_data(df_data):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Pre-Proccess Data ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    # ----------- Check for NaN Values ------------- #\n",
        "    print(\"Checking for NaN vallues ...\")\n",
        "    if df_data.isnull().values.any():\n",
        "        print(\"NaN values found in the dataset. Handling missing values...\")\n",
        "        # Handling missing values by imputing with mean (you can choose other methods as well)\n",
        "        df_data = df_data.fillna(df_data.mean())\n",
        "    df_data.replace([np.inf, -np.inf,np.nan,np.NAN],0, inplace=True)\n",
        "\n",
        "    if df_data.isnull().sum().sum() == 0 : # and np.isinf(df_data.values).sum() == 0 :\n",
        "        print('Done Handling missing values')\n",
        "\n",
        "\n",
        "    # ----------- lowercase + strip 'sentence' ------------- #\n",
        "    # Convert to lowercase & Remove spaces in start and end\n",
        "    df_data['sentence'] = df_data['sentence'].str.lower().str.strip()\n",
        "\n",
        "\n",
        "    # ------------------------------------  separate target from predictors ------------------------------------ #\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}separate target from predictors{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    class_Col = 'attack_type'\n",
        "    df_data['len_payload'] = df_data['len_payload'].astype(int)\n",
        "    df_data[class_Col] = df_data[class_Col].astype(int)     #pd_frames['attack_type'] = pd_frames['attack_type'].astype(int)\n",
        "    y = df_data[class_Col]      # main col to classify at end (class_Col --> attack_type)\n",
        "    n_classes = y.nunique()      # no. classes/category in class_Col (attack_type)     n_classes = pd_frames['attack_type'].nunique()\n",
        "\n",
        "    \"\"\" Start New track\n",
        "    ##$$ track 1\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}Encode categorical labels using Label Encoder {TextStyle.RESET_ALL}\")\n",
        "    encoder = LabelEncoder()                  # Initialize OneHotEncoder\n",
        "    y_encoded = encoder.fit_transform(y)      # Fit and transform the target column 'y'\n",
        "    End New track \"\"\"\n",
        "\n",
        "\n",
        "    # ------------------------------------ TfidfVectorizer train data ------------------------------------ #\n",
        "    tfidf_vectorizer = TfidfVectorizer(dtype='float32')\n",
        "    X_tfidf_values = tfidf_vectorizer.fit_transform(df_data['sentence'])\n",
        "    X_tfidf_selected_features = tfidf_vectorizer.get_feature_names_out()\n",
        "    df_data.drop(columns=['sentence'], inplace=True)\n",
        "\n",
        "\n",
        "    # ------------------------------------ train_test_split ------------------------------------ #\n",
        "    # Divide the dataset into training (70%) and testing (30%)\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}---------------- Divide the dataset into training (70%) and testing (30%) ----------------{TextStyle.RESET_ALL}\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf_values, df_data['attack_type'], test_size=0.3, random_state=42)\n",
        "    \"\"\" Start New track\n",
        "    ##$$ track 1\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf_values, y_encoded, test_size=0.3, random_state=42)\n",
        "    End New track \"\"\"\n",
        "\n",
        "    X_train = pd.DataFrame(X_train.toarray(), columns=X_tfidf_selected_features)\n",
        "    X_test  = pd.DataFrame(X_test.toarray(),  columns=X_tfidf_selected_features)\n",
        "    y_train = pd.DataFrame(y_train, columns=['attack_type'])\n",
        "    y_test  = pd.DataFrame(y_test,  columns=['attack_type'])\n",
        "\n",
        "    X_train = X_train.reset_index(drop=True)\n",
        "    X_test  = X_test.reset_index(drop=True)\n",
        "    y_train = y_train.reset_index(drop=True)\n",
        "    y_test  = y_test.reset_index(drop=True)\n",
        "\n",
        "    return n_classes, X_train, X_test, y_train, y_test#,, encoder\n",
        "    \"\"\" Start New track\n",
        "    ##$$ track 1\n",
        "    return n_classes, X_train, X_test, y_train, y_test, encoder\n",
        "    End New track \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_and_preprocess_data1(df_data):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Pre-Proccess Data ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    # ----------- Check for NaN Values ------------- #\n",
        "    print(\"Checking for NaN vallues ...\")\n",
        "    if df_data.isnull().values.any():\n",
        "        print(\"NaN values found in the dataset. Handling missing values...\")\n",
        "        # Handling missing values by imputing with mean (you can choose other methods as well)\n",
        "        df_data = df_data.fillna(df_data.mean())\n",
        "    df_data.replace([np.inf, -np.inf,np.nan,np.NAN],0, inplace=True)\n",
        "\n",
        "    if df_data.isnull().sum().sum() == 0 : # and np.isinf(df_data.values).sum() == 0 :\n",
        "        print('Done Handling missing values')\n",
        "\n",
        "\n",
        "    # ----------- lowercase + strip 'sentence' ------------- #\n",
        "    # Convert to lowercase & Remove spaces in start and end\n",
        "    df_data['sentence'] = df_data['sentence'].str.lower().str.strip()\n",
        "\n",
        "\n",
        "\n",
        "    # ------------------------------------  separate target from predictors ------------------------------------ #\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}separate target from predictors{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    class_Col = 'attack_type'\n",
        "\n",
        "    df_data['len_payload'] = df_data['len_payload'].astype(int)\n",
        "    df_data[class_Col] = df_data[class_Col].astype(int)     #pd_frames['attack_type'] = pd_frames['attack_type'].astype(int)\n",
        "    y = df_data[class_Col]      # main col to classify at end (class_Col --> attack_type)\n",
        "    n_classes = y.nunique()      # no. classes/category in class_Col (attack_type)     n_classes = pd_frames['attack_type'].nunique()\n",
        "\n",
        "\n",
        "    # remove y column (class_Col) from data columns list (df_data)\n",
        "    df_data.drop([class_Col], axis=1, inplace=True)           # comment prev. 10:00 - 19-5-2024\n",
        "\n",
        "    X = df_data.copy()\n",
        "    X_columns = X.columns       #['sentence', 'len_payload', ....]\n",
        "\n",
        "\n",
        "\n",
        "    # ------------------------------------ Apply encoding for categorical columns if any ------------------------------------ #\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}Handling Categorical Features{TextStyle.RESET_ALL}\")\n",
        "    # all categorical columns (any column with object dataType ant its value not number)\n",
        "    categorical_cols = [cname for cname in X.columns if\n",
        "                        X[cname].dtype == \"object\"]\n",
        "    print(\"Categorical columns len: \", len(categorical_cols))\n",
        "    print(\"Categorical columns: \", categorical_cols)\n",
        "\n",
        "    # Columns that will be one-hot encoded\n",
        "    # X[cname].nunique() < 4   : less that 4 categories in the column\n",
        "    one_hot_cols = [cname for cname in X.columns if\n",
        "                      X[cname].nunique() < 4 and\n",
        "                      X[cname].dtype == \"object\"]\n",
        "    print('one_hot_cols : ', one_hot_cols)\n",
        "    print(\"No. Columns need to encoded using one-hot-encoding: \", len(one_hot_cols))\n",
        "\n",
        "    # Skip this columns because it needs to use TfidFVectorizer With it\n",
        "    skip_cols = [main_text_col]\n",
        "\n",
        "    # Columns that will be target encoded  (will get columns with more than 4 categories in it :: like 'sentence' column)\n",
        "    target_encode_cols = list(set(categorical_cols)-set(one_hot_cols) - set(skip_cols) )\n",
        "    print('target_encode_cols : ', target_encode_cols)\n",
        "    print(\"No. Columns need to encoded using target-encoding: \", len(target_encode_cols))\n",
        "\n",
        "\n",
        "    # Encode categorical columns if there is any\n",
        "    if(len(categorical_cols) > 0):\n",
        "\n",
        "      # to perform one-hot encoding on specified columns of a DataFrame.\n",
        "      if(len(one_hot_cols)> 0):\n",
        "        X = one_hot_encoder_func(X, one_hot_cols)\n",
        "\n",
        "      if(len(target_encode_cols) > 0):\n",
        "        target_column = class_Col\n",
        "        X = target_encode(X, y, target_encode_cols, target_column)\n",
        "\n",
        "    else:\n",
        "      print(\"No categorical columns, nothing todo\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # ------------------------------------ train_test_split ------------------------------------ #\n",
        "    # Divide the dataset into training (70%) and testing (30%)\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}---------------- Divide the dataset into training (70%) and testing (30%) ----------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "    # ------------------------------------ TfidfVectorizer train data ------------------------------------ #\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}---------------- TfidfVectorizer ----------------{TextStyle.RESET_ALL}\")\n",
        "    tfidf_vectorizer = TfidfVectorizer(dtype='float32')\n",
        "\n",
        "    X_tfidf_values = tfidf_vectorizer.fit_transform(X_train['sentence']) #X_train)\n",
        "    X_tfidf_selected_features = tfidf_vectorizer.get_feature_names_out()\n",
        "    #pd_frames.drop(columns=['sentence'], inplace=True)\n",
        "\n",
        "    X_test_tfidf_values = tfidf_vectorizer.transform(X_test['sentence']) #X_test)\n",
        "    X_test_tfidf_selected_features = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "    origin_x_train = X_train\n",
        "    origin_x_test = X_test\n",
        "    X_train.drop(columns=['sentence'], inplace=True)\n",
        "    X_test.drop(columns=['sentence'], inplace=True)\n",
        "\n",
        "    # Concatenate TF-IDF features with the original data excluding the sentence column\n",
        "    X_train = pd.concat([X_train, X_tfidf_values], axis=1)    #.drop(['sentence'], axis=1)\n",
        "    X_test = pd.concat([X_test, X_test_tfidf_values], axis=1)\n",
        "\n",
        "    X_train = pd.DataFrame(X_train.toarray(), columns=X_tfidf_selected_features)\n",
        "    X_test  = pd.DataFrame(X_test.toarray(),  columns=X_test_tfidf_selected_features)\n",
        "    y_train = pd.DataFrame(y_train, columns=['attack_type'])\n",
        "    y_test  = pd.DataFrame(y_test,  columns=['attack_type'])\n",
        "\n",
        "    X_train = X_train.reset_index(drop=True)\n",
        "    X_test  = X_test.reset_index(drop=True)\n",
        "    y_train = y_train.reset_index(drop=True)\n",
        "    y_test  = y_test.reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "    # ------------------------------------ MinMaxScaler ------------------------------------ #\n",
        "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Scaling Features using 'MinMaxScaler' ------------{TextStyle.RESET_ALL}\")\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    X_train_scaled = scaler.fit_transform(X_train, axis=1)\n",
        "    X_test_scaled = scaler.transform(X_test, axis=1)\n",
        "\n",
        "    # Convert the scaled numerical features back to DataFrames\n",
        "    X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "    X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "\n",
        "\n",
        "    # ------------------------------------ PCA for feature selection ------------------------------------ #\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ PCA ------------{TextStyle.RESET_ALL}\")\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn import preprocessing\n",
        "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "    n_components = 0.95\n",
        "    #X_train_final_df = simple_perform_pca_new(n_components, 'x_train', X_train_scaled_df , y_train_copy )\n",
        "    #X_final_df = X_train_final_df\n",
        "\n",
        "    # Fit & Transform PCA on the training data\n",
        "    pca = PCA(n_components=n_components, random_state=453)\n",
        "    #print('pca: ' , pca)\n",
        "\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "\n",
        "    # Convert PCA results back to DataFrames and concatenate with the original non-sentence columns\n",
        "    # Generate new feature names for PCA components\n",
        "    train_pca_feature_names = [f'pca_{i+1}' for i in range(X_train_pca.shape[1])]\n",
        "    X_train_pca_df = pd.DataFrame(data=X_train_pca, columns=train_pca_feature_names)      # Create a DataFrame with the PCA-transformed data\n",
        "\n",
        "    test_pca_feature_names = [f'pca_{i+1}' for i in range(X_test_pca.shape[1])]\n",
        "    X_test_pca_df = pd.DataFrame(data=X_test_pca, columns=test_pca_feature_names)         # Create a DataFrame with the PCA-transformed data\n",
        "\n",
        "    # Combine the PCA-transformed features with the target variable\n",
        "    X_train = pd.concat([X_train_pca_df, y_train], axis=1)\n",
        "    X_test  = pd.concat([X_test_pca_df, y_test], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "    return n_classes, X_train, X_test, y_train, y_test\n",
        "\n"
      ],
      "metadata": {
        "id": "716czaiqv5Al"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implement TDCGAN Models**"
      ],
      "metadata": {
        "id": "6d9eAcBUxLK5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aE7UTuexLLM"
      },
      "source": [
        "### **build_generator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZmA_EDyxLLN"
      },
      "source": [
        "The model of the generator is a deep multi-layer perceptron (MLP) composed of an input layer, output layer and four hidden layers. Initially, the generator takes a point from the latent space to generate new data. The latent space is a multi-dimensional hypersphere normal distributed points, where each variable is drawn from the distribution of the data in the dataset. An embedded layer in the generator creates a vector representation for the generated point.\n",
        "The generator model has four hidden layers. The first one is composed of 256 neurons with a rectified linear unit (ReLU) activation function. An embedded layer is used between hidden layers to efficiently map input data from a high-dimension to lower-dimension space. This allows the neural network to learn the data relationship and process it efficiently. The second hidden layer consists of 128 neurons, the third has 64 neurons and the last one has 32 neurons, with the ReLU activation function used with them all, and a regularization dropout of 20% is added to avoid overfitting. The output layer is activated using the Softmax activation function with 14 neurons as the number of features in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_N0TLfc6xLLN"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Creates a generator model for text generation using a deep MLP.\n",
        "\n",
        "Args:\n",
        "    input_dim  : The dimension of the latent space.  (latent_dim)\n",
        "    num_classes: The size of the vocabulary (number of unique words).  ((get it by tokeniz))  (vocabulary_size)\n",
        "    GENERATOR_DROPOUT_RATE : it is 20% ==> 0.2\n",
        "\n",
        "Returns:\n",
        "    A TensorFlow Keras model representing the text generation GAN generator.\n",
        "\"\"\"\n",
        "def build_generator(input_dim, num_classes):   # vocabulary_size / num_class\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ build_generator ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "  GENERATOR_DROPOUT_RATE = 0.2\n",
        "\n",
        "  # Input layers for data (real or generated) and label\n",
        "  # our data is Conditional Generation so label is needed\n",
        "  noise = Input(shape=(input_dim,), name='noise')    # latent space noise (Features , ex: 100 samples, 10 features)\n",
        "  label = Input(shape=(num_classes,), name='label')  # no need for One-hot encoded label, its numeric\n",
        "  print('len noise: ', noise.shape[1])\n",
        "\n",
        "  #concatenated = Concatenate()([data, label])\n",
        "  # Concatenate features and label  (Concatenate along the last dimension)\n",
        "  \"\"\"  Concatenation: A Concatenate layer is used to combine the features and label along the last dimension (axis=-1).\n",
        "      This creates a single input for the subsequent hidden layers in the discriminator model.\"\"\"\n",
        "  concatenated = Concatenate()([noise, label])   #Concatenate(axis=-1) is defualt\n",
        "\n",
        "  # Embedded layer to map noise to lower-dimensional space\n",
        "  # It creates a vector representation for the generated point.\n",
        "  hidden = Embedding(input_dim, num_classes // 2)(concatenated)  # Adjust embedding dim as needed\n",
        "\n",
        "  # Hidden layers with ReLU activation and dropout\n",
        "  hidden = Dense(256, activation='relu')(concatenated)  #(hidden)\n",
        "  hidden = Dropout(GENERATOR_DROPOUT_RATE)(hidden)\n",
        "\n",
        "  hidden = Dense(128, activation='relu')(hidden)\n",
        "  hidden = Dropout(GENERATOR_DROPOUT_RATE)(hidden)\n",
        "\n",
        "  hidden = Dense(64, activation='relu')(hidden)\n",
        "  hidden = Dropout(GENERATOR_DROPOUT_RATE)(hidden)\n",
        "\n",
        "  hidden = Dense(32, activation='relu')(hidden)\n",
        "  hidden = Dropout(GENERATOR_DROPOUT_RATE)(hidden)\n",
        "\n",
        "  # Output layer with Softmax activation\n",
        "  generated_text = Dense(input_dim, activation='softmax', name='generated_text')(hidden)\n",
        "  print('generated_text shape: ', generated_text.shape)\n",
        "\n",
        "  # Create the generator model\n",
        "  generator_model = Model(inputs=[noise, label], outputs=generated_text, name='generator')\n",
        "\n",
        "  generator_model.compile(\n",
        "      loss='binary_crossentropy',\n",
        "      optimizer=Adam(learning_rate= 0.0002 , beta_1= 0.5),\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return generator_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFlEUROixLLO"
      },
      "source": [
        "### **build_discriminator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mUXlDWExLLO"
      },
      "source": [
        "In the TDCGAN architecture consists of three discriminators. Each discriminator is a deep neural network with different architecture and different parameter settings. Each discriminator’s role is to extract features from the output of the generator and classify the data with varying levels of accuracy for each them. The model aims to classify data into two groups: normal flows for the background traffic with 0 representation, and anomaly flows for the attack data with 1 representation. The discriminator distinguishes the new data generated by the generator from the true data distribution. It classifies them as either real or fake. Subsequently, the discriminator undergoes updates to improve its ability to distinguish between real and fake samples in the subsequent round. The discriminators try to classify the data into their corresponding class, which is done through a fully connected MLP network.\n",
        "\n",
        "Each discriminator is a MLP model with a different number of hidden layers, different number of neurons and different dropout percentage. The first discriminator is composed of 3 hidden layers with 100 neurons for each and 10% dropout regularization. The second has five hidden layers with 64, 128, 256, 512, and 1024 neurons for each layer, respectively. The dropout percentage is 40%. The last discriminator has 4 hidden layers with 512, 256, 128, and 64 neurons for each layer and 20% dropout percentage.\n",
        "The LeakyReLU(alpha = 0.2) is used as an activation function for the hidden layers in the discriminators. Two output layers are used for each discriminator with the Softmax function as an activation function for one output layer and the Sigmoid activation function for the second output layer.\n",
        "\n",
        "***Why 3 discriminator?***\n",
        "\n",
        "In this model, three discriminators are used and each discriminator haas different architecture. These considered a modified training strategy which helped to face challenge in detection tasks. So, it help the generator to reach its optimal state even when the discriminator converges quickly during the initial stages of training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L6PMFyR7xLLP"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "\n",
        "\"\"\"\n",
        "Creates a discriminator model for TDCGAN with specified architecture.\n",
        "\n",
        "Args:\n",
        "    input_dim         : Dimensionality of the data features (based on your data size)\n",
        "    num_classes       : Number of classes in the dataset (maybe - num_attack_classes)\n",
        "    name              : Name for the discriminator model.\n",
        "    num_hidden_layers : Number of hidden layers in the discriminator.\n",
        "    neurons_per_layer : List of integers specifying the number of neurons in each hidden layer.\n",
        "    dropout_rate      : Dropout rate for regularization.\n",
        "\n",
        "Returns:\n",
        "    A TensorFlow Keras model representing the TDCGAN discriminator.\n",
        "\"\"\"\n",
        "\n",
        "def build_discriminator(input_dim, num_classes, name, num_hidden_layers, neurons_per_layer, dropout_rate):\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ build_discriminator (\", name , f\") ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "  # Input layers for data (real or generated) and label\n",
        "  # our data is Conditional Generation so label is needed\n",
        "  data  = Input(shape=(input_dim,), name='data')       # Features    (ex: 100 samples, 10 features)\n",
        "  label = Input(shape=(num_classes,), name='label')   # Tf-IDF Vectorizer not One-hot encoded label\n",
        "  print('len data: ', data.shape[1])\n",
        "  # Concatenate features and label  (Concatenate along the last dimension)\n",
        "  \"\"\"  Concatenation: A Concatenate layer is used to combine the features and label along the last dimension (axis=-1).\n",
        "      This creates a single input for the subsequent hidden layers in the discriminator model.\"\"\"\n",
        "  concatenated = Concatenate()([data, label])   #Concatenate(axis=-1) is defualt\n",
        "\n",
        "  # Hidden layers with LeakyReLU activation and dropout\n",
        "  # These layers extract the features\n",
        "  hidden = data\n",
        "  for i in range(num_hidden_layers):\n",
        "    if i == 0:\n",
        "      #hidden = Dense(neurons_per_layer[i], activation='leaky_relu', alpha=0.2)(concatenated)\n",
        "      hidden = Dense(neurons_per_layer[i])(concatenated)\n",
        "      hidden = LeakyReLU(alpha=0.2)(hidden)\n",
        "      hidden = Dropout(dropout_rate)(hidden)\n",
        "    else:\n",
        "      #hidden = Dense(neurons_per_layer[i], activation='leaky_relu', alpha=0.2)(hidden)\n",
        "      hidden = Dense(neurons_per_layer[i])(hidden)\n",
        "      hidden = LeakyReLU(alpha=0.2)(hidden)\n",
        "      hidden = Dropout(dropout_rate)(hidden)\n",
        "      # Consider adding for training stability (important) but this will make disc summary non-trainable params != 0\n",
        "      #hidden = BatchNormalization()(hidden)\n",
        "\n",
        "  ## Flatten ?!\n",
        "\n",
        "  # Output layer for real vs fake prediction (Real/fake classification)\n",
        "  validity_output = Dense(1, activation='sigmoid', name='validity_output')(hidden)\n",
        "\n",
        "  # Output layer for class label prediction (normal/sqli attack classification)   (Auxiliary classification / class_output)\n",
        "  auxiliary_output = Dense(num_classes, activation='softmax', name='auxiliary_output')(hidden)\n",
        "\n",
        "  print('ttsm - real/fake validity_output : ', validity_output)\n",
        "  print('ttsm - normal/sqli auxiliary_output : ', auxiliary_output)\n",
        "  print('ttsm - real/fake validity_output value : ', K.print_tensor(validity_output, message='K1 = '))\n",
        "  print('ttsm - normal/sqli auxiliary_output value : ', K.print_tensor(auxiliary_output, message='K2 = '))\n",
        "\n",
        "  # Build and compile the discriminator model\n",
        "  discriminator_model = Model(inputs=[data, label], outputs=[auxiliary_output, validity_output], name=name)    # inputs=data\n",
        "\n",
        "  # auxiliary_output (attack_type maybe) may binary_crossentropy  (0 --> normal / 1 --> SQLi)\n",
        "  #'auxiliary_output': 'categorical_crossentropy', // binary_crossentropy\n",
        "  discriminator_model.compile(\n",
        "      loss={'auxiliary_output': 'categorical_crossentropy', 'validity_output': 'binary_crossentropy'},\n",
        "      optimizer=Adam(learning_rate= 0.0002 , beta_1= 0.5),\n",
        "      metrics={'auxiliary_output': 'accuracy', 'validity_output': 'accuracy'})\n",
        "\n",
        "  return discriminator_model\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\" ----------------- Apply Discriminater with req. values ------------------ \"\"\"\n",
        "  # Define discriminator architectures\n",
        "  #discriminator_1 = create_discriminator(2, 'discriminator_1', 3, [100, 100, 100], 0.1)\n",
        "  #discriminator_2 = create_discriminator(2, 'discriminator_2', 5, [64, 128, 256, 512, 1024], 0.4)\n",
        "  #discriminator_3 = create_discriminator(2, 'discriminator_3', 4, [512, 256, 128, 64], 0.2)\n",
        "\n",
        "  \"\"\"\n",
        "  # May used in training as EX\n",
        "  # Example Data (assuming you have 3 classes and input_dim is 10)\n",
        "  real_data = tf.random.normal(shape=(100, 10))  # Sample real data (100 samples, 10 features)\n",
        "  label_one_hot = tf.one_hot(tf.random.uniform(shape=(100,), minval=0, maxval=num_classes, dtype=tf.int32), depth=num_classes)  # One-hot encoded labels (100 samples)\n",
        "  \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEI7YYh-xLLQ"
      },
      "source": [
        "### **Election Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JG6lpMi-xLLQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "# Election Layer\n",
        "def election_layer(discriminators_outputs):\n",
        "    def majority_vote(logits):\n",
        "        votes = tf.reduce_sum(logits, axis=0)\n",
        "        ### votes = tf.reduce_sum(logits)  #.numpy()      ## try this\n",
        "\n",
        "        # use argmax to finds the index of the maximum value in the summed logits vector.\n",
        "        # This index corresponds to the class with the highest combined vote.\n",
        "        #xx = tf.round(tf.argmax(votes, axis=-1))\n",
        "\n",
        "        return votes\n",
        "\n",
        "    print('election 0')\n",
        "    # Stack the discriminator outputs along the last axis\n",
        "    class_outputs_stack = Lambda(lambda x: tf.stack(x, axis=0))(discriminators_outputs['class'])\n",
        "    validity_outputs_stack = Lambda(lambda x: tf.stack(x, axis=0))(discriminators_outputs['validity'])\n",
        "\n",
        "    # Unpack the class and validity outputs from each discriminator\n",
        "    #class_outputs = [output[0] for output in discriminators_outputs]\n",
        "    #validity_outputs = [output[1] for output in discriminators_outputs]\n",
        "\n",
        "\n",
        "    # Ensure that we do not calculate mean on an empty tensor\n",
        "    if class_outputs_stack.shape[0] == 0:\n",
        "        raise ValueError(\"Class outputs stack is empty.\")\n",
        "    if validity_outputs_stack.shape[0] == 0:\n",
        "        raise ValueError(\"Validity outputs stack is empty.\")\n",
        "\n",
        "\n",
        "    # Perform majority voting\n",
        "    #class_output = Lambda(lambda x: majority_vote(x), name='comb_class_output')(class_outputs_stack)  # election_class_output\n",
        "    #validity_output = Lambda(lambda x: majority_vote(x), name='comb_validity_output')(validity_outputs_stack)  # election_validity_output\n",
        "\n",
        "    print('election 1')\n",
        "    # Perform majority voting by averaging the stacked outputs\n",
        "    class_output = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=0), name='comb_class_output')(class_outputs_stack)   # election_class_output\n",
        "    validity_output = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=0), name='comb_validity_output')(validity_outputs_stack)  # election_validity_output\n",
        "\n",
        "    # Calculate the majority vote for class outputs\n",
        "    #class_output = Lambda(lambda x: majority_vote(x), name='comb_class_output')(class_outputs_stack)\n",
        "\n",
        "    # Calculate the mean of the validity outputs\n",
        "    #validity_output = Lambda(lambda x: tf.reduce_mean(x, axis=0), name='comb_validity_output')(validity_outputs_stack)\n",
        "\n",
        "\n",
        "    # Return the rounded average as the final decision (0 or 1)\n",
        "    #class_output = tf.cast( tf.round(class_output) , tf.int64)\n",
        "    #validity_output = tf.cast( tf.round(validity_output) , tf.int64)\n",
        "    print('election 3')\n",
        "    return class_output, validity_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def election_layer_old(discriminators_outputs):\n",
        "    concatenated_class_outputs = Concatenate()(discriminators_outputs['class'])         # auxiliary_output\n",
        "    concatenated_validity_outputs = Concatenate()(discriminators_outputs['validity'])   # validity_output\n",
        "\n",
        "    class_output = Dense(discriminators_outputs['class'][0].shape[-1], activation='softmax', name='election_class_output')(concatenated_class_outputs)\n",
        "    validity_output = Dense(1, activation='sigmoid', name='election_validity_output')(concatenated_validity_outputs)\n",
        "\n",
        "    return class_output, validity_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM3a3h-JxLLR"
      },
      "source": [
        "***class***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_oD4N9w6xLLR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class ElectionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ElectionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Inputs will be a list of outputs from the three discriminators\n",
        "        d1_output, d2_output, d3_output = inputs\n",
        "\n",
        "        # Stack the discriminator outputs along the last axis\n",
        "        stacked_outputs = tf.stack([d1_output, d2_output, d3_output], axis=-1)\n",
        "\n",
        "        # Perform majority voting by averaging the stacked outputs\n",
        "        majority_vote = tf.reduce_mean(stacked_outputs, axis=-1)\n",
        "\n",
        "        # Return the rounded average as the final decision (0 or 1)\n",
        "        return tf.round(majority_vote)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0][:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK5cWwDgxLLR"
      },
      "source": [
        "### **create_combined_model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jqZrxefkxLLS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_combined_model(input_dim, num_classes, generator_model, discriminators_model):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ build_combined_model ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    print('comb 0')\n",
        "    # functional API model has its input shape, it is suited for more complex models, accepting branches, concatenations, etc.\n",
        "    #noise = Input(shape=(input_dim,))\n",
        "    noise_data  = Input(shape=(input_dim,), name='noise_data')\n",
        "    labels = Input(shape=(num_classes,), name='labels')\n",
        "    print('labels sm : ', labels)\n",
        "\n",
        "    # Generator output features for the discriminator\n",
        "    generated_data = generator_model([noise_data, labels])      # generated_features\n",
        "\n",
        "\n",
        "    print('comb 1')\n",
        "    \"\"\" With loop (num of Disc. (3 Disc)  ) \"\"\"\n",
        "\n",
        "    # Define list for Discriminator outputs for the generated data\n",
        "    discriminators_class_outputs =[]\n",
        "    discriminators_validity_outputs =[]\n",
        "\n",
        "    # another way (not in loop) see -->  ##### Comment 1 #####\n",
        "\n",
        "    for discriminator in discriminators_model:\n",
        "        # Freeze discriminator\n",
        "        # (Freeze Trainable Parameters) -> this make lots params non-trainable\n",
        "        # For the combined model, we only train the generator (disable discriminator training) for all 3 Discr.\n",
        "        discriminator.trainable = False\n",
        "\n",
        "        # Discriminator Output for (class label prediction) and (validity --> real vs fake prediction)\n",
        "        class_outputs , validity_outputs = discriminator([generated_data, labels])\n",
        "        discriminators_class_outputs.append(class_outputs)\n",
        "        discriminators_validity_outputs.append(validity_outputs)\n",
        "\n",
        "        #print('class_outputs ', class_outputs )\n",
        "        #print('shap disc class output (class[0] layers [1]): ', discriminators_outputs['class'][0].get_layer().output )\n",
        "\n",
        "\n",
        "    discriminators_outputs = {\n",
        "                                'class': discriminators_class_outputs,\n",
        "                                'validity': discriminators_validity_outputs\n",
        "                            }\n",
        "\n",
        "\n",
        "    \"\"\" ------- Election Layer ------- \"\"\"\n",
        "    # discriminators_outputs[class][0]  => discriminator 0 , [1] disc 1 , [2] disc 2\n",
        "    print('comb 2')\n",
        "    print('shap disc class output discriminators_outputs[class]: ', discriminators_outputs['class'] )\n",
        "    print('shap disc class output discriminators_outputs[class][0]: ', discriminators_outputs['class'][0] )\n",
        "    print('shap disc class output: ', discriminators_outputs['class'][0].shape[-1] )\n",
        "\n",
        "\n",
        "\n",
        "    print('discriminators_outputs : ', discriminators_outputs)\n",
        "    #class_output, validity_output = election_layer(discriminators_outputs)\n",
        "\n",
        "    #election_layer_class = ElectionLayer()(discriminators_class_outputs)\n",
        "    #election_layer_validity = ElectionLayer()(discriminators_validity_outputs)\n",
        "\n",
        "    \"\"\"\"\"\"\n",
        "    print('comb 3')\n",
        "    # Concatenate discriminators outputs\n",
        "    concatenated_discriminator_class_outputs = Concatenate()(discriminators_class_outputs)\n",
        "    concatenated_discriminator_validity_outputs = Concatenate()(discriminators_validity_outputs)\n",
        "\n",
        "    print('comb 4')\n",
        "    # Output for class label prediction\n",
        "    class_output = Dense(num_classes, activation='softmax', name='comb_class_output')(concatenated_discriminator_class_outputs)\n",
        "\n",
        "    # Output for real vs fake prediction\n",
        "    validity_output = Dense(1, activation='sigmoid', name='comb_validity_output')(concatenated_discriminator_validity_outputs)\n",
        "\n",
        "\n",
        "    # Apply Election Layer to get the final majority vote\n",
        "    #election_class_output = ElectionLayer(name='election_class_output')([class_outputs for class_outputs in discriminators_class_outputs])\n",
        "    #election_validity_output = ElectionLayer(name='election_validity_output')([validity_outputs for validity_outputs in discriminators_validity_outputs])\n",
        "\n",
        "\n",
        "\n",
        "    # Perform the election using tf.reduce_max (can also use other combining strategies)\n",
        "    ##$class_output = tf.reduce_max(tf.stack(discriminators_outputs['class'], axis=-1), axis=-1)\n",
        "    ##$validity_output = tf.reduce_max(tf.stack(discriminators_outputs['validity'], axis=-1), axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print('comb 5')\n",
        "    # Build the combined model\n",
        "    combined_model = Model(inputs=[noise_data, labels],outputs=[class_output,validity_output], name='Combined_Model_TDCGAN')\n",
        "    print('comb 6')\n",
        "\n",
        "    combined_model.compile(\n",
        "        loss={'comb_class_output': 'categorical_crossentropy', 'comb_validity_output': 'binary_crossentropy'},  # categorical_crossentropy\n",
        "        optimizer = Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "        metrics={'comb_class_output': 'accuracy', 'comb_validity_output': 'accuracy'}  #['accuracy']  #\n",
        "    )\n",
        "    print(77)\n",
        "\n",
        "    return combined_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    ##### Comment 1 #####\n",
        "\n",
        "    # --------> Disc 0 <------- #\n",
        "    # Freeze discriminator\n",
        "    # For the combined model, we only train the generator (disable discriminator training) for all 3 Discr.\n",
        "    discriminators_model[0].trainable = False\n",
        "\n",
        "    # create_discriminator(data_dim, num_classes, name, num_hidden_layers, neurons_per_layer, dropout_rate):\n",
        "    class_outputs_0 , validity_outputs_0 = discriminators_model[0]([generated_data, labels])\n",
        "    discriminators_class_outputs.append(class_outputs_0)\n",
        "    discriminators_validity_outputs.append(validity_outputs_0)\n",
        "    print('class_outputs 0 : ', class_outputs_0)\n",
        "    print('validity_outputs 0 : ', validity_outputs_0)\n",
        "\n",
        "\n",
        "    # --------> Disc 1 <------- #\n",
        "    # Freeze discriminator\n",
        "    # For the combined model, we only train the generator (disable discriminator training) for all 3 Discr.\n",
        "    discriminators_model[1].trainable = False\n",
        "\n",
        "    # create_discriminator(data_dim, num_classes, name, num_hidden_layers, neurons_per_layer, dropout_rate):\n",
        "    class_outputs_1 , validity_outputs_1 = discriminators_model[1]([generated_data, labels])\n",
        "    discriminators_class_outputs.append(class_outputs_1)\n",
        "    discriminators_validity_outputs.append(validity_outputs_1)\n",
        "    print('class_outputs 1 : ', class_outputs_1)\n",
        "    print('validity_outputs 1 : ', validity_outputs_1)\n",
        "\n",
        "    # --------> Disc 2 <------- #\n",
        "    # Freeze discriminator\n",
        "    # For the combined model, we only train the generator (disable discriminator training) for all 3 Discr.\n",
        "    discriminators_model[2].trainable = False\n",
        "\n",
        "    # create_discriminator(data_dim, num_classes, name, num_hidden_layers, neurons_per_layer, dropout_rate):\n",
        "    class_outputs_2 , validity_outputs_2 = discriminators_model[2]([generated_data, labels])\n",
        "    discriminators_class_outputs.append(class_outputs_2)\n",
        "    discriminators_validity_outputs.append(validity_outputs_2)\n",
        "    print('class_outputs 2 : ', class_outputs_2)\n",
        "    print('validity_outputs 2 : ', validity_outputs_2)\n",
        "\n",
        "    class_output = class_outputs_2\n",
        "    validity_output = validity_outputs_2\n",
        "\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mpdF4pQxLLT"
      },
      "source": [
        "### **print_models_summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uj0h_JwkxLLT"
      },
      "outputs": [],
      "source": [
        "def print_models_summary(generator_model, combined_model, discriminators_model):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Print Summary ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLUE}\\nGenerator summary (TDCGAN): \\n{TextStyle.RESET_ALL}\")\n",
        "    generator_model.summary(show_trainable=True,expand_nested=True)\n",
        "\n",
        "\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLUE}\\nDiscriminators summary (TDCGAN): \\n{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    i = 1\n",
        "    for discriminator in discriminators_model:\n",
        "        print(f\"{TextStyle.BOLD}\\nDiscriminator \" , i , f\" summary (TDCGAN):{TextStyle.RESET_ALL} \")\n",
        "        discriminator.summary(show_trainable=True,expand_nested=True)\n",
        "        i = i + 1\n",
        "\n",
        "\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLUE}\\nCombined_model summary (TDCGAN):\\n{TextStyle.RESET_ALL} \")\n",
        "    combined_model.summary(show_trainable=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBymezFqxLLT"
      },
      "source": [
        "### **plot_architecture (not wok)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "E9DME5YTxLLU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_architecture(generator_model, discriminators_model, combined_model):\n",
        "\n",
        "    # Create a single row of 6 subplots\n",
        "    fig, axes = plt.subplots(6, 1, figsize=(10, 40))\n",
        "    path = '/content/datasets/assets/'\n",
        "\n",
        "    # Plot Generator Architecture\n",
        "    #print(path+'generator.png')\n",
        "    file_name = os.path.join(path, 'generator.png')\n",
        "    plot_model(generator_model, to_file=file_name, show_shapes=True, show_layer_names=True,show_trainable=True,expand_nested=True)\n",
        "    img = plt.imread('generator.png')\n",
        "    axes[0].imshow(img)\n",
        "    axes[0].axis('off')\n",
        "    axes[0].set_title('TDCGAN Generator Architecture')\n",
        "\n",
        "    # Plot Discriminator Architecture\n",
        "\n",
        "    i = 0\n",
        "    axes_idx = 1\n",
        "    for discriminator in discriminators_model:\n",
        "      disc_name = path + 'discriminator_' + i + '.png'\n",
        "      file_name = os.path.join(path, disc_name)\n",
        "      plot_model(discriminator, to_file=file_name, show_shapes=True, show_layer_names=True,show_trainable=True,expand_nested=True)\n",
        "      img = plt.imread(disc_name)\n",
        "      axes[axes_idx].imshow(img)\n",
        "      axes[axes_idx].axis('off')\n",
        "      i = i + 1\n",
        "      axes[axes_idx].set_title('TDCGAN Discriminator ' + i + ' Architecture')\n",
        "      axes_idx = axes_idx + 1\n",
        "\n",
        "    # Plot Combined Architecture\n",
        "    file_name = os.path.join(path, 'combined.png')\n",
        "    plot_model(combined_model, to_file=file_name, show_shapes=True, show_layer_names=True,show_trainable=True,expand_nested=True)\n",
        "    img = plt.imread('combined.png')\n",
        "    axes[4].imshow(img)\n",
        "    axes[4].axis('off')\n",
        "    axes[4].set_title('TDCGAN Combined Architecture')\n",
        "\n",
        "    # Plot Combined Architecture\n",
        "    file_name = os.path.join(path, 'combined2.png')\n",
        "    plot_model(combined_model, to_file=file_name, show_shapes=True, show_layer_names=True,show_trainable=True)\n",
        "    img = plt.imread('combined2.png')\n",
        "    axes[5].imshow(img)\n",
        "    axes[5].axis('off')\n",
        "    axes[5].set_title('TDCGAN Combined 2 Architecture')\n",
        "\n",
        "    #plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train**"
      ],
      "metadata": {
        "id": "1q2htCK9u0fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# input_dim = latent_dim = noise_dim\n",
        "\n",
        "def train_model(batch_size, noise_dim, num_classes, generator_model, discriminators_model, combined_model,\n",
        "          X_train, y_train, NUM_EPOCHS):\n",
        "\n",
        "  #X_train = np.array(X_train)  #.values.tolist()   #.toarray()  #to_numpy()   #csr_matrix.toarray\n",
        "  #y_train = np.array(y_train)   #.values.tolist()   #.toarray()  #to_numpy()\n",
        "\n",
        "  X_train = X_train.to_numpy()\n",
        "  y_train = y_train.to_numpy()\n",
        "  #print(y_train.head)\n",
        "  # Save losses for plotting\n",
        "  d0_real_losses = []   # left discriminator losses   (disc 0)\n",
        "  d0_fake_losses = []   # left discriminator losses   (disc 0)\n",
        "  d0_losses      = []   # discriminator losses        (disc 0)\n",
        "  d1_real_losses = []   # Middle discriminator losses (disc 1)\n",
        "  d1_fake_losses = []   # Middle discriminator losses (disc 1)\n",
        "  d1_losses      = []   # discriminator losses        (disc 1)\n",
        "  d2_real_losses = []   # right discriminator losses  (disc 2)\n",
        "  d2_fake_losses = []   # right discriminator losses  (disc 2)\n",
        "  d2_losses      = []   # discriminator losses        (disc 2)\n",
        "  g_losses       = []   # generator losses\n",
        "  d_losses       = []   # discriminator losses\n",
        "\n",
        "  #$valid = np.ones((batch_size, 1))  #tf.ones((batch_size, 1), dtype=tf.float32)\n",
        "  #$fake  = np.zeros((batch_size, 1))  #tf.zeros((batch_size, 1), dtype=tf.float32)\n",
        "\n",
        "  # -----------> Epochs <---------- #\n",
        "  for epoch in tqdm(range(NUM_EPOCHS), desc=\"Epoch\", unit=\"epoch\"):\n",
        "      d_loss_list = []\n",
        "      g_loss_list = []\n",
        "      q_loss_list = []\n",
        "      start = time.time()\n",
        "\n",
        "      #print(1)\n",
        "      # Get a batch of real data\n",
        "      idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "      #print('1 a')\n",
        "      real_data   = X_train[idx]\n",
        "      #print('len real_data: ', len(real_data))      #128\n",
        "      #print('1 b')\n",
        "      real_labels = y_train[idx]\n",
        "      #print('len real_labels: ', len(real_labels))     #128\n",
        "\n",
        "      #print(2)\n",
        "      # Ensure real_labels has the correct shape\n",
        "      one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "      one_hot_labels = one_hot_encoder.fit_transform(np.array(y_train).reshape(-1, 1))\n",
        "      real_labels = one_hot_labels[idx]\n",
        "      #print('len one-hot real_labels: ', len(real_labels))        #128\n",
        "      #print('len one-hot real_labels 10: ', real_labels[:10])       # [ [1. 0.] [1. 0.] [0. 1.] .... ]\n",
        "\n",
        "      #print(3)\n",
        "      # Sample a random batch of noise\n",
        "      noise = np.random.normal(0, 1,size=(batch_size, noise_dim))\n",
        "      real_labels1 = real_labels #np.random.normal(0, 2,size=(batch_size, num_classes))\n",
        "      #print('noise', noise[:10])\n",
        "      #print('real_labels1', real_labels1[:10])\n",
        "\n",
        "      #print('3 a')\n",
        "      #print('noise shape: ', noise.shape)\n",
        "      #print('real_labels1 shape: ', real_labels1.shape)\n",
        "      # Generate fake data using the generator -- Generate a batch of fake data\n",
        "      ##$generated_data = generator_model([noise, real_labels])    # more direct and convenient during training\n",
        "      generated_data = generator_model.predict([noise, real_labels1], verbose='0')\n",
        "      #print('3 b')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Discriminator\n",
        "      # ---------------------\n",
        "      #print('4')\n",
        "      valid = np.ones((batch_size, 1))  #tf.ones((batch_size, 1), dtype=tf.float32)\n",
        "      fake  = np.zeros((batch_size, 1))  #tf.zeros((batch_size, 1), dtype=tf.float32)\n",
        "\n",
        "      # -------->   Disc 0   <--------- #\n",
        "      #print('4 disc 0')\n",
        "      disc_loss_real_0 = discriminators_model[0].train_on_batch( [real_data, real_labels1], {'auxiliary_output': real_labels1, 'validity_output': valid })[0]\n",
        "      disc_loss_fake_0 = discriminators_model[0].train_on_batch( [generated_data, real_labels1], {'auxiliary_output': real_labels1, 'validity_output': fake })[0]\n",
        "      d_loss_0 = 0.5 * np.add(disc_loss_real_0, disc_loss_fake_0)   #take average loss from real and fake images.\n",
        "\n",
        "      # -------->   Disc 1   <--------- #\n",
        "      #print('4 disc 1')\n",
        "      disc_loss_real_1 = discriminators_model[1].train_on_batch( [real_data, real_labels1], {'auxiliary_output': real_labels1, 'validity_output': valid })[0]\n",
        "      disc_loss_fake_1 = discriminators_model[1].train_on_batch( [generated_data, real_labels1], {'auxiliary_output': real_labels1, 'validity_output': fake })[0]\n",
        "      d_loss_1 = 0.5 * np.add(disc_loss_real_1, disc_loss_fake_1)   #take average loss from real and fake images.\n",
        "\n",
        "      # -------->   Disc 2   <--------- #\n",
        "      #print('4 disc 2')\n",
        "      disc_loss_real_2 = discriminators_model[2].train_on_batch( [real_data, real_labels1], {'auxiliary_output': real_labels1, 'validity_output': valid })[0]\n",
        "      disc_loss_fake_2 = discriminators_model[2].train_on_batch( [generated_data, real_labels1], {'auxiliary_output': real_labels1, 'validity_output': fake })[0]\n",
        "      d_loss_2 = 0.5 * np.add(disc_loss_real_2, disc_loss_fake_2)   #take average loss from real and fake images.\n",
        "\n",
        "\n",
        "      d0_real_losses.append(disc_loss_real_0)\n",
        "      d1_real_losses.append(disc_loss_real_1)\n",
        "      d2_real_losses.append(disc_loss_real_2)\n",
        "\n",
        "      d0_fake_losses.append(disc_loss_fake_0)\n",
        "      d1_fake_losses.append(disc_loss_fake_1)\n",
        "      d2_fake_losses.append(disc_loss_fake_2)\n",
        "\n",
        "      d0_losses.append(d_loss_0)\n",
        "      d1_losses.append(d_loss_1)\n",
        "      d2_losses.append(d_loss_2)\n",
        "\n",
        "      # ---------------------\n",
        "      #  Train Generator\n",
        "      # ---------------------\n",
        "      #noise = np.random.normal(0, 1,size=(batch_size, noise_dim))\n",
        "      g_loss = combined_model.train_on_batch([noise, real_labels1], [real_labels1, valid])\n",
        "      g_losses.append(g_loss[0])\n",
        "\n",
        "\n",
        "\n",
        "      # Print the loss every few epochs (Print progre)\n",
        "      # can remove it & print epoch all time OR  at finish (epoch % 100)\n",
        "      if epoch % 100 == 0 or epoch == NUM_EPOCHS:\n",
        "          # try this --->  direct g_loss_list not np.mean(g_loss_list)\n",
        "          #print (\n",
        "          tqdm.write(f'{TextStyle.BOLD}{TextStyle.BLUE}Epoch: {epoch}, \\n  \\\n",
        "                    Mean Generator Loss: {np.mean(g_losses)}, \\n \\\n",
        "                    Mean Discriminator Loss: {np.mean([d0_losses, d1_losses, d2_losses])}\\n  \\\n",
        "                    Generator Loss: {g_loss[0]:.3f}, \\n  \\\n",
        "                    Discriminators Loss: [D1 loss: {d_loss_0:.3f} | D2 loss: {d_loss_1:.3f} | D3 loss: {d_loss_2:.3f}  \\\n",
        "                    {TextStyle.RESET_ALL}' ,end=''\n",
        "                )\n",
        "          print (f'Took {time.time()-start} seconds. \\n\\n')\n",
        "\n",
        "  print('----- End Epoch : -----', )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ## ------- Plot TDCGAN Training Losses Curves ------- ##\n",
        "  print('--- plot_losses ---')\n",
        "  # Define labels for each loss\n",
        "  labels      = ['Generator Loss' , 'Disc 0 Loss', 'Disc 1 Loss', 'Disc 2 Loss' ]\n",
        "  colors_list = ['Red'            , 'Orange'     , 'Blue'       , 'Green'       ]\n",
        "  epochs = list(range(NUM_EPOCHS))\n",
        "\n",
        "  plt.figure(num= 'normal_1', figsize=(12, 6))\n",
        "  plt.subplot(2, 1, 1)\n",
        "\n",
        "  plt.plot(epochs, g_losses, color=colors_list[0], label=labels[0])\n",
        "  plt.plot(epochs, d0_losses, color=colors_list[1], label=labels[1])\n",
        "  plt.plot(epochs, d1_losses, color=colors_list[2], label=labels[2])\n",
        "  plt.plot(epochs, d2_losses, color=colors_list[3], label=labels[3])\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('TDCGAN Training Loss Curves')\n",
        "  plt.legend()\n",
        "  #plt.grid(True)\n",
        "  #plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Optional: Save the plot as an image (empty content)\n",
        "  #plt.savefig('tdcgan_losses.png')\n",
        "\n",
        "\n",
        "\n",
        "  ## ------- Plot TDCGAN Training (3 Discriminator) Real-fake Loss Curves ------- ##\n",
        "  print('--- plot_3discs ---')\n",
        "  labels      = ['Disc 0 Real' , 'Disc 0 Fake',     'Disc 1 Real', 'Disc 1 Fake',     'Disc 2 Real', 'Disc 2 Fake']\n",
        "  colors_list = ['Red'         , 'pink'       ,     'Blue'       , 'Purple'     ,     'Green'      ,  'Cyan' ]\n",
        "  #colors_list = ['Red','Orange', 'Blue', 'Purple','Green','Pink','Gray','Tan','Lime','Cyan']\n",
        "  epochs = list(range(NUM_EPOCHS))\n",
        "\n",
        "  #plt.figure()\n",
        "  plt.figure(num= 'normal_21', figsize=(12, 6))\n",
        "  #plt.subplot(2, 1, 2)\n",
        "\n",
        "  plt.plot(epochs, d0_real_losses, color=colors_list[0], label=labels[0])\n",
        "  plt.plot(epochs, d0_fake_losses, color=colors_list[1], label=labels[1])\n",
        "  plt.plot(epochs, d1_real_losses, color=colors_list[2], label=labels[2])\n",
        "  plt.plot(epochs, d1_fake_losses, color=colors_list[3], label=labels[3])\n",
        "  plt.plot(epochs, d2_real_losses, color=colors_list[4], label=labels[4])\n",
        "  plt.plot(epochs, d2_fake_losses, color=colors_list[5], label=labels[5])\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('TDCGAN Training (3 Discriminator) Real-fake Loss Curves')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  return one_hot_encoder\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Szwuye2Iu1OT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluate (Performace / test data)**"
      ],
      "metadata": {
        "id": "v42Ph7qbvc7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_tdcgan(X_test, y_test, n_classes, batch_size, one_hot_encoder): ###$$$, encoder): #track 1\n",
        "    # ---------------------------> X_test dataset in TDCGAN <--------------------------- #\n",
        "\n",
        "    ##print('e1')\n",
        "    input_dim = X_test.shape[1]\n",
        "    num_samples = len(X_test)\n",
        "\n",
        "    ##print('e2')\n",
        "    # One-hot encode labels for evaluation\n",
        "    #one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "    one_hot_labels = one_hot_encoder.transform(np.array(y_test).reshape(-1, 1))\n",
        "    real_labels = one_hot_labels #[idx]\n",
        "    ##print('e3')\n",
        "\n",
        "    # Evaluate the combined model on real data\n",
        "    #y_pred_combined_real = combined_model([X_test, real_labels1])\n",
        "    y_pred_combined_real = combined_model.predict([X_test, one_hot_labels])\n",
        "    ##print('e4')\n",
        "    predicted_labels_real = np.argmax(y_pred_combined_real[0], axis=1)\n",
        "    ##print('e5')\n",
        "    predicted_real_vs_fake_combined = y_pred_combined_real[1]\n",
        "    ##print('e6')\n",
        "\n",
        "    ## way 1\n",
        "    # Print the classification report for the combined model on real data\n",
        "    print(f\"{TextStyle.BOLD}\\nClassification Report (Combined Model - Real Data - Class Prediction):{TextStyle.RESET_ALL}\")\n",
        "    ###$$$print(classification_report(encoder.inverse_transform(y_test), encoder.inverse_transform(predicted_labels_real),zero_division=1))  # track 1\n",
        "    print(classification_report(y_test, predicted_labels_real, zero_division=1))\n",
        "\n",
        "    ##print('e7')\n",
        "    print(f\"{TextStyle.BOLD}\\nClassification Report (Combined Model - Real Data - Real vs. Fake Prediction):{TextStyle.RESET_ALL}\")\n",
        "    print(classification_report(np.ones(num_samples), np.round(predicted_real_vs_fake_combined),zero_division=1))\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    ## way 2\n",
        "    print('-------')\n",
        "    print(f\"{TextStyle.BOLD}\\nAnother way for performance (Combined Model - Real Data - Class Prediction):{TextStyle.RESET_ALL}\")\n",
        "    accuracy = accuracy_score(y_test, predicted_labels_real)\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    mae = mean_absolute_error(y_test, predicted_labels_real)\n",
        "    print(\"Mean Absolute Error:\", mae)\n",
        "    # Calculate Precision score\n",
        "    precision = precision_score(y_test, predicted_labels_real, average='macro')\n",
        "    print(\"Precision:\", precision)\n",
        "    recall = recall_score(y_test, predicted_labels_real, average='macro')\n",
        "    print(\"Recall:\", recall)\n",
        "    conf_matrix = confusion_matrix(y_test, predicted_labels_real)\n",
        "    TN = conf_matrix[0, 0]\n",
        "    FP = conf_matrix[0, 1]\n",
        "    FN = conf_matrix[1, 0]\n",
        "    TP = conf_matrix[1, 1]\n",
        "    print(\"True Positives (TP):\", TP)\n",
        "    print(\"True Negatives (TN):\", TN)\n",
        "    print(\"False Positives (FP):\", FP)\n",
        "    print(\"False Negatives (FN):\", FN)\n",
        "\n",
        "\n",
        "    print('-------')\n",
        "    print(f\"{TextStyle.BOLD}\\nAnother way for performance (Combined Model - Real Data - Real vs. Fake Prediction):{TextStyle.RESET_ALL}\")\n",
        "    y_testing = np.ones(num_samples)\n",
        "    y_predection = np.round(predicted_real_vs_fake_combined)\n",
        "\n",
        "    accuracy = accuracy_score(y_testing, y_predection)\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    mae = mean_absolute_error(y_testing, y_predection)\n",
        "    print(\"Mean Absolute Error:\", mae)\n",
        "    # Calculate Precision score\n",
        "    precision = precision_score(y_testing, y_predection, average='macro')\n",
        "    print(\"Precision:\", precision)\n",
        "    recall = recall_score(y_testing, y_predection, average='macro')\n",
        "    print(\"Recall:\", recall)\n",
        "    conf_matrix = confusion_matrix(y_testing, y_predection)\n",
        "    TN = conf_matrix[0, 0]\n",
        "    FP = conf_matrix[0, 1]\n",
        "    FN = conf_matrix[1, 0]\n",
        "    TP = conf_matrix[1, 1]\n",
        "    print(\"True Positives (TP):\", TP)\n",
        "    print(\"True Negatives (TN):\", TN)\n",
        "    print(\"False Positives (FP):\", FP)\n",
        "    print(\"False Negatives (FN):\", FN)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    ## way 3\n",
        "    Update discriminator metrics based on true labels and predicted labels.\n",
        "    Parameters:\n",
        "      - y_true: True labels (ground truth).\n",
        "      - y_pred: Predicted labels.\n",
        "    Returns:\n",
        "      - metrics_dict: Dictionary containing updated discriminator metrics.\n",
        "\n",
        "    def update_discriminator_metrics(y_true, y_pred):\n",
        "\n",
        "        metrics_dict = {}\n",
        "        metrics_dict['accuracy'] = accuracy_score(y_true, y_pred)           # Calculate accuracy\n",
        "        metrics_dict['precision'] = precision_score(y_true, y_pred)         # Calculate precision\n",
        "        metrics_dict['recall'] = recall_score(y_true, y_pred)               # Calculate recall\n",
        "        metrics_dict['f1'] = f1_score(y_true, y_pred)                       # Calculate F1 score\n",
        "\n",
        "        return metrics_dict\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "WW7a42qNvd93"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Run Model**"
      ],
      "metadata": {
        "id": "yA8hhLWYwQB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# ---------------------------> Read Data of CSV Files <--------------------------- #\n",
        "dataset_directory = \"/content/datasets\"   #files_path\n",
        "percent = 100\n",
        "data = read_csv_files(dataset_directory,percent)\n",
        "print('len data : ' , len(data))\n",
        "\n",
        "class_Col = 'attack_type'\n",
        "min_rows_per_class = 2000  #50000\n",
        "origin_data = data\n",
        "n_components = 0.95\n",
        "\n",
        "# >> ---------------------------> Balance Data Before Start <--------------------------- #\n",
        "#df_balanced_before = balance_data_before_tdcgan(data, class_Col, min_rows_per_class)\n",
        "#data = df_balanced_before\n",
        "\n",
        "\n",
        "# ---------------------------> Apply pre-processing <--------------------------- #\n",
        "main_text_col = 'sentence'\n",
        "n_classes, X_train, X_test, y_train, y_test = load_and_preprocess_data(data)        ## , encoder    # track 1\n",
        "\n",
        "num_classes = n_classes\n",
        "\n",
        "\n",
        "# ---------------------------> Run TDCGAN Models (Gen., Disc., Comb.) - run_TDCGAN_models(old) <--------------------------- #\n",
        "input_dim = X_train.shape[1]     # (features_dim) Dimensionality of TF-IDF vectors\n",
        "print('features_dim shape: ', input_dim)\n",
        "\n",
        "## Build the generator\n",
        "generator_model = build_generator(input_dim, num_classes)   # random_noise_size / input_shape / input_dim\n",
        "\n",
        "## Build multiple discriminators (3 disc.)\n",
        "discriminators_model = []\n",
        "discriminators_model.append(build_discriminator(input_dim, num_classes, 'discriminator_0', 3, [100, 100, 100], 0.1))\n",
        "discriminators_model.append(build_discriminator(input_dim, num_classes, 'discriminator_1', 5, [64, 128, 256, 512, 1024], 0.4))\n",
        "discriminators_model.append(build_discriminator(input_dim, num_classes, 'discriminator_2', 4, [512, 256, 128, 64], 0.2))\n",
        "\n",
        "## Build the combined model (GAN Model)\n",
        "combined_model = build_combined_model(input_dim, num_classes, generator_model, discriminators_model)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------> print_models_summary <--------------------------- #\n",
        "print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Print Summary ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "## Generator Summary\n",
        "print(f\"{TextStyle.BOLD}{TextStyle.BLUE}\\nGenerator summary (TDCGAN): \\n{TextStyle.RESET_ALL}\")\n",
        "generator_model.summary(show_trainable=True,expand_nested=True)   #generator_model.summary()\n",
        "\n",
        "## Discriminators Summary\n",
        "print(f\"{TextStyle.BOLD}{TextStyle.BLUE}\\nDiscriminators summary (TDCGAN): \\n{TextStyle.RESET_ALL}\")\n",
        "i = 1\n",
        "for discriminator in discriminators_model:\n",
        "    print(f\"{TextStyle.BOLD}\\nDiscriminator \" , i , f\" summary (TDCGAN):{TextStyle.RESET_ALL} \")\n",
        "    discriminator.summary(show_trainable=True,expand_nested=True)\n",
        "    i = i + 1\n",
        "\n",
        "## Combined Summary\n",
        "print(f\"{TextStyle.BOLD}{TextStyle.BLUE}\\nCombined_model summary (TDCGAN):\\n{TextStyle.RESET_ALL} \")\n",
        "combined_model.summary(show_trainable=True)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------> Train Model <--------------------------- #\n",
        "print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ train_model ------------{TextStyle.RESET_ALL}\")\n",
        "batch_size = BATCH_SIZE       # 128\n",
        "noise_dim = input_dim\n",
        "one_hot_encoder = train_model(BATCH_SIZE, noise_dim, num_classes, generator_model, discriminators_model, combined_model,\n",
        "                      X_train, y_train, NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------> Evaluate Model <--------------------------- #\n",
        "evaluate_tdcgan(X_test, y_test, n_classes, batch_size, one_hot_encoder)  #, encoder)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0ac2308269ba48d5942d8460677faf8a",
            "736fab2900b54f7391c72ba7d807ec8d",
            "fc17e67498c04663bd22eff4d7d7fa33",
            "61a21e41a3cd4790aa57e80201b7e5b2",
            "4bf270b45c0144eb83e1073fcd073f6d",
            "70147ad69d0f48e3a38e603e1324a9be",
            "2bda29b21fa444c4a26503d80f57d05a",
            "a85a6fea7881416e97fbb0a4de378c1b",
            "1ca2a236a3754641b29635aa7dbe79db",
            "310ddc25840e4f9fb2cc94defec23939",
            "903fb65cad6d4db681dbb446aa37a8a3"
          ]
        },
        "id": "UETAmttkwQz7",
        "outputId": "d9ce60d0-8c60-42de-a473-d79f87ce647b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[30m------------ Handling Read CSV Files ------------\u001b[0m\n",
            "Your files are:  ['before_only_4000.csv']\n",
            "File Path:  /content/datasets/before_only_4000.csv\n",
            "Total rows in df/file:  4000\n",
            "Total rows in df/file 100%:  4000\n",
            "                                               sentence  attack_type  \\\n",
            "210                         admin\" )  or  ( \"1\" = \"1\"/*            1   \n",
            "1007  -3596  )  )   )  or 5903 =  ( 'qqpjq'|| ( sele...            1   \n",
            "353   ' )  AND 4605 =  ( SELECT UPPER ( XMLType ( CH...            1   \n",
            "1014   ( SELECT  ( CASE WHEN  ( 3989 = 7130 )  THEN ...            1   \n",
            "2034                                        m_at1i852se            0   \n",
            "...                                                 ...          ...   \n",
            "804   1%'  )  )   )  and 8148 = like ( 'abcdefg',upp...            1   \n",
            "2464                                              colin            0   \n",
            "1592  1%'  )  )   )  and  ( select 9067 from ( selec...            1   \n",
            "1790  select  ( case when  ( 8808 = 1367 )  then 1 e...            1   \n",
            "102   -3790\" )  as ojgb where 5925 = 5925 union all ...            1   \n",
            "\n",
            "      len_payload  \n",
            "210            27  \n",
            "1007          146  \n",
            "353           322  \n",
            "1014          128  \n",
            "2034           11  \n",
            "...           ...  \n",
            "804            99  \n",
            "2464            5  \n",
            "1592          250  \n",
            "1790          107  \n",
            "102            63  \n",
            "\n",
            "[4000 rows x 3 columns]\n",
            "len data :  4000\n",
            "\u001b[1m\u001b[30m------------ Pre-Proccess Data ------------\u001b[0m\n",
            "Checking for NaN vallues ...\n",
            "Done Handling missing values\n",
            "\u001b[1m\u001b[30mseparate target from predictors\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:2072: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. float32 'dtype' will be converted to np.float64.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[30m---------------- Divide the dataset into training (70%) and testing (30%) ----------------\u001b[0m\n",
            "features_dim shape:  7352\n",
            "\u001b[1m\u001b[30m------------ build_generator ------------\u001b[0m\n",
            "len noise:  7352\n",
            "generated_text shape:  (None, 7352)\n",
            "\u001b[1m\u001b[30m------------ build_discriminator ( discriminator_0 ) ------------\u001b[0m\n",
            "len data:  7352\n",
            "ttsm - real/fake validity_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='validity_output/Sigmoid:0', description=\"created by layer 'validity_output'\")\n",
            "ttsm - normal/sqli auxiliary_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='auxiliary_output/Softmax:0', description=\"created by layer 'auxiliary_output'\")\n",
            "ttsm - real/fake validity_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_30/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_30'\")\n",
            "ttsm - normal/sqli auxiliary_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_31/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_31'\")\n",
            "\u001b[1m\u001b[30m------------ build_discriminator ( discriminator_1 ) ------------\u001b[0m\n",
            "len data:  7352\n",
            "ttsm - real/fake validity_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='validity_output/Sigmoid:0', description=\"created by layer 'validity_output'\")\n",
            "ttsm - normal/sqli auxiliary_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='auxiliary_output/Softmax:0', description=\"created by layer 'auxiliary_output'\")\n",
            "ttsm - real/fake validity_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_32/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_32'\")\n",
            "ttsm - normal/sqli auxiliary_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_33/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_33'\")\n",
            "\u001b[1m\u001b[30m------------ build_discriminator ( discriminator_2 ) ------------\u001b[0m\n",
            "len data:  7352\n",
            "ttsm - real/fake validity_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='validity_output/Sigmoid:0', description=\"created by layer 'validity_output'\")\n",
            "ttsm - normal/sqli auxiliary_output :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='auxiliary_output/Softmax:0', description=\"created by layer 'auxiliary_output'\")\n",
            "ttsm - real/fake validity_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_34/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_34'\")\n",
            "ttsm - normal/sqli auxiliary_output value :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='tf.keras.backend.print_tensor_35/Identity:0', description=\"created by layer 'tf.keras.backend.print_tensor_35'\")\n",
            "\u001b[1m\u001b[30m------------ build_combined_model ------------\u001b[0m\n",
            "comb 0\n",
            "labels sm :  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name='labels'), name='labels', description=\"created by layer 'labels'\")\n",
            "comb 1\n",
            "comb 2\n",
            "shap disc class output discriminators_outputs[class]:  [<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_0')>, <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_1')>, <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_2')>]\n",
            "shap disc class output discriminators_outputs[class][0]:  KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='discriminator_0/auxiliary_output/Softmax:0', description=\"created by layer 'discriminator_0'\")\n",
            "shap disc class output:  2\n",
            "discriminators_outputs :  {'class': [<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_0')>, <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_1')>, <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'discriminator_2')>], 'validity': [<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'discriminator_0')>, <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'discriminator_1')>, <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'discriminator_2')>]}\n",
            "comb 3\n",
            "comb 4\n",
            "comb 5\n",
            "comb 6\n",
            "77\n",
            "\u001b[1m\u001b[30m------------ Print Summary ------------\u001b[0m\n",
            "\u001b[1m\u001b[34m\n",
            "Generator summary (TDCGAN): \n",
            "\u001b[0m\n",
            "Model: \"generator\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " noise (InputLayer)          [(None, 7352)]               0         []                            Y          \n",
            "                                                                                                             \n",
            " label (InputLayer)          [(None, 2)]                  0         []                            Y          \n",
            "                                                                                                             \n",
            " concatenate_30 (Concatenat  (None, 7354)                 0         ['noise[0][0]',               Y          \n",
            " e)                                                                  'label[0][0]']                          \n",
            "                                                                                                             \n",
            " dense_80 (Dense)            (None, 256)                  1882880   ['concatenate_30[0][0]']      Y          \n",
            "                                                                                                             \n",
            " dropout_80 (Dropout)        (None, 256)                  0         ['dense_80[0][0]']            Y          \n",
            "                                                                                                             \n",
            " dense_81 (Dense)            (None, 128)                  32896     ['dropout_80[0][0]']          Y          \n",
            "                                                                                                             \n",
            " dropout_81 (Dropout)        (None, 128)                  0         ['dense_81[0][0]']            Y          \n",
            "                                                                                                             \n",
            " dense_82 (Dense)            (None, 64)                   8256      ['dropout_81[0][0]']          Y          \n",
            "                                                                                                             \n",
            " dropout_82 (Dropout)        (None, 64)                   0         ['dense_82[0][0]']            Y          \n",
            "                                                                                                             \n",
            " dense_83 (Dense)            (None, 32)                   2080      ['dropout_82[0][0]']          Y          \n",
            "                                                                                                             \n",
            " dropout_83 (Dropout)        (None, 32)                   0         ['dense_83[0][0]']            Y          \n",
            "                                                                                                             \n",
            " generated_text (Dense)      (None, 7352)                 242616    ['dropout_83[0][0]']          Y          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 2168728 (8.27 MB)\n",
            "Trainable params: 2168728 (8.27 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_____________________________________________________________________________________________________________\n",
            "\u001b[1m\u001b[34m\n",
            "Discriminators summary (TDCGAN): \n",
            "\u001b[0m\n",
            "\u001b[1m\n",
            "Discriminator  1  summary (TDCGAN):\u001b[0m \n",
            "Model: \"discriminator_0\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " data (InputLayer)           [(None, 7352)]               0         []                            N          \n",
            "                                                                                                             \n",
            " label (InputLayer)          [(None, 2)]                  0         []                            N          \n",
            "                                                                                                             \n",
            " concatenate_31 (Concatenat  (None, 7354)                 0         ['data[0][0]',                N          \n",
            " e)                                                                  'label[0][0]']                          \n",
            "                                                                                                             \n",
            " dense_84 (Dense)            (None, 100)                  735500    ['concatenate_31[0][0]']      N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_60 (LeakyReLU)  (None, 100)                  0         ['dense_84[0][0]']            N          \n",
            "                                                                                                             \n",
            " dropout_84 (Dropout)        (None, 100)                  0         ['leaky_re_lu_60[0][0]']      N          \n",
            "                                                                                                             \n",
            " dense_85 (Dense)            (None, 100)                  10100     ['dropout_84[0][0]']          N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_61 (LeakyReLU)  (None, 100)                  0         ['dense_85[0][0]']            N          \n",
            "                                                                                                             \n",
            " dropout_85 (Dropout)        (None, 100)                  0         ['leaky_re_lu_61[0][0]']      N          \n",
            "                                                                                                             \n",
            " dense_86 (Dense)            (None, 100)                  10100     ['dropout_85[0][0]']          N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_62 (LeakyReLU)  (None, 100)                  0         ['dense_86[0][0]']            N          \n",
            "                                                                                                             \n",
            " dropout_86 (Dropout)        (None, 100)                  0         ['leaky_re_lu_62[0][0]']      N          \n",
            "                                                                                                             \n",
            " auxiliary_output (Dense)    (None, 2)                    202       ['dropout_86[0][0]']          N          \n",
            "                                                                                                             \n",
            " validity_output (Dense)     (None, 1)                    101       ['dropout_86[0][0]']          N          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 756003 (2.88 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 756003 (2.88 MB)\n",
            "_____________________________________________________________________________________________________________\n",
            "\u001b[1m\n",
            "Discriminator  2  summary (TDCGAN):\u001b[0m \n",
            "Model: \"discriminator_1\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " data (InputLayer)           [(None, 7352)]               0         []                            N          \n",
            "                                                                                                             \n",
            " label (InputLayer)          [(None, 2)]                  0         []                            N          \n",
            "                                                                                                             \n",
            " concatenate_32 (Concatenat  (None, 7354)                 0         ['data[0][0]',                N          \n",
            " e)                                                                  'label[0][0]']                          \n",
            "                                                                                                             \n",
            " dense_87 (Dense)            (None, 64)                   470720    ['concatenate_32[0][0]']      N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_63 (LeakyReLU)  (None, 64)                   0         ['dense_87[0][0]']            N          \n",
            "                                                                                                             \n",
            " dropout_87 (Dropout)        (None, 64)                   0         ['leaky_re_lu_63[0][0]']      N          \n",
            "                                                                                                             \n",
            " dense_88 (Dense)            (None, 128)                  8320      ['dropout_87[0][0]']          N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_64 (LeakyReLU)  (None, 128)                  0         ['dense_88[0][0]']            N          \n",
            "                                                                                                             \n",
            " dropout_88 (Dropout)        (None, 128)                  0         ['leaky_re_lu_64[0][0]']      N          \n",
            "                                                                                                             \n",
            " dense_89 (Dense)            (None, 256)                  33024     ['dropout_88[0][0]']          N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_65 (LeakyReLU)  (None, 256)                  0         ['dense_89[0][0]']            N          \n",
            "                                                                                                             \n",
            " dropout_89 (Dropout)        (None, 256)                  0         ['leaky_re_lu_65[0][0]']      N          \n",
            "                                                                                                             \n",
            " dense_90 (Dense)            (None, 512)                  131584    ['dropout_89[0][0]']          N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_66 (LeakyReLU)  (None, 512)                  0         ['dense_90[0][0]']            N          \n",
            "                                                                                                             \n",
            " dropout_90 (Dropout)        (None, 512)                  0         ['leaky_re_lu_66[0][0]']      N          \n",
            "                                                                                                             \n",
            " dense_91 (Dense)            (None, 1024)                 525312    ['dropout_90[0][0]']          N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_67 (LeakyReLU)  (None, 1024)                 0         ['dense_91[0][0]']            N          \n",
            "                                                                                                             \n",
            " dropout_91 (Dropout)        (None, 1024)                 0         ['leaky_re_lu_67[0][0]']      N          \n",
            "                                                                                                             \n",
            " auxiliary_output (Dense)    (None, 2)                    2050      ['dropout_91[0][0]']          N          \n",
            "                                                                                                             \n",
            " validity_output (Dense)     (None, 1)                    1025      ['dropout_91[0][0]']          N          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 1172035 (4.47 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 1172035 (4.47 MB)\n",
            "_____________________________________________________________________________________________________________\n",
            "\u001b[1m\n",
            "Discriminator  3  summary (TDCGAN):\u001b[0m \n",
            "Model: \"discriminator_2\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " data (InputLayer)           [(None, 7352)]               0         []                            N          \n",
            "                                                                                                             \n",
            " label (InputLayer)          [(None, 2)]                  0         []                            N          \n",
            "                                                                                                             \n",
            " concatenate_33 (Concatenat  (None, 7354)                 0         ['data[0][0]',                N          \n",
            " e)                                                                  'label[0][0]']                          \n",
            "                                                                                                             \n",
            " dense_92 (Dense)            (None, 512)                  3765760   ['concatenate_33[0][0]']      N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_68 (LeakyReLU)  (None, 512)                  0         ['dense_92[0][0]']            N          \n",
            "                                                                                                             \n",
            " dropout_92 (Dropout)        (None, 512)                  0         ['leaky_re_lu_68[0][0]']      N          \n",
            "                                                                                                             \n",
            " dense_93 (Dense)            (None, 256)                  131328    ['dropout_92[0][0]']          N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_69 (LeakyReLU)  (None, 256)                  0         ['dense_93[0][0]']            N          \n",
            "                                                                                                             \n",
            " dropout_93 (Dropout)        (None, 256)                  0         ['leaky_re_lu_69[0][0]']      N          \n",
            "                                                                                                             \n",
            " dense_94 (Dense)            (None, 128)                  32896     ['dropout_93[0][0]']          N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_70 (LeakyReLU)  (None, 128)                  0         ['dense_94[0][0]']            N          \n",
            "                                                                                                             \n",
            " dropout_94 (Dropout)        (None, 128)                  0         ['leaky_re_lu_70[0][0]']      N          \n",
            "                                                                                                             \n",
            " dense_95 (Dense)            (None, 64)                   8256      ['dropout_94[0][0]']          N          \n",
            "                                                                                                             \n",
            " leaky_re_lu_71 (LeakyReLU)  (None, 64)                   0         ['dense_95[0][0]']            N          \n",
            "                                                                                                             \n",
            " dropout_95 (Dropout)        (None, 64)                   0         ['leaky_re_lu_71[0][0]']      N          \n",
            "                                                                                                             \n",
            " auxiliary_output (Dense)    (None, 2)                    130       ['dropout_95[0][0]']          N          \n",
            "                                                                                                             \n",
            " validity_output (Dense)     (None, 1)                    65        ['dropout_95[0][0]']          N          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 3938435 (15.02 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 3938435 (15.02 MB)\n",
            "_____________________________________________________________________________________________________________\n",
            "\u001b[1m\u001b[34m\n",
            "Combined_model summary (TDCGAN):\n",
            "\u001b[0m \n",
            "Model: \"Combined_Model_TDCGAN\"\n",
            "_____________________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  Trainable  \n",
            "=============================================================================================================\n",
            " noise_data (InputLayer)     [(None, 7352)]               0         []                            Y          \n",
            "                                                                                                             \n",
            " labels (InputLayer)         [(None, 2)]                  0         []                            Y          \n",
            "                                                                                                             \n",
            " generator (Functional)      (None, 7352)                 2168728   ['noise_data[0][0]',          Y          \n",
            "                                                                     'labels[0][0]']                         \n",
            "                                                                                                             \n",
            " discriminator_0 (Functiona  [(None, 2),                  756003    ['generator[0][0]',           N          \n",
            " l)                           (None, 1)]                             'labels[0][0]']                         \n",
            "                                                                                                             \n",
            " discriminator_1 (Functiona  [(None, 2),                  1172035   ['generator[0][0]',           N          \n",
            " l)                           (None, 1)]                             'labels[0][0]']                         \n",
            "                                                                                                             \n",
            " discriminator_2 (Functiona  [(None, 2),                  3938435   ['generator[0][0]',           N          \n",
            " l)                           (None, 1)]                             'labels[0][0]']                         \n",
            "                                                                                                             \n",
            " concatenate_34 (Concatenat  (None, 6)                    0         ['discriminator_0[0][0]',     Y          \n",
            " e)                                                                  'discriminator_1[0][0]',                \n",
            "                                                                     'discriminator_2[0][0]']                \n",
            "                                                                                                             \n",
            " concatenate_35 (Concatenat  (None, 3)                    0         ['discriminator_0[0][1]',     Y          \n",
            " e)                                                                  'discriminator_1[0][1]',                \n",
            "                                                                     'discriminator_2[0][1]']                \n",
            "                                                                                                             \n",
            " comb_class_output (Dense)   (None, 2)                    14        ['concatenate_34[0][0]']      Y          \n",
            "                                                                                                             \n",
            " comb_validity_output (Dens  (None, 1)                    4         ['concatenate_35[0][0]']      Y          \n",
            " e)                                                                                                          \n",
            "                                                                                                             \n",
            "=============================================================================================================\n",
            "Total params: 8035219 (30.65 MB)\n",
            "Trainable params: 2168746 (8.27 MB)\n",
            "Non-trainable params: 5866473 (22.38 MB)\n",
            "_____________________________________________________________________________________________________________\n",
            "\u001b[1m\u001b[30m------------ train_model ------------\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch:   0%|          | 0/500 [00:00<?, ?epoch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ac2308269ba48d5942d8460677faf8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-2227f155edc1>\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m       \u001b[0;31m# 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mnoise_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m one_hot_encoder = train_model(BATCH_SIZE, noise_dim, num_classes, generator_model, discriminators_model, combined_model,\n\u001b[0m\u001b[1;32m     71\u001b[0m                       X_train, y_train, NUM_EPOCHS)\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-97e888e7ae1c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(batch_size, noise_dim, num_classes, generator_model, discriminators_model, combined_model, X_train, y_train, NUM_EPOCHS)\u001b[0m\n\u001b[1;32m     86\u001b[0m       \u001b[0;31m# -------->   Disc 1   <--------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;31m#print('4 disc 1')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0mdisc_loss_real_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminators_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'auxiliary_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreal_labels1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validity_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0mdisc_loss_fake_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminators_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerated_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'auxiliary_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreal_labels1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validity_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfake\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0md_loss_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_loss_real_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_loss_fake_1\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#take average loss from real and fake images.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2785\u001b[0m             )\n\u001b[1;32m   2786\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       return api.converted_call(\n\u001b[0m\u001b[1;32m     42\u001b[0m           \u001b[0moriginal_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 )\n\u001b[1;32m   1383\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m             outputs = reduce_per_replica(\n\u001b[1;32m   1386\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1679\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1680\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1681\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3269\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3270\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3271\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3273\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   4067\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4068\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4069\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4071\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m                 \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_target_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0;31m# Run backwards pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \"\"\"\n\u001b[1;32m    543\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_current_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_gradients_aggregation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexperimental_aggregate_gradients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply_weight_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_weight_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_apply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;31m# Apply variable constraints after applying gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_apply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m         return tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[0m\u001b[1;32m   1254\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_apply_gradients_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/merge_call_interim.py\u001b[0m in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \"\"\"\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     return distribute_lib.get_replica_context().merge_call(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m             distribution.extended.update(\n\u001b[0m\u001b[1;32m   1346\u001b[0m                 \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3014\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3015\u001b[0;31m       return self._replica_ctx_update(\n\u001b[0m\u001b[1;32m   3016\u001b[0m           var, fn, args=args, kwargs=kwargs, group=group)\n\u001b[1;32m   3017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_replica_ctx_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2892\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2894\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mreplica_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_gather_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3484\u001b[0m     merge_fn = autograph.tf_convert(\n\u001b[1;32m   3485\u001b[0m         merge_fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 3486\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3488\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3491\u001b[0m         _CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[1;32m   3492\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3493\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3494\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3495\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_fn\u001b[0;34m(_, *merged_args, **merged_kwargs)\u001b[0m\n\u001b[1;32m   2890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2891\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmerged_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmerged_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2892\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreplica_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3011\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m   3012\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3014\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3015\u001b[0m       return self._replica_ctx_update(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4081\u001b[0m     \u001b[0;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4082\u001b[0m     \u001b[0;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4083\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4085\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4087\u001b[0m     \u001b[0;31m# once that value is used for something.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4088\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4089\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4090\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4091\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1340\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_step_xla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_update_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0;34mf\"`tf.keras.optimizers.legacy.{self.__class__.__name__}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             )\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/adam.py\u001b[0m in \u001b[0;36mupdate_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_velocities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta_2_power\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta_1_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexedSlices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mr_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m   1511\u001b[0m       \u001b[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_same_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1513\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m   \u001b[0;31m# Propagate func.__doc__ to the wrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36msubtract\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"math.subtract\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"subtract\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_binary_elementwise_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}