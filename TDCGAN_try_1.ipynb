{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AU3gkhF1vDiU",
        "YP7kzVcWvQJS",
        "d8JZwxbdvY02",
        "lpxdVQOE0hN8",
        "YeBK_kzH8wFy",
        "Cn3GLPi28Dzq",
        "cywdPaotCTSe",
        "aRX63PzwXaX-",
        "UHnSWUeKtnCZ",
        "6lhMl8UJuZwl",
        "gd-1Os0wvyg1",
        "blzcqzz13WEr",
        "g0Ftjen4hT3n",
        "8kHvObl8iY6q",
        "Jk5eOJg8bDNj",
        "7msjiV9Gil15",
        "23KIY8XptvUZ",
        "aq6EPGPqx6n5",
        "jf78A8IV2T8g",
        "20ZTUhX-gDI-",
        "fTf8nmXNgGMt"
      ],
      "authorship_tag": "ABX9TyO7/Vfrcj+oiIOF3mBiYLaU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SolinSM/TDCGAN/blob/main/TDCGAN_try_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **For Dataset**"
      ],
      "metadata": {
        "id": "AU3gkhF1vDiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Compare sentence Column - Dataset**\n",
        "\n",
        "*   Used with our-dataset security course\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e_zQ04S4vHHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Compare 'sentence' column in 2 excel file**"
      ],
      "metadata": {
        "id": "YP7kzVcWvQJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"  ##### In 2 excel Files ##### \"\"\"\n",
        "file_1 = '/content/SQLiDataset.xlsx'\n",
        "file_2 = '/content/c_our_Dataset-old.xlsx'\n",
        "\n",
        "df1 = pd.read_excel(file_1)\n",
        "df2 = pd.read_excel(file_2)\n",
        "\n",
        "dff1 = pd.DataFrame()\n",
        "dff1['sentence_1'] = df1['sentence']\n",
        "dff1['sentence_2'] = df2['sentence']\n",
        "\n",
        "# use equal to compare when index not same in files\n",
        "#dff1['diff'] = np.where(df1['sentence'] == df2['sentence'], 0,1)\n",
        "dff1['diff'] = np.where( df1['sentence'].equals(df2['sentence']) , 0,1)\n",
        "\n",
        "dff1.to_excel('/content/new_SQLiDataset.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "iLfVm8JtvMSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Compare 'sentence' in 2 column in same excel file**"
      ],
      "metadata": {
        "id": "d8JZwxbdvY02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "file_2 = '/content/new_SQLiDataset.xlsx'\n",
        "\n",
        "df2 = pd.read_excel(file_2)\n",
        "\n",
        "dff1 = pd.DataFrame()\n",
        "\n",
        "dff1['sentence_1'] = df2['sentence_1']\n",
        "dff1['sentence_2'] = df2['sentence_2']\n",
        "dff1['diff'] = np.where(df2['sentence_1'] == df2['sentence_2'], 0,1)\n",
        "\n",
        "dff1.to_excel('/content/new_SQLiDataset_1.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "J5lJQ776ve5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Read Specific Column (Sentence) + add new Column \"len_payload\" + values**"
      ],
      "metadata": {
        "id": "lpxdVQOE0hN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas library\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define CSV File Path\n",
        "file_path =  '/content/s_SQLiDataset.csv'\n",
        "#file_path =  '/content/s_SQLiDataset.xlsx'\n",
        "\n",
        "# Step 2: Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(file_path, encoding = \"ISO-8859-1\" , usecols=['sentence'])\n",
        "#df = pd.read_excel(file_path)\n",
        "\n",
        "total_rows_befor = len(df)\n",
        "\n",
        "# ## show head 10 of data before adding new column\n",
        "# print('----------------------')\n",
        "# print('Before adding new column')\n",
        "# print(df.head())\n",
        "# print('----------------------')\n",
        "\n",
        "# Step 3: Add the new \"len_payload\" column to the DataFrame and dataType it as int\n",
        "df['len_payload'] = 0\n",
        "df = df.astype({'len_payload': int})\n",
        "\n",
        "# Step 4: Set length payload to a new added column \"len_payload\"\n",
        "# iterating over rows using iterrows() function\n",
        "for i, j in df.iterrows():\n",
        "\n",
        "    # Step 4.1: Get 'sentence' value\n",
        "    sentence = df['sentence'][i]\n",
        "\n",
        "    # Step 4.2: Drop row with null payload\n",
        "    if pd.isna(sentence):\n",
        "      print('index sentence with value nan : ', i)\n",
        "      df.drop( labels = i, axis=0, inplace = True )\n",
        "\n",
        "    else:\n",
        "      #print('index : ', i)\n",
        "      # Step 4.3: Get the payload value and strip White Spaces\n",
        "      sentence = str(sentence).strip()\n",
        "\n",
        "      # Step 4.4: Find the sentence length\n",
        "      len_payload = len(sentence)\n",
        "\n",
        "      # Step 4.5: Update value of \"len_payload\" payload/sentence length\n",
        "      df.loc[i, 'len_payload'] = len_payload\n",
        "\n",
        "\n",
        "# Step 5: Write the DataFrame back to the CSV file\n",
        "df.to_csv(file_path, index=False)\n",
        "#df.to_excel('/content/new_len_SQLiDataset_1.xlsx', index=False)\n",
        "\n",
        "# Check length file\n",
        "print('----------------------')\n",
        "total_rows_after = len(df)\n",
        "print('Total rows total_rows_befor: ', total_rows_befor)\n",
        "print('Total rows total_rows_after: ', total_rows_after)\n",
        "\n",
        "\n",
        "## Check new data is added\n",
        "print('----------------------')\n",
        "print('Aefore adding new column')\n",
        "pd.read_csv(file_path, encoding = \"ISO-8859-1\").head()\n",
        "#pd.read_excel(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LZgEDBHU0i75",
        "outputId": "7c0943f2-9e2b-4548-e83b-9a6123202d27"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index nan :  12211\n",
            "index nan :  12212\n",
            "index nan :  12213\n",
            "----------------------\n",
            "Total rows total_rows_befor:  39688\n",
            "Total rows total_rows_after:  39685\n",
            "----------------------\n",
            "Aefore adding new column\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sentence  len_payload\n",
              "0     -3752            5\n",
              "1  02:00 AM            8\n",
              "2         0            1\n",
              "3       21%            3\n",
              "4       26%            3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-77c85aba-41ca-474d-8360-2399eafd7246\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>len_payload</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-3752</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>02:00 AM</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>21%</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>26%</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77c85aba-41ca-474d-8360-2399eafd7246')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-77c85aba-41ca-474d-8360-2399eafd7246 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-77c85aba-41ca-474d-8360-2399eafd7246');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c257dabf-0d3c-4782-bc46-4b736583792e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c257dabf-0d3c-4782-bc46-4b736583792e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c257dabf-0d3c-4782-bc46-4b736583792e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"#pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"02:00 AM\",\n          \"26%\",\n          \"0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len_payload\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 8,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          8,\n          3,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decode Payload**"
      ],
      "metadata": {
        "id": "YeBK_kzH8wFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import urllib.parse\n",
        "\n",
        "### ------------- Decode function ------------- ###\n",
        "def unicode_decode(str):\n",
        "  return str.encode('utf-8').decode('unicode-escape')\n",
        "\n",
        "def url_decode(str):\n",
        "  return urllib.parse.unquote(str)\n",
        "\n",
        "def base64_decode(base64_string):\n",
        "  base64_bytes = base64_string.encode(\"ascii\")\n",
        "  sample_string_bytes = base64.b64decode(base64_bytes)\n",
        "  sample_str = sample_string_bytes.decode(\"ascii\")\n",
        "  return sample_str\n",
        "\n",
        "def unicode_url_decoding(sql_payload):\n",
        "  if '%20' in sql_payload or '%' in sql_payload:\n",
        "    try:\n",
        "      d_payload = url_decode(sql_payload)\n",
        "    except: # Exception as e:\n",
        "      #print(f\"Decoding error in - unicode_url_decoding() - with: {e}\")\n",
        "      d_payload = sql_payload\n",
        "\n",
        "  elif sql_payload.startswith(('\\\\u', '\\\\U')):\n",
        "    try:\n",
        "      d_payload = unicode_decode(sql_payload)\n",
        "    except: # Exception as e:\n",
        "      d_payload = sql_payload\n",
        "\n",
        "  else:\n",
        "    d_payload = sql_payload\n",
        "\n",
        "  return d_payload\n",
        "\n",
        "\n",
        "\n",
        "### ------------- Decode function ------------- ###\n",
        "def decode_payload(sql_payload):\n",
        "\n",
        "  if sql_payload[-1] == '=' and sql_payload[-2] == '=':\n",
        "    try:\n",
        "      base64_payload = base64_decode(sql_payload)\n",
        "      if base64_payload:\n",
        "        decode_payload = unicode_url_decoding(base64_payload)\n",
        "      else:\n",
        "        decode_payload = unicode_url_decoding(sql_payload)\n",
        "    except Exception as e:\n",
        "        decode_payload = unicode_url_decoding(sql_payload)\n",
        "\n",
        "  else:\n",
        "    sql_payload += '=='\n",
        "    try:\n",
        "      base64_payload = base64_decode(sql_payload)\n",
        "      if base64_payload:\n",
        "        decode_payload = unicode_url_decoding(base64_payload)\n",
        "      else:\n",
        "        decode_payload = unicode_url_decoding(sql_payload)\n",
        "    except Exception as e:\n",
        "      sql_payload = sql_payload[:-2]      # remove last 2 chat == added in this section\n",
        "      decode_payload = unicode_url_decoding(sql_payload)\n",
        "\n",
        "  return decode_payload\n",
        "\n",
        "\n",
        "### ------------- main ------------- ###\n",
        "# Example usage:\n",
        "sql_payload = \"SELECT%20*%20FROM%20users%20WHERE%20username%3D%27john%27%3B\"    # url encoded\n",
        "sql_payload = \"U0VMRUNUICogRlJPTSB1c2VycyBXSEVSRSB1c2VybmFtZT0nam9obic7==\"      # base64 encoded\n",
        "sql_payload = \"\\\\u0053\\\\u0045\\\\u004C\\\\u0045\\\\u0043\\\\u0054\\\\u0020\\\\u002A\\\\u0020\\\\u0046\\\\u0052\\\\u004F\\\\u004D\\\\u0020\\\\u0075\\\\u0073\\\\u0065\\\\u0072\\\\u0073\\\\u0020\\\\u0057\\\\u0048\\\\u0045\\\\u0052\\\\u0045\\\\u0020\\\\u0075\\\\u0073\\\\u0065\\\\u0072\\\\u006E\\\\u0061\\\\u006D\\\\u0065\\\\u003D\\\\u0027\\\\u006A\\\\u006F\\\\u0068\\\\u006E\\\\u0027\\\\u003B\"   # unicode encoded\n",
        "sql_payload = \"U0VMRUNUJTIwKiUyMEZST00lMjB1c2VycyUyMFdIRVJFJTIwdXNlcm5hbWUlM0QlMjdqb2huJTI3JTNC\"   # base64 + url encoded\n",
        "sql_payload = \"XHUwMDUzXHUwMDQ1XHUwMDRDXHUwMDQ1XHUwMDQzXHUwMDU0XHUwMDIwXHUwMDJBXHUwMDIwXHUwMDQ2XHUwMDUyXHUwMDRGXHUwMDREXHUwMDIwXHUwMDc1XHUwMDczXHUwMDY1XHUwMDcyXHUwMDczXHUwMDIwXHUwMDU3XHUwMDQ4XHUwMDQ1XHUwMDUyXHUwMDQ1XHUwMDIwXHUwMDc1XHUwMDczXHUwMDY1XHUwMDcyXHUwMDZFXHUwMDYxXHUwMDZEXHUwMDY1XHUwMDNEXHUwMDI3XHUwMDZBXHUwMDZGXHUwMDY4XHUwMDZFXHUwMDI3XHUwMDNC\"  # base64 + unicode encoded\n",
        "print(\"Befor Decoded payload:\", sql_payload)\n",
        "\n",
        "decoded_payload = decode_payload(sql_payload)\n",
        "if decoded_payload:\n",
        "    print(\"After Decoded payload:\", decoded_payload)\n",
        "else:\n",
        "    print(\"Unable to decode payload using any method.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lXyRmiU9W5C",
        "outputId": "bfbc69de-14a2-4b36-cec6-a2f8caa02a1e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Befor Decoded payload: XHUwMDUzXHUwMDQ1XHUwMDRDXHUwMDQ1XHUwMDQzXHUwMDU0XHUwMDIwXHUwMDJBXHUwMDIwXHUwMDQ2XHUwMDUyXHUwMDRGXHUwMDREXHUwMDIwXHUwMDc1XHUwMDczXHUwMDY1XHUwMDcyXHUwMDczXHUwMDIwXHUwMDU3XHUwMDQ4XHUwMDQ1XHUwMDUyXHUwMDQ1XHUwMDIwXHUwMDc1XHUwMDczXHUwMDY1XHUwMDcyXHUwMDZFXHUwMDYxXHUwMDZEXHUwMDY1XHUwMDNEXHUwMDI3XHUwMDZBXHUwMDZGXHUwMDY4XHUwMDZFXHUwMDI3XHUwMDNC\n",
            "After Decoded payload: SELECT * FROM users WHERE username='john';\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Processing**\n",
        "* Input text String\n",
        "* Remove white spaces from start & end in text, using strip()\n",
        "* Remove accents\n",
        "* Convert all letters of the string to lower case\n",
        "* Tokenization\n",
        "* Remove stop words, and except some important words used in queries\n",
        "\n",
        "ref code: [https://www.geeksforgeeks.org/python-ai/?ref=shm](https://www.geeksforgeeks.org/python-ai/?ref=shm)"
      ],
      "metadata": {
        "id": "AiSUty8p8TGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stop words + Tokenization**"
      ],
      "metadata": {
        "id": "Cn3GLPi28Dzq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-yHYKc4722S",
        "outputId": "9a558ca8-d739-46f4-87f6-9dbbcde99bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['select', '*', 'from', 'users', 'where', 'username', \"='\", 'john', \"'\", 'and', 'fname', \"='\", 'john', \"';\", 'is']\n",
            "------\n",
            "['select', '*', 'from', 'users', 'where', 'username', \"='\", 'john', \"'\", 'and', 'fname', \"='\", 'john', \"';\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "#import unicodedata\n",
        "#import string\n",
        "\n",
        "#from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer      ## WordPunctTokenizer --> splits words based on punctuation boundaries.\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Filters stop words from a list of tokens, keeping words in the exception list.\n",
        "\n",
        "Args:\n",
        "    word_tokens: A list of tokens (words).\n",
        "    except_stop_word: A set of stop words to be excluded from removal (optional).\n",
        "\n",
        "Returns:\n",
        "    A list of tokens with stop words removed, except for those in the exception list.\n",
        "\"\"\"\n",
        "def filter_stop_words(word_tokens):\n",
        "    stop_words = set(stopwords.words('english'))  # Load default stop words\n",
        "    except_stop_word = set(['and', 'or', 'where', 'from', 'over'])  # Exception list defined here\n",
        "\n",
        "    filtered_sentence = []\n",
        "    for w in word_tokens:\n",
        "        if w in except_stop_word or w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "\n",
        "    return filtered_sentence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "text = \"     SELECT * FROM users WHERE username='john' and    fname='john'; is\"\n",
        "\n",
        "# convert to lower case\n",
        "text = text.lower()\n",
        "\n",
        "# Remove spaces in start and end of sentence\n",
        "text = text.strip()\n",
        "\n",
        "# Remove accents function such as é\n",
        "#text = ''.join(x for x in unicodedata.normalize('NFKD', text) if x in string.ascii_letters or x == \" \")\n",
        "\n",
        "#word_tokens = word_tokenize(text)\n",
        "tokenizer   = WordPunctTokenizer()\n",
        "word_tokens = tokenizer.tokenize(text)\n",
        "\n",
        "# stop_words  (filter tokeniz word by remove any stop word with except some word as from, where ... etc)\n",
        "filtered_tokens = filter_stop_words(word_tokens)\n",
        "\n",
        "print(word_tokens)\n",
        "print('------')\n",
        "print(filtered_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define CSV file path (replace with your actual path)\n",
        "csv_path = \"your_data.csv\"\n",
        "\n",
        "# Stop words with exceptions\n",
        "stop_words = set(stopwords.words('english'))\n",
        "except_stop_word = ['and', 'or', 'where', 'from', 'over']\n",
        "\n",
        "# Function for text pre-processing (optional)\n",
        "\"\"\"\n",
        "Preprocesses text data by performing various cleaning steps.\n",
        "\n",
        "Args:\n",
        "    text: The text string to be preprocessed.\n",
        "\n",
        "Returns:\n",
        "    The preprocessed text string.\n",
        "\"\"\"\n",
        "def preprocess_text(text):\n",
        "  # Convert to lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  # Remove spaces in start and end\n",
        "  text = text.strip()\n",
        "\n",
        "  # Remove accents (optional)\n",
        "  # text = ''.join(x for x in unicodedata.normalize('NFKD', text) if x in string.ascii_letters or x == \" \")\n",
        "\n",
        "  return text\n",
        "\n",
        "# Read data from CSV using pandas\n",
        "data = pd.read_csv(csv_path)\n",
        "\n",
        "# Get sentences as a list\n",
        "sentences = data[\"sentence\"].apply(preprocess_text)  # Apply pre-processing\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = WordPunctTokenizer()\n",
        "tokenized_sentences = []\n",
        "for sentence in sentences:\n",
        "  word_tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "  # Filter stop words (considering exceptions)\n",
        "  filtered_sentence = []\n",
        "  for w in word_tokens:\n",
        "    if w not in except_stop_word and w not in stop_words:\n",
        "      filtered_sentence.append(w)\n",
        "\n",
        "  tokenized_sentences.append(filtered_sentence)\n",
        "\n",
        "# Create vocabulary\n",
        "all_tokens = [token for sentence in tokenized_sentences for token in sentence]\n",
        "vocabulary = set(all_tokens)\n",
        "\n",
        "# One-hot encoding (using scikit-learn's OneHotEncoder)\n",
        "encoder = OneHotEncoder(sparse=False)  # Dense output for easier visualization\n",
        "encoded_sentences = []\n",
        "for tokens in tokenized_sentences:\n",
        "  encoded_sentences.append(encoder.fit_transform(np.array([tokens]).reshape(1, -1))[0])\n",
        "\n",
        "# Print results (optional)\n",
        "for i, sentence in enumerate(sentences):\n",
        "  print(f\"Sentence {i+1}:\", sentence)\n",
        "  print(\"Tokens:\", tokenized_sentences[i])\n",
        "  print(\"One-hot encoded:\", encoded_sentences[i])\n",
        "  print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "9_vKJBudWVPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read Multi Files csv (done below)**"
      ],
      "metadata": {
        "id": "cywdPaotCTSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "\"\"\" ----------------- Read CSV File Function ----------------- \"\"\"\n",
        "def read_csv_files(dataset_directory, percent):\n",
        "  files = [f for f in os.listdir(dataset_directory) if f.endswith('.csv')]\n",
        "\n",
        "  if files == []:\n",
        "    print('Not found any csv files')\n",
        "  else:\n",
        "    print('Your files are: ', files)\n",
        "\n",
        "    np_array_values = []\n",
        "    data_df = pd.DataFrame()\n",
        "    firstFile = True\n",
        "\n",
        "    for file in files:\n",
        "      file_path = os.path.join(dataset_directory, file)   # csv_file_path\n",
        "      print('File Path: ', file_path)\n",
        "\n",
        "      try:\n",
        "        df = ''\n",
        "        df = pd.read_csv(file_path, encoding = \"ISO-8859-1\")  #.head()   #,low_memory=False   ISO-8859-1\n",
        "        total_rows = len(df)\n",
        "        print('Total rows in df/file: ', total_rows)\n",
        "\n",
        "        num_rows = int(total_rows * (percent / 100))\n",
        "        print('Total rows in df/file 100%: ', num_rows)\n",
        "\n",
        "        \"\"\" Start From Net Code \"\"\"\n",
        "        ##np_array_values.append(df)\n",
        "        #print('np_array_values: ' , np_array_values)\n",
        "\n",
        "        ###merge_values = np.vstack(np_array_values)\n",
        "        #print('merge_values: ', merge_values)\n",
        "\n",
        "        #new_data = pd.DataFrame(merge_values)\n",
        "        #new_data = pd.DataFrame(merge_values , columns=['sentence','attack_type_code', 'attack_type', 'http_method', 'param_as', 'flag_success_attack', 'http_status', 'param_length'])\n",
        "        ###new_data = pd.DataFrame(merge_values , columns=['sentence','attack_type_code', 'param_length'])\n",
        "        ###print('new_data: ' , new_data)\n",
        "        \"\"\" End From Net Code \"\"\"\n",
        "\n",
        "        \"\"\" Start From Teacher Code \"\"\"\n",
        "        # Generate a list of random indices\n",
        "        random_indices = random.sample(range(total_rows), num_rows)\n",
        "        #print('random_indices: ' , random_indices)\n",
        "\n",
        "        # Select the random rows from the DataFrame\n",
        "        temp_df = df.iloc[random_indices]\n",
        "        if(firstFile):\n",
        "          # Concatenate all DataFrames into one\n",
        "          data_df = temp_df.copy()\n",
        "          firstFile = False\n",
        "        else:\n",
        "          # Concatenate all DataFrames into one\n",
        "          data_df = pd.concat([data_df,temp_df], ignore_index=True)\n",
        "\n",
        "        print(data_df)\n",
        "        return data_df\n",
        "        \"\"\" End From Teacher Code \"\"\"\n",
        "\n",
        "        ## Add DataFrame to new CSV file\n",
        "        #new_csv_file_path = os.path.join(dataset_directory, 'new_sqli.csv')  # \"/content/dataset/new_sqli.csv\"\n",
        "        #df.to_csv(new_csv_file_path, index=False)\n",
        "\n",
        "      except Exception as e:\n",
        "        print('Can not Read File called : ', file)\n",
        "        print('File path: ', file_path)\n",
        "        print(\"Errpr Exception e : \", e)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" ----------------- Apply Code ----------------- \"\"\"\n",
        "dataset_directory = \"/content/datasets\"   #files_path\n",
        "percent = 100\n",
        "data = read_csv_files(dataset_directory,percent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3mh6j-CCYfr",
        "outputId": "a141b356-f736-42ff-b424-14dac096aa69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your files are:  ['SQLiDataset_2.csv']\n",
            "File Path:  /content/datasets/SQLiDataset_2.csv\n",
            "Total rows in df/file:  39685\n",
            "Total rows in df/file 100%:  39685\n",
            "                                                sentence  attack_type  \\\n",
            "28661                                do cotaredo, 147 9e            0   \n",
            "38839                   vidom.kapatos@discotecateatre.om            0   \n",
            "6589    select * from users where id = 1 or \" )  ( \" ...            1   \n",
            "17838  1%' or 4915 =  ( select count ( * )  from doma...            1   \n",
            "18982                1'+ ( select idnb where 2338 = 2338            1   \n",
            "...                                                  ...          ...   \n",
            "12402  1'  )  )   )  and 6537 = dbms_pipe.receive_mes...            1   \n",
            "1083   '  )  )   )  UNION ALL SELECT NULL, NULL, NULL...            1   \n",
            "31117                                          incr622to            0   \n",
            "34798                                        pradoluengo            0   \n",
            "10326  \" )  AND 7767 = CAST ( CHR ( 58 ) ||CHR ( 99 )...            1   \n",
            "\n",
            "       len_payload  \n",
            "28661           19  \n",
            "38839           32  \n",
            "6589            62  \n",
            "17838          108  \n",
            "18982           35  \n",
            "...            ...  \n",
            "12402          140  \n",
            "1083           318  \n",
            "31117            9  \n",
            "34798           11  \n",
            "10326          269  \n",
            "\n",
            "[39685 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read random n row**"
      ],
      "metadata": {
        "id": "11p3SsjUYLNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ----------------- Read Random n rows Function ----------------- \"\"\"\n",
        "def read_random_n_rows(df, n, percent):\n",
        "    # Read the entire CSV file into a DataFrame\n",
        "    #df = read_csv_files(path,percent)\n",
        "\n",
        "    # Check if requested sample size is greater than the total number of rows\n",
        "\t# n --> requested sample size\n",
        "\t# len(df) --> total number of rows\n",
        "    if n >= len(df):\n",
        "        return df\n",
        "\n",
        "    # Randomly select n rows from the DataFrame\n",
        "    random_sample = df.sample(n=n, random_state=42)\n",
        "    random_sample.reset_index(drop=True,inplace=True)\n",
        "    return random_sample\n",
        "\n",
        "\n",
        "\"\"\" ----------------- Apply Read Random n rows Function ----------------- \"\"\"\n",
        "nrows = 12000\n",
        "random_sample = read_random_n_rows(data, nrows, percent)\n",
        "random_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "oEpOY8w1YPc_",
        "outputId": "38e73ba7-6f86-4aa7-ae7f-b4841e04a33f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                sentence  attack_type  \\\n",
              "0                          UNION ALL SELECT 1,2,3,4,5,6#            1   \n",
              "1       The man &apos;s distraught nine-year-old son ...            0   \n",
              "2      html M Series United States Deluxe Laminated 5...            0   \n",
              "3       It attractive able determine daily working ro...            0   \n",
              "4      ' AND 9263 =  ( SELECT 9263 FROM PG_SLEEP ( 5 ...            1   \n",
              "...                                                  ...          ...   \n",
              "11995                      c/ veinat de cerdanyola, 3 6e            0   \n",
              "11996                                   naredo reggiardo            0   \n",
              "11997  ' )  UNION ALL SELECT NULL, NULL, NULL, NULL, ...            1   \n",
              "11998                               hornillos del camino            0   \n",
              "11999              -2411 or  ( 8459 = 8459 ) *4906# pxwg            1   \n",
              "\n",
              "       len_payload  \n",
              "0               29  \n",
              "1              128  \n",
              "2               62  \n",
              "3               50  \n",
              "4               51  \n",
              "...            ...  \n",
              "11995           29  \n",
              "11996           16  \n",
              "11997          220  \n",
              "11998           20  \n",
              "11999           37  \n",
              "\n",
              "[12000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7087a3d7-7a88-4fce-86ef-a1db041d31d4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>attack_type</th>\n",
              "      <th>len_payload</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>UNION ALL SELECT 1,2,3,4,5,6#</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The man &amp;apos;s distraught nine-year-old son ...</td>\n",
              "      <td>0</td>\n",
              "      <td>128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>html M Series United States Deluxe Laminated 5...</td>\n",
              "      <td>0</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It attractive able determine daily working ro...</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>' AND 9263 =  ( SELECT 9263 FROM PG_SLEEP ( 5 ...</td>\n",
              "      <td>1</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11995</th>\n",
              "      <td>c/ veinat de cerdanyola, 3 6e</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11996</th>\n",
              "      <td>naredo reggiardo</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11997</th>\n",
              "      <td>' )  UNION ALL SELECT NULL, NULL, NULL, NULL, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11998</th>\n",
              "      <td>hornillos del camino</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11999</th>\n",
              "      <td>-2411 or  ( 8459 = 8459 ) *4906# pxwg</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12000 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7087a3d7-7a88-4fce-86ef-a1db041d31d4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7087a3d7-7a88-4fce-86ef-a1db041d31d4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7087a3d7-7a88-4fce-86ef-a1db041d31d4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d6b944c7-9a73-4c19-9ff9-8c1a4a60c63d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d6b944c7-9a73-4c19-9ff9-8c1a4a60c63d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d6b944c7-9a73-4c19-9ff9-8c1a4a60c63d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_392a9980-ad99-4b51-9095-ef790e27dc14\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('random_sample')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_392a9980-ad99-4b51-9095-ef790e27dc14 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('random_sample');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "random_sample",
              "summary": "{\n  \"name\": \"random_sample\",\n  \"rows\": 12000,\n  \"fields\": [\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11829,\n        \"samples\": [\n          \" On 15 March , anniversary year officially launched , Geisingen currently suitable venues , launch event take place location two towns first documented 1,250 years ago , Kirchen-Hausen \",\n          \"\\\" AND 5465 = 8501--\",\n          \" select * from users where id = 1 or \\\",.\\\" or 1 = 1 -- 1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"attack_type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"len_payload\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95,\n        \"min\": 1,\n        \"max\": 1701,\n        \"num_unique_values\": 381,\n        \"samples\": [\n          357,\n          245\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Up Classes (not work)**"
      ],
      "metadata": {
        "id": "aRX63PzwXaX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"  No Neeeed (Long Time + 23 min & not end loop)   \"\"\"\n",
        "\n",
        "\"\"\" ----------------- Read Up Class Function ----------------- \"\"\"\n",
        "\n",
        "def upclasses(df,label,num):\n",
        "  #calculate the occurrences of each class and identify classes with fewer\n",
        "  #instances than the specified minimum (min_instances_per_class).\n",
        "  #We then duplicate rows for those classes using random sampling.\n",
        "  # Set the minimum number of instances per class\n",
        "  min_instances_per_class = num\n",
        "\n",
        "  # Count the occurrences of each class in the original labels\n",
        "  class_counts = df[label].value_counts()\n",
        "  #print('Count the occurrences of each class in the original labels: ' , class_counts)        # list of sentence and at end --> Name: count, Length: 38025, dtype: int64\n",
        "\n",
        "  # Identify classes with fewer instances than the minimum\n",
        "  classes_to_duplicate = class_counts[class_counts < min_instances_per_class].index\n",
        "  #print('classes with fewer instances than the minimum (classes_to_duplicate): ' , classes_to_duplicate)\n",
        "\n",
        "  # Create an empty DataFrame to store duplicated rows\n",
        "  df_duplicated = ''\n",
        "  df_duplicated = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "  # Duplicate rows for classes with fewer instances than the minimum\n",
        "  # df[label] : list of all values in column label that selected\n",
        "  # class_label : from classes_to_duplicate\n",
        "  # class_df : if equal then get all values in row index (all column not only selected label)\n",
        "  for class_label in classes_to_duplicate:\n",
        "    class_df = df[df[label] == class_label]\n",
        "\n",
        "    # Duplicate rows using random sampling with replacement (duplicate class_df in num rows where num is param)\n",
        "    duplicated_rows = class_df.sample(n=min_instances_per_class, replace=True, random_state=42)\n",
        "\n",
        "    # Concatenate the duplicated rows to the new DataFrame\n",
        "    df_duplicated = pd.concat([df_duplicated, duplicated_rows])\n",
        "\n",
        "  print('===================')\n",
        "  print('df_duplicated: ', df_duplicated)\n",
        "\n",
        "  # Concatenate the original DataFrame with the duplicated rows\n",
        "  df_balanced = pd.concat([df, df_duplicated], ignore_index=True)\n",
        "  return df_balanced\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" ----------------- Apply Up Class Function ----------------- \"\"\"\n",
        "\n",
        "upclasses(data,'sentence',2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "mXT8rxKCIKrT",
        "outputId": "daed710f-0382-42c2-89f3-f530f11f83a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f47d7f584dc7>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\"\"\" ----------------- Apply Up Class Function ----------------- \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mupclasses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-f47d7f584dc7>\u001b[0m in \u001b[0;36mupclasses\u001b[0;34m(df, label, num)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Concatenate the duplicated rows to the new DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdf_duplicated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_duplicated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduplicated_rows\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'==================='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    383\u001b[0m     )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m             new_data = concatenate_managers(\n\u001b[0m\u001b[1;32m    617\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbm_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m#  we can use np.concatenate, which is more performant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m#  than concat_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;31m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Embeddings in NLP (One-hot encoding)**\n",
        "\n",
        "It is a way to represent words as a numeric representations in a lower-dimensional space and to preserve syntactical and semantic information. It can be used as a method to extracting features out of text, and use them as input into ML model to work with text data.\n",
        "\n",
        "Here, we will use one of a traditional Approachs which is One-hot encoding. It is a simple method used to represent words in NLP. Each word in the vocabulary is represented as a unique vector, where the vector's dimensional is equal to the vocabulary size. This encoding works by set 0 to all elements in the vector, except the element corresponding to the index of the word in the vocabulary, which is set to 1.\n",
        "\n",
        "لماذا هي وليس غيرها\n",
        "تعتمد الأساليب مثل\n",
        " Bag of Words (BOW) وCountVectorizer وTFIDF\n",
        " على عدد الكلمات في الجملة ولكنها لا تحفظ أي معلومات نحوية أو دلالية. في هذه الخوارزميات، حجم المتجه هو عدد العناصر الموجودة في المفردات. يمكننا الحصول على مصفوفة متفرقة إذا كانت معظم العناصر صفرًا. ستعني ناقلات المدخلات الكبيرة عددًا كبيرًا من الأوزان مما سيؤدي إلى عمليات حسابية عالية مطلوبة للتدريب.\n",
        " توفر Word Embeddings حلاً لهذه المشكلات.\n",
        "\n",
        "\n",
        " [reff](https://www.geeksforgeeks.org/word-embeddings-in-nlp/)\n",
        "\n"
      ],
      "metadata": {
        "id": "X8StbS9bxJ2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TfidfVectorizer Function**"
      ],
      "metadata": {
        "id": "UHnSWUeKtnCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = {\n",
        "\t'text': [ 'This is the first doc.', 'this doc is the second doc this nice', 'and the third, the 4', 'solin sm', 'hi doc'],\n",
        "\t'cat':[ 'cat1', 'cat2', 'cat2', 'cat1', 'cat1' ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['cat'] = label_encoder.fit_transform(df['cat'])\n",
        "\n",
        "X = df['text']\n",
        "y = df['cat']\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\t# ,stratify=y\n",
        "#print(f\"X_train shape: {X_train.shape}\")\n",
        "#print(f\"y_train shape: {y_train.shape}\")\n",
        "#print(f\"X_test shape : {X_test.shape}\")\n",
        "#print(f\"y_test shape : {y_test.shape}\")\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_train_tfidf_selected_features = tfidf_vectorizer.get_feature_names_out()\n",
        "#print(\"Shape of X_train_tfidf:\", X_train_tfidf[:5].toarray().shape)\n",
        "#print(\"Shape of X_train_tfidf:\", len(tfidf_vectorizer.get_feature_names_out()) )\n",
        "\n",
        "X_test_tfidf = tfidf_vectorizer.fit_transform(X_test)\n",
        "X_test_tfidf_selected_features = tfidf_vectorizer.get_feature_names_out()\n",
        "#print(\"Shape of X_test_tfidf:\", X_test_tfidf[:5].toarray().shape)\n",
        "#print(\"Shape of X_test_tfidf:\", len(tfidf_vectorizer.get_feature_names_out()))\n",
        "\n",
        "\n",
        "print('Shape of X_train_tfidf : ', X_train_tfidf.shape)\n",
        "print('Shape of X_test_tfidf : ', X_test_tfidf.shape)\n",
        "\n",
        "df_tfidf = pd.DataFrame(X_train_tfidf[:5].toarray(), index = X_train, columns=X_train_tfidf_selected_features)\n",
        "\n",
        "print(df_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mguzGuYHtrbg",
        "outputId": "5f6dff01-d854-4a14-af52-c99c9fc550a4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_tfidf :  (4, 10)\n",
            "Shape of X_test_tfidf :  (1, 6)\n",
            "                            and       doc     first        hi        is  \\\n",
            "text                                                                      \n",
            "hi doc                  0.00000  0.619130  0.000000  0.785288  0.000000   \n",
            "and the third, the 4    0.47212  0.000000  0.000000  0.000000  0.000000   \n",
            "This is the first doc.  0.00000  0.382743  0.485461  0.000000  0.485461   \n",
            "solin sm                0.00000  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "                              sm     solin       the    third      this  \n",
            "text                                                                     \n",
            "hi doc                  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
            "and the third, the 4    0.000000  0.000000  0.744450  0.47212  0.000000  \n",
            "This is the first doc.  0.000000  0.000000  0.382743  0.00000  0.485461  \n",
            "solin sm                0.707107  0.707107  0.000000  0.00000  0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class TDCGAN**\n",
        "\n",
        "*   init definition\n",
        "*   build_generator\n",
        "*   build_discriminator\n",
        "*   build_combined_model\n",
        "*   evaluate_on_real\n",
        "*   generate_synthetic_data    \n",
        "*   plot_losses\n",
        "*   plot_architecture\n",
        "*   print_parameters_and_summary\n",
        "*   train\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EPmL2F3w_DwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TDCGAN Model**"
      ],
      "metadata": {
        "id": "xn8D62ciqzh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **libraries**"
      ],
      "metadata": {
        "id": "6lhMl8UJuZwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for read csv file\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "# pre-process\n",
        "##for stop word\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "## not used now\n",
        "#import unicodedata     # Remove accents\n",
        "#import string\n",
        "\n",
        "## for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer      ## WordPunctTokenizer --> splits words based on punctuation boundaries.\n",
        "\n",
        "## for divide data to (train / test/ validate)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# for one-hor encode (sentence to 2D)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# for TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "\n",
        "# for TDCGAN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    Concatenate,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoevigoMucqs",
        "outputId": "c30fa59c-e56b-4b7b-f1da-f1160c1fba5b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **definition**"
      ],
      "metadata": {
        "id": "gd-1Os0wvyg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GENERATOR_DROPOUT_RATE = 0.1\n",
        "DISCRIMINATOR_DROPOUT_RATE = 0.3      #Adjust the dropout rate to prevent overfitting during training.\n",
        "LEAKY_RELU_ALPA = 0.2\n",
        "\n",
        "NUM_EPOCHS = 1000\n",
        "BATCH_SIZE = 128\n",
        "OPTIMIZER_LR = 0.0001                 # learning rate\n",
        "OPTIMIZER_BETAS = (0.5, 0.999)\n"
      ],
      "metadata": {
        "id": "PycJ6J6zv06N"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Color list + TextStyle**"
      ],
      "metadata": {
        "id": "blzcqzz13WEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors_list = [\n",
        "    'Red', 'Green', 'Blue', 'Purple', 'Orange', 'Pink', 'Brown', 'Yellow',\n",
        "    'Cyan', 'Magenta', 'Lime', 'Teal', 'Lavender', 'Maroon', 'Navy', 'Olive', 'Silver', 'Gold',\n",
        "    'Indigo', 'Turquoise', 'Beige', 'Crimson', 'Salmon','Tan','Lime', 'Fuchsia', 'Plum',\n",
        "    'Tomato', 'Violet']\n",
        "\n",
        "class TextStyle:\n",
        "    # Font Styles\n",
        "    BOLD = '\\033[1m'\n",
        "    DIM = '\\033[2m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "    BLINK = '\\033[5m'\n",
        "    REVERSE = '\\033[7m'\n",
        "    RESET_ALL = '\\033[0m'\n",
        "\n",
        "    # Font Colors\n",
        "    BLACK = '\\033[30m'\n",
        "    RED = '\\033[31m'\n",
        "    GREEN = '\\033[32m'\n",
        "    YELLOW = '\\033[33m'\n",
        "    BLUE = '\\033[34m'\n",
        "    MAGENTA = '\\033[35m'\n",
        "    CYAN = '\\033[36m'\n",
        "    WHITE = '\\033[37m'\n",
        "\n",
        "    # Background Colors\n",
        "    BG_BLACK = '\\033[40m'\n",
        "    BG_RED = '\\033[41m'\n",
        "    BG_GREEN = '\\033[42m'\n",
        "    BG_YELLOW = '\\033[43m'\n",
        "    BG_BLUE = '\\033[44m'\n",
        "    BG_MAGENTA = '\\033[45m'\n",
        "    BG_CYAN = '\\033[46m'\n",
        "    BG_WHITE = '\\033[47m'\n"
      ],
      "metadata": {
        "id": "CKQbM4263W02"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Read Multi Files csv**"
      ],
      "metadata": {
        "id": "g0Ftjen4hT3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ----------------- Read CSV File Function ----------------- \"\"\"\n",
        "def read_csv_files(dataset_directory, percent):\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Handling Read CSV Files ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "  files = [f for f in os.listdir(dataset_directory) if f.endswith('.csv')]\n",
        "\n",
        "  if files == []:\n",
        "    print('Not found any csv files')\n",
        "  else:\n",
        "    print('Your files are: ', files)\n",
        "\n",
        "    np_array_values = []\n",
        "    data_df = pd.DataFrame()\n",
        "    firstFile = True\n",
        "\n",
        "    for file in files:\n",
        "      file_path = os.path.join(dataset_directory, file)   # csv_file_path\n",
        "      print('File Path: ', file_path)\n",
        "\n",
        "      try:\n",
        "        df = ''\n",
        "        df = pd.read_csv(file_path, encoding = \"ISO-8859-1\")  #.head()   #,low_memory=False   ISO-8859-1\n",
        "        total_rows = len(df)\n",
        "        print('Total rows in df/file: ', total_rows)\n",
        "\n",
        "        num_rows = int(total_rows * (percent / 100))\n",
        "        print('Total rows in df/file 100%: ', num_rows)\n",
        "\n",
        "\n",
        "        \"\"\" Start From Teacher Code \"\"\"\n",
        "        # Generate a list of random indices\n",
        "        random_indices = random.sample(range(total_rows), num_rows)\n",
        "        #print('random_indices: ' , random_indices)\n",
        "\n",
        "        # Select the random rows from the DataFrame\n",
        "        temp_df = df.iloc[random_indices]\n",
        "        if(firstFile):\n",
        "          # Concatenate all DataFrames into one\n",
        "          data_df = temp_df.copy()\n",
        "          firstFile = False\n",
        "        else:\n",
        "          # Concatenate all DataFrames into one\n",
        "          data_df = pd.concat([data_df,temp_df], ignore_index=True)\n",
        "\n",
        "        print(data_df)\n",
        "        return data_df\n",
        "        \"\"\" End From Teacher Code \"\"\"\n",
        "\n",
        "        ## Add DataFrame to new CSV file\n",
        "        #new_csv_file_path = os.path.join(dataset_directory, 'new_sqli.csv')  # \"/content/dataset/new_sqli.csv\"\n",
        "        #df.to_csv(new_csv_file_path, index=False)\n",
        "\n",
        "      except Exception as e:\n",
        "        print('Can not Read File called : ', file)\n",
        "        print('File path: ', file_path)\n",
        "        print(\"Errpr Exception e : \", e)\n",
        "\n",
        "\n",
        "#\"\"\" ----------------- Apply Code ----------------- \"\"\"\n",
        "#dataset_directory = \"/content/datasets\"   #files_path\n",
        "#percent = 100\n",
        "#data = read_csv_files(dataset_directory,percent)\n"
      ],
      "metadata": {
        "id": "km03UwAwhdIF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **pre-process data**"
      ],
      "metadata": {
        "id": "GVo6VxdQnc8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Functions to used here**"
      ],
      "metadata": {
        "id": "8kHvObl8iY6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Filters stop words from a list of tokens, keeping words in the exception list.\n",
        "Args:\n",
        "    word_tokens: A list of tokens (words).\n",
        "    except_stop_word: A set of stop words to be excluded from removal (optional).\n",
        "Returns:\n",
        "    A list of tokens with stop words removed, except for those in the exception list.\n",
        "\"\"\"\n",
        "def filter_stop_words(word_tokens):\n",
        "    stop_words = set(stopwords.words('english'))  # Load default stop words\n",
        "    except_stop_word = set(['and', 'or', 'where', 'from', 'over'])  # Exception list defined here\n",
        "\n",
        "    filtered_sentence = []\n",
        "    for w in word_tokens:\n",
        "        if w in except_stop_word or w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "\n",
        "    return filtered_sentence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Tokenization + stop word\n",
        "def tokenization_text(sentences):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Tokenization + stop word ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "    tokenizer = WordPunctTokenizer()\n",
        "    tokenized_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        #word_tokens = word_tokenize(sentence)\n",
        "        word_tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "        # stop_words  (filter tokeniz word by remove any stop word with except some word as from, where ... etc)\n",
        "        filtered_tokens = filter_stop_words(word_tokens)\n",
        "\n",
        "        tokenized_sentences.extend(filtered_tokens)\n",
        "        #tokenized_sentences.append(word_tokens)\n",
        "\n",
        "\n",
        "    # Get unique tokens ( Remove duplicates using set() and convert back to list )\n",
        "    #print('len tokens before unique: ', len(tokenized_sentences))\n",
        "    #print('1 tokens: ', tokenized_sentences )\n",
        "    tokenized_sentences = list(set(tokenized_sentences))\n",
        "    #print('len tokens after unique: ', len(tokenized_sentences))\n",
        "    #print('2 tokens: ', tokenized_sentences )\n",
        "\n",
        "    print(\"tokenized_sentences len:\", len(tokenized_sentences) )\n",
        "    print(\"tokenized_sentences:\", tokenized_sentences)\n",
        "\n",
        "    return tokenized_sentences\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  # Tf-idf Vectorizer (Term Frequency-Inverse Document Frequency Vectorizer)\n",
        "  # Used to convert text data to numerical features\n",
        "  # and get each one weight\n",
        "  # care about frequency of a word within a sentence (term frequency)\n",
        "  # and its rarity across the entire dataset (inverse document frequency)\n",
        "\"\"\"\n",
        "def Tfidf_Vectorizer(tokenized_sentences, X_train, X_test):\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ TfidfVectorizer ------------{TextStyle.RESET_ALL}\")\n",
        "    print('len tokenized_sentences -- : ', len(tokenized_sentences))\n",
        "    tfidf_vectorizer = ''\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "    X_train_tfidf_selected_features = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "    X_test_tfidf = tfidf_vectorizer.fit_transform(X_test)\n",
        "    X_test_tfidf_selected_features = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Checking the shape of the output matrices\n",
        "    print('Shape of X_train_tfidf : ', X_train_tfidf.shape)\n",
        "    print('Shape of X_test_tfidf : ', X_test_tfidf.shape)\n",
        "\n",
        "    #Convert the first 5 vectors of the training set into a Dense DataFrame for better readability\n",
        "    df_X_train_tfidf = pd.DataFrame(X_train_tfidf.toarray(), index = X_train, columns=X_train_tfidf_selected_features)\n",
        "    df_X_test_tfidf  = pd.DataFrame(X_test_tfidf.toarray(), index = X_test, columns=X_test_tfidf_selected_features)\n",
        "    print(df_X_train_tfidf)\n",
        "\n",
        "    return X_train_tfidf, X_test_tfidf, df_X_train_tfidf, df_X_test_tfidf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def one_hot_decoder(df_data, tokenized_sentences):\n",
        "    # One-hot encoding (using scikit-learn's OneHotEncoder)\n",
        "    print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ OneHotEncoder ------------{TextStyle.RESET_ALL}\")\n",
        "    encoder_x = OneHotEncoder(sparse_output=False, categories=[tokenized_sentences])  # Dense output for easier visualization  (sparse to sparse_output to avoid warning remove/depracate)\n",
        "    encoded_sentences = []\n",
        "    encoded_sentences = encoder.fit_transform([[token in sentence for token in tokenized_sentences]\n",
        "                                              for sentence in df_data['sentence']])\n",
        "\n",
        "    # Get feature names\n",
        "    tokens_names = encoder.get_feature_names_out(input_features=tokenized_sentences)\n",
        "\n",
        "    print('tokens_names : ')\n",
        "    print(tokens_names)\n",
        "\n",
        "    # Convert encoded data to DataFrame with column names\n",
        "    encoded_df = pd.DataFrame(encoded_sentences, columns=tokens_names)\n",
        "\n",
        "    # Add sentence as header of row\n",
        "    encoded_df.index = sentences\n",
        "\n",
        "    print('encoded_sentences : ')\n",
        "    print(encoded_sentences)\n",
        "\n",
        "    print('------------------------')\n",
        "    print('encoded_df : ')\n",
        "    print(encoded_df)\n",
        "\n",
        "    return encoded_df\n"
      ],
      "metadata": {
        "id": "Or_DpAOjicTo"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **performs feature selection using PCA (not used yet)**"
      ],
      "metadata": {
        "id": "Jk5eOJg8bDNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\"\"\"\n",
        "performs feature selection using Principal Component Analysis (PCA)\n",
        "Args:\n",
        "  data: X , which is array value of (num_rows, features)\n",
        "  n_components : cols num. without dropd y column (column features)\n",
        "  dataset_name : just name of dataset file\n",
        "Returns:\n",
        "  selected features df\n",
        "\"\"\"\n",
        "def perform_pca(data, n_components,dataset_name):\n",
        "    # Extract feature names\n",
        "    feature_names = data.columns.tolist()\n",
        "    #Scaling Features\n",
        "    print(\"Scaling Features using 'StandardScaler' from scikit-learn\")\n",
        "\n",
        "    \"\"\" Start net code \"\"\"\n",
        "    # Standardize features (optional, but recommended)\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "    # Perform PCA\n",
        "    pca = PCA(n_components=n_components)  # Number of components initially set to all features\n",
        "    pca_result = pca.fit(data_scaled)\n",
        "    \"\"\" End net code \"\"\"\n",
        "\n",
        "    \"\"\" Start Teacher code \"\"\"\n",
        "    #data_scaled = pd.DataFrame(preprocessing.scale(data),columns = data.columns)\n",
        "    #pca = PCA(n_components=n_components)\n",
        "    #pca_result = pca.fit_transform(data_scaled)\n",
        "    \"\"\" End Teacher code \"\"\"\n",
        "\n",
        "\n",
        "    # Calculate explained variance ratio and cumulative variance\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    cumulative_variance      = np.cumsum(explained_variance_ratio)    # Calculates the cumulative explained variance ratio for each component.\n",
        "\n",
        "    \"\"\"  Start net code \"\"\"\n",
        "    # Determine the number of components based on explained variance ratio\n",
        "    total_variance = np.sum(pca.explained_variance_)\n",
        "    explained_variance_ratio = cumulative_variance / total_variance\n",
        "    num_components = np.searchsorted(explained_variance_ratio, explained_variance_ratio) + 1\n",
        "\n",
        "    # Select features based on explained variance ratio threshold\n",
        "    selected_features = pca.components_[:, :num_components]\n",
        "    print(f\"{TextStyle.BOLD}Selected_features:{TextStyle.RESET_ALL} \\n\")\n",
        "\n",
        "    # Transform data using selected components\n",
        "    X_reduced = pca.transform(data_scaled)[:, :num_components]\n",
        "    print('X_reduced : ', X_reduced)\n",
        "    \"\"\" End net code \"\"\"\n",
        "\n",
        "    \"\"\" start Teacher code \"\"\"\n",
        "    # Get the selected feature names (corresponding to the original columns with highest loadings)\n",
        "    selected_features_order = pd.DataFrame(pca.components_, columns=feature_names)\n",
        "    selected_features_order = selected_features_order.abs().idxmax(axis=1)\n",
        "    selected_features_order = selected_features_order.drop_duplicates()\n",
        "\n",
        "    # keep 97%+ of the variance.\n",
        "    number_of_cumulative_variance = 1 # init the number of components to keep\n",
        "    vsum = 0 # to sum variance for each component (sumcum)\n",
        "    for n in explained_variance_ratio:\n",
        "      if vsum < 0.98:\n",
        "        number_of_cumulative_variance += 1\n",
        "      vsum += n\n",
        "    selected_features = selected_features_order[:number_of_cumulative_variance]\n",
        "    print(\"Keep 97%+ of the variance and thus keep \" + str(number_of_cumulative_variance) + \" Principal components from \" + str(n_components) + \".\\n\")\n",
        "\n",
        "    print(f\"{TextStyle.BOLD}Selected_features:{TextStyle.RESET_ALL} \\n\")\n",
        "    print(selected_features[:number_of_cumulative_variance +1])\n",
        "\n",
        "    \"\"\" End Teacher code \"\"\"\n",
        "\n",
        "\n",
        "    # Create a DataFrame for PCA results\n",
        "    pca_df = pd.DataFrame(data=pca_result, columns=feature_names)\n",
        "    final_df = pca_df[selected_features]\n",
        "    final_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "\n",
        "    # Plot component variance and cumulative variance\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot component variance with percentages\n",
        "    plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio * 100, label='Component Variance')\n",
        "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance * 100, marker='o', color='r', label='Cumulative Variance')\n",
        "\n",
        "    plt.title('Variance Explained by Principal Components\\n Dataset: ' + dataset_name)\n",
        "    plt.xlabel('Principal Component')\n",
        "    plt.ylabel('Percentage of Variance Explained (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "og-1WONtTzDG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pre-process function**"
      ],
      "metadata": {
        "id": "7msjiV9Gil15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_data(df_data, class_Col, name):   # class_col ....\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Pre-Proccess Data ------------{TextStyle.RESET_ALL}\")\n",
        "\n",
        "\n",
        "  ## Remove rows with missing target.\n",
        "  #//print(f\"{TextStyle.BOLD}{TextStyle.BLACK}------------ Handling Missing Target ------------{TextStyle.RESET_ALL}\")\n",
        "  #//print(\"Remove rows with missing target, separate target from predictors\")\n",
        "\n",
        "\n",
        "  # ----------- lowercase + strip 'sentence' ------------- #\n",
        "  # Convert to lowercase & Remove spaces in start and end\n",
        "  df_data['sentence'] = df_data['sentence'].str.lower().str.strip()\n",
        "\n",
        "  # Remove accents (optional)\n",
        "  # df_data['sentence'] = ''.join(x for x in unicodedata.normalize('NFKD', df_data['sentence']) if x in string.ascii_letters or x == \" \")\n",
        "\n",
        "\n",
        "  # ----------- Tokenization ------------- #\n",
        "  tokenized_sentences = tokenization_text(df_data['sentence'])\n",
        "\n",
        "\n",
        "  # ----------- OneHotEncoder ------------- #\n",
        "  # encoded_df = one_hot_decoder(df_data, tokenized_sentences )\n",
        "\n",
        "\n",
        "\n",
        "  #separate target from predictors\n",
        "  n_classes = df_data[class_Col].nunique()    #no. classes/category in class_Col (attack_type)\n",
        "\n",
        "  y = df_data[class_Col]\n",
        "  y_attack_type = np.unique(y).tolist()\n",
        "\n",
        "  # remove y column (class_Col) from column list\n",
        "  #//df_data.drop([class_Col], axis=1, inplace=True)\n",
        "\n",
        "  X = df_data.copy()\n",
        "  X_columns = X.columns       #['sentence', 'len_payload']\n",
        "  x_sentence_data = X['sentence']\n",
        "  #print('X_columns : ', X['sentence'])\n",
        "\n",
        "\n",
        "  #print('------------ 4 --------------')\n",
        "  print(\" X: \", X.shape)\n",
        "  print(\" y: \", y.shape)\n",
        "\n",
        "  #change components number in case new columns add after preprossing\n",
        "  n_components = X.shape[1]      # cols num. without dropd y column\n",
        "  print('n_components : ', n_components)\n",
        "\n",
        "\n",
        "  # ----------- PCA for feature selection ------------- #\n",
        "\n",
        "\n",
        "\n",
        "  # ----------- train_test_split ------------- #\n",
        "  # Divide the dataset into training (70%) and testing (30%)\n",
        "  print(f\"{TextStyle.BOLD}{TextStyle.BLACK}---------------- Divide the dataset into training (70%) and testing (30%) ----------------{TextStyle.RESET_ALL}\")\n",
        "  #### selected_features_df, y_encoder\n",
        "  selected_features_df = df_data['sentence']  #x_sentence_data\n",
        "  y_encoder = y  #_attack_type\n",
        "  X_train, X_test, y_train, y_test = train_test_split(selected_features_df, y_encoder, test_size=0.3, random_state=42,stratify=y)\n",
        "  print(f\"X_train shape: {X_train.shape}\")\n",
        "  print(f\"y_train shape: {y_train.shape}\")\n",
        "  print(f\"X_test shape : {X_test.shape}\")\n",
        "  print(f\"y_test shape : {y_test.shape}\")\n",
        "\n",
        "\n",
        "  # ----------- TfidfVectorizer ------------- #\n",
        "  X_train_tfidf, X_test_tfidf, df_X_train_tfidf, df_X_test_tfidf = Tfidf_Vectorizer(tokenized_sentences, X_train, X_test)\n",
        "\n",
        "\n",
        "\n",
        "  return n_classes, X_train, X_test, y_train, y_test, X_train_tfidf, X_test_tfidf, df_X_train_tfidf, df_X_test_tfidf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## ------------- PCA for feature selection ------------------\n",
        "## performs feature selection using Principal Component Analysis (PCA)\n",
        "print(f\"{TextStyle.BOLD}{TextStyle.BLACK}-------------- Principal Component Analysis (PCA) --------------{TextStyle.RESET_ALL}\")\n",
        "selected_features_df = perform_pca(X,n_components=n_components,dataset_name=name)\n",
        "columns_witout_taget = selected_features_df.columns\n",
        "#change components number in case new columns add after preprossing\n",
        "n_components = selected_features_df.shape[1] - 1\n",
        "# Encode categorical labels using one-hot encoding\n",
        "print(f\"{TextStyle.BOLD}{TextStyle.BLACK}Encode categorical labels using Label Encoder {TextStyle.RESET_ALL}\")\n",
        "# Initialize OneHotEncoder\n",
        "encoder = LabelEncoder()\n",
        "# Fit and transform the target column 'y'\n",
        "y_encoded = encoder.fit_transform(y)\n",
        "print(np.unique(y_encoded))\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" ---------------- Apply --------------------\n",
        "# Read Data of CSV Files\n",
        "dataset_directory = \"/content/datasets\"   #files_path\n",
        "percent = 100\n",
        "data = read_csv_files(dataset_directory,percent)\n",
        "print('len data : ' , len(data))\n",
        "\n",
        "\n",
        "# Apply pre-processing\n",
        "class_Col = 'attack_type'\n",
        "n_classes, X_train, X_test, y_train, y_test, X_train_tfidf, \\\n",
        "X_test_tfidf, df_X_train_tfidf, df_X_test_tfidf = preprocess_data(data, class_Col, 'csv files')\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Print results (optional)\n",
        "#print('======================')\n",
        "#for i, sentence in enumerate(sentences):\n",
        "#  print(f\"Sentence {i+1}:\", sentence)\n",
        "#  print(\"Tokens:\", tokenized_sentences[i])\n",
        "#  print(\"One-hot encoded:\", encoded_sentences[i])\n",
        "#  print(\"-\" * 20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFBxITVnnikK",
        "outputId": "8833312e-c3a9-4983-9560-67457f4cb7b5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[30m------------ Handling Read CSV Files ------------\u001b[0m\n",
            "Your files are:  ['SQLiDataset_3_test.csv']\n",
            "File Path:  /content/datasets/SQLiDataset_3_test.csv\n",
            "Total rows in df/file:  89\n",
            "Total rows in df/file 100%:  89\n",
            "                                             sentence  attack_type  \\\n",
            "48                  end and   (  (   ( \"poee\" = \"poee            1   \n",
            "86                          stiefel8@spainbareback.eu            0   \n",
            "53  -5596'  )  )   )  union all select 4877,4877,4...            1   \n",
            "74                                         or a  =  a            1   \n",
            "34   ) ; IF ( 3509 = 3509 )  SELECT 3509 ELSE DROP...            1   \n",
            "..                                                ...          ...   \n",
            "58                                   blakley@asyra.ao            0   \n",
            "66                                           or 1 = 1            1   \n",
            "71                                          or 1 = 1#            1   \n",
            "82  s:,s: select * from users where id = 1 or \":,\"...            1   \n",
            "33           ) ; BEGIN DBMS_LOCK.SLEEP ( 5 ) ; END;--            1   \n",
            "\n",
            "    len_payload attack_type_text  \n",
            "48           33             SQLi  \n",
            "86           25           normal  \n",
            "53           86             SQLi  \n",
            "74           10             SQLi  \n",
            "34           62             SQLi  \n",
            "..          ...              ...  \n",
            "58           16           normal  \n",
            "66            8             SQLi  \n",
            "71            9             SQLi  \n",
            "82           60             SQLi  \n",
            "33           40             SQLi  \n",
            "\n",
            "[89 rows x 4 columns]\n",
            "len data :  89\n",
            "\u001b[1m\u001b[30m------------ Pre-Proccess Data ------------\u001b[0m\n",
            "\u001b[1m\u001b[30m------------ Tokenization + stop word ------------\u001b[0m\n",
            "tokenized_sentences len: 255\n",
            "tokenized_sentences: ['function', 'and', 'edu', '5596', \"'\", 'described', 'related', 'int', '6232', 'makeup', 'male', '||', 'kenworthy', 'drop', 'md5', '))#', 'security', 'jrrv', 'eu', 'food', '¤', '14', 'asyra', 'blunkett', '\\\\<\\\\.', 'wants', 'limit', 'relates', 'still', '4877', 'peoples', '12', 'adults', 'member', 'defined', 'status', 'stief', '3', 'quot', '20select', 'union', 'authorities', '2011', 'dead', 'bahnhofplatz', 'dbms_pipe', ';--', '\"%\"', 'poee', '17', 'banning', 'typical', 'experts', 'end', '!', 'left', 'direction', 'incriminate', '3752', '1', '20', '5', '28', 'iif', 'generate_series', 'old', ':,', \"'%'\", 'stiefel8', '1802', 'clearly', '29', 'leftover', 'dbms_lock', 'edis', '26', 'id', 'bl', 'top', '9606', 'pg_sleep', 'one', 'gender', '16', '15', 'story', '-', 'ladies', 'users', 'select', \"||'\", '))', 'biological', 'footpath', '4718', '55z8do', '55964057c', '1c', 'todomodelos', '21', '*', 'men', 'contrast', 'srmq', 'year', 'different', 'begin', \"+'\", 'noonu', '2329', 'isnull', 'nothing', 'instead', '00', '#', '106', '\"\"', 'stewart', 'man', 'fdyw', '2164', '+', ':&', 'true', '02', '1625', 'european', '#\":', 'america', 'or', 'sleep', 'delay', 'home', 'null', ',@@', 'secretary', 'vrci', 'transsexual', 'violence', 'mjwi', '\"', 'benchmark', 'pm', '#\"', 'week', 'blakley', '2490', 'chr', 'waitfor', 'forcing', 'trample', 'make', 'followed', '--', 'version', 'confessions', 'vegetable', ',', '3509', 'spainbareback', 'p', 'apos', '&', 'despite', '100', 'dyos', ';', 'staff', 'illegally', '\":,\"', 'coerced', '@', 'difference', '201', 'ao', '0', '%', 'teo', 'sign', '.', '/*', 'filed', 'like', 'commission', 'growing', 'said', 'either', 'much', '2', 'seven', '6461', '8213', 'killed', 'marktstã', '98', ')', 'coulson', 'memh', '$+*$', 'receive_message', 'bicycle', '/', '(', 'earlier', '71', ':', 'ue', '=', 'law', '8214', 'change', '3000000', '105', 'david', '2116', 'also', '\"))', 'gurc', 'random', '4209', 'cast', '5597', 'neither', 'report', 'say', 'chance', 'anyone', 'else', 'street', 'former', 'female', 'make_set', 'exclusively', 'dual', 'diet', 'from', 'people', 'cycling', 'bags', 'routinely', 'children', '10000000', '2629', '4465', 'where', 'involvement', 'important', '\\\\', 'case', 'mr', 'nzmz', 'tte', 'pattern', 'transgender', 'intersex']\n",
            " X:  (89, 4)\n",
            " y:  (89,)\n",
            "n_components :  4\n",
            "\u001b[1m\u001b[30m---------------- Divide the dataset into training (70%) and testing (30%) ----------------\u001b[0m\n",
            "X_train shape: (62,)\n",
            "y_train shape: (62,)\n",
            "X_test shape : (27,)\n",
            "y_test shape : (27,)\n",
            "\u001b[1m\u001b[30m------------ TfidfVectorizer ------------\u001b[0m\n",
            "len tokenized_sentences -- :  255\n",
            "Shape of X_train_tfidf :  (62, 159)\n",
            "Shape of X_test_tfidf :  (27, 85)\n",
            "                                                     00   02  10000000  105  \\\n",
            "sentence                                                                      \n",
            ", 1: 00 p                                           1.0  0.0       0.0  0.0   \n",
            "16                                                  0.0  0.0       0.0  0.0   \n",
            "1 week                                              0.0  0.0       0.0  0.0   \n",
            ", 26-year-old man cycling illegally , left foot...  0.0  0.0       0.0  0.0   \n",
            "stief.kenworthy@todomodelos.edu                     0.0  0.0       0.0  0.0   \n",
            "...                                                 ...  ...       ...  ...   \n",
            "or 0 = 0 --                                         0.0  0.0       0.0  0.0   \n",
            "'  )  )   )  union all select null, null, null,...  0.0  0.0       0.0  0.0   \n",
            "or 1 = 1#                                           0.0  0.0       0.0  0.0   \n",
            ") ; if ( 3509 = 3509 )  select 3509 else drop f...  0.0  0.0       0.0  0.0   \n",
            "end ) +'                                            0.0  0.0       0.0  0.0   \n",
            "\n",
            "                                                    106   12   14   15   16  \\\n",
            "sentence                                                                      \n",
            ", 1: 00 p                                           0.0  0.0  0.0  0.0  0.0   \n",
            "16                                                  0.0  0.0  0.0  0.0  1.0   \n",
            "1 week                                              0.0  0.0  0.0  0.0  0.0   \n",
            ", 26-year-old man cycling illegally , left foot...  0.0  0.0  0.0  0.0  0.0   \n",
            "stief.kenworthy@todomodelos.edu                     0.0  0.0  0.0  0.0  0.0   \n",
            "...                                                 ...  ...  ...  ...  ...   \n",
            "or 0 = 0 --                                         0.0  0.0  0.0  0.0  0.0   \n",
            "'  )  )   )  union all select null, null, null,...  0.0  0.0  0.0  0.0  0.0   \n",
            "or 1 = 1#                                           0.0  0.0  0.0  0.0  0.0   \n",
            ") ; if ( 3509 = 3509 )  select 3509 else drop f...  0.0  0.0  0.0  0.0  0.0   \n",
            "end ) +'                                            0.0  0.0  0.0  0.0  0.0   \n",
            "\n",
            "                                                    1625  ...  typical   ue  \\\n",
            "sentence                                                  ...                 \n",
            ", 1: 00 p                                            0.0  ...      0.0  0.0   \n",
            "16                                                   0.0  ...      0.0  0.0   \n",
            "1 week                                               0.0  ...      0.0  0.0   \n",
            ", 26-year-old man cycling illegally , left foot...   0.0  ...      0.0  0.0   \n",
            "stief.kenworthy@todomodelos.edu                      0.0  ...      0.0  0.0   \n",
            "...                                                  ...  ...      ...  ...   \n",
            "or 0 = 0 --                                          0.0  ...      0.0  0.0   \n",
            "'  )  )   )  union all select null, null, null,...   0.0  ...      0.0  0.0   \n",
            "or 1 = 1#                                            0.0  ...      0.0  0.0   \n",
            ") ; if ( 3509 = 3509 )  select 3509 else drop f...   0.0  ...      0.0  0.0   \n",
            "end ) +'                                             0.0  ...      0.0  0.0   \n",
            "\n",
            "                                                      union  users  version  \\\n",
            "sentence                                                                      \n",
            ", 1: 00 p                                           0.00000    0.0      0.0   \n",
            "16                                                  0.00000    0.0      0.0   \n",
            "1 week                                              0.00000    0.0      0.0   \n",
            ", 26-year-old man cycling illegally , left foot...  0.00000    0.0      0.0   \n",
            "stief.kenworthy@todomodelos.edu                     0.00000    0.0      0.0   \n",
            "...                                                     ...    ...      ...   \n",
            "or 0 = 0 --                                         0.00000    0.0      0.0   \n",
            "'  )  )   )  union all select null, null, null,...  0.18774    0.0      0.0   \n",
            "or 1 = 1#                                           0.00000    0.0      0.0   \n",
            ") ; if ( 3509 = 3509 )  select 3509 else drop f...  0.00000    0.0      0.0   \n",
            "end ) +'                                            0.00000    0.0      0.0   \n",
            "\n",
            "                                                    waitfor  week  when  \\\n",
            "sentence                                                                  \n",
            ", 1: 00 p                                               0.0   0.0   0.0   \n",
            "16                                                      0.0   0.0   0.0   \n",
            "1 week                                                  0.0   1.0   0.0   \n",
            ", 26-year-old man cycling illegally , left foot...      0.0   0.0   0.0   \n",
            "stief.kenworthy@todomodelos.edu                         0.0   0.0   0.0   \n",
            "...                                                     ...   ...   ...   \n",
            "or 0 = 0 --                                             0.0   0.0   0.0   \n",
            "'  )  )   )  union all select null, null, null,...      0.0   0.0   0.0   \n",
            "or 1 = 1#                                               0.0   0.0   0.0   \n",
            ") ; if ( 3509 = 3509 )  select 3509 else drop f...      0.0   0.0   0.0   \n",
            "end ) +'                                                0.0   0.0   0.0   \n",
            "\n",
            "                                                    where      year  \n",
            "sentence                                                             \n",
            ", 1: 00 p                                             0.0  0.000000  \n",
            "16                                                    0.0  0.000000  \n",
            "1 week                                                0.0  0.000000  \n",
            ", 26-year-old man cycling illegally , left foot...    0.0  0.245985  \n",
            "stief.kenworthy@todomodelos.edu                       0.0  0.000000  \n",
            "...                                                   ...       ...  \n",
            "or 0 = 0 --                                           0.0  0.000000  \n",
            "'  )  )   )  union all select null, null, null,...    0.0  0.000000  \n",
            "or 1 = 1#                                             0.0  0.000000  \n",
            ") ; if ( 3509 = 3509 )  select 3509 else drop f...    0.0  0.000000  \n",
            "end ) +'                                              0.0  0.000000  \n",
            "\n",
            "[62 rows x 159 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **create_generator**"
      ],
      "metadata": {
        "id": "23KIY8XptvUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model of the generator is a deep multi-layer perceptron (MLP) composed of an input layer, output layer and four hidden layers. Initially, the generator takes a point from the latent space to generate new data. The latent space is a multi-dimensional hypersphere normal distributed points, where each variable is drawn from the distribution of the data in the dataset. An embedded layer in the generator creates a vector representation for the generated point.\n",
        "The generator model has four hidden layers. The first one is composed of 256 neurons with a rectified linear unit (ReLU) activation function. An embedded layer is used between hidden layers to efficiently map input data from a high-dimension to lower-dimension space. This allows the neural network to learn the data relationship and process it efficiently. The second hidden layer consists of 128 neurons, the third has 64 neurons and the last one has 32 neurons, with the ReLU activation function used with them all, and a regularization dropout of 20% is added to avoid overfitting. The output layer is activated using the Softmax activation function with 14 neurons as the number of features in the dataset."
      ],
      "metadata": {
        "id": "fRJt_ebtzQAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates a generator model for text generation using a deep MLP.\n",
        "\n",
        "Args:\n",
        "    latent_dim: The dimension of the latent space.\n",
        "    vocabulary_size: The size of the vocabulary (number of unique words).  ((get it by tokeniz))\n",
        "\n",
        "Returns:\n",
        "    A TensorFlow Keras model representing the text generation GAN generator.\n",
        "\"\"\"\n",
        "\n",
        "def create_generator(latent_dim, vocabulary_size):\n",
        "\n",
        "  # Input layer for latent space noise\n",
        "  noise = Input(shape=(latent_dim,), name='noise')\n",
        "\n",
        "  # Embedded layer to map noise to lower-dimensional space\n",
        "  # It creates a vector representation for the generated point.\n",
        "  hidden = Embedding(latent_dim, vocabulary_size // 2)(noise)  # Adjust embedding dim as needed\n",
        "\n",
        "  # Hidden layers with ReLU activation and dropout\n",
        "  hidden = Dense(256, activation='relu')(hidden)\n",
        "  hidden = Dropout(DISCRIMINATOR_DROPOUT_RATE)(hidden)        # DISCRIMINATOR_DROPOUT_RATE = 0.3\n",
        "\n",
        "  hidden = Dense(128, activation='relu')(hidden)\n",
        "  hidden = Dropout(DISCRIMINATOR_DROPOUT_RATE)(hidden)        # DISCRIMINATOR_DROPOUT_RATE = 0.3\n",
        "\n",
        "  hidden = Dense(64, activation='relu')(hidden)\n",
        "  hidden = Dropout(DISCRIMINATOR_DROPOUT_RATE)(hidden)        # DISCRIMINATOR_DROPOUT_RATE = 0.3\n",
        "\n",
        "  hidden = Dense(32, activation='relu')(hidden)\n",
        "  hidden = Dropout(DISCRIMINATOR_DROPOUT_RATE)(hidden)        # DISCRIMINATOR_DROPOUT_RATE = 0.3\n",
        "\n",
        "  # Output layer with Softmax activation\n",
        "  generated_text = Dense(vocabulary_size, activation='softmax', name='generated_text')(x)\n",
        "\n",
        "  # Create the generator model\n",
        "  generator = Model(inputs=noise, outputs=generated_text, name='generator')\n",
        "\n",
        "  return generator"
      ],
      "metadata": {
        "id": "OH72rhdfttJX"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **create_discriminator**"
      ],
      "metadata": {
        "id": "aq6EPGPqx6n5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the TDCGAN architecture consists of three discriminators. Each discriminator is a deep neural network with different architecture and different parameter settings. Each discriminator’s role is to extract features from the output of the generator and classify the data with varying levels of accuracy for each them. The model aims to classify data into two groups: normal flows for the background traffic with 0 representation, and anomaly flows for the attack data with 1 representation. The discriminator distinguishes the new data generated by the generator from the true data distribution. It classifies them as either real or fake. Subsequently, the discriminator undergoes updates to improve its ability to distinguish between real and fake samples in the subsequent round. The discriminators try to classify the data into their corresponding class, which is done through a fully connected MLP network.\n",
        "\n",
        "Each discriminator is a MLP model with a different number of hidden layers, different number of neurons and different dropout percentage. The first discriminator is composed of 3 hidden layers with 100 neurons for each and 10% dropout regularization. The second has five hidden layers with 64, 128, 256, 512, and 1024 neurons for each layer, respectively. The dropout percentage is 40%. The last discriminator has 4 hidden layers with 512, 256, 128, and 64 neurons for each layer and 20% dropout percentage.\n",
        "The LeakyReLU(alpha = 0.2) is used as an activation function for the hidden layers in the discriminators. Two output layers are used for each discriminator with the Softmax function as an activation function for one output layer and the Sigmoid activation function for the second output layer.\n",
        "\n",
        "***Why 3 discriminator?***\n",
        "\n",
        "In this model, three discriminators are used and each discriminator haas different architecture. These considered a modified training strategy which helped to face challenge in detection tasks. So, it help the generator to reach its optimal state even when the discriminator converges quickly during the initial stages of training.\n"
      ],
      "metadata": {
        "id": "cSg03D5g0m5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates a discriminator model for TDCGAN with specified architecture.\n",
        "\n",
        "Args:\n",
        "    data_dim          : Dimensionality of the data features (based on your data size)\n",
        "    num_classes       : Number of classes in the dataset (maybe - num_attack_classes)\n",
        "    name              : Name for the discriminator model.\n",
        "    num_hidden_layers : Number of hidden layers in the discriminator.\n",
        "    neurons_per_layer : List of integers specifying the number of neurons in each hidden layer.\n",
        "    dropout_rate      : Dropout rate for regularization.\n",
        "\n",
        "Returns:\n",
        "    A TensorFlow Keras model representing the TDCGAN discriminator.\n",
        "\"\"\"\n",
        "\n",
        "def create_discriminator(data_dim, num_classes, name, num_hidden_layers, neurons_per_layer, dropout_rate):\n",
        "\n",
        "  # Input layers for data (real or generated) and label\n",
        "  # our data is Conditional Generation so label is needed\n",
        "  data  = Input(shape=(data_dim,), name='data')       # Features    (ex: 100 samples, 10 features)\n",
        "  label = Input(shape=(num_classes,), name='label')   # One-hot encoded label\n",
        "\n",
        "\n",
        "  # Concatenate features and label  (Concatenate along the last dimension)\n",
        "  \"\"\"  Concatenation: A Concatenate layer is used to combine the features and label along the last dimension (axis=-1).\n",
        "      This creates a single input for the subsequent hidden layers in the discriminator model.\"\"\"\n",
        "  concatenated = Concatenate(axis=-1)([data, label])   #Concatenate(axis=-1)\n",
        "\n",
        "\n",
        "  # Hidden layers with LeakyReLU activation and dropout\n",
        "  hidden = data\n",
        "  for i in range(num_hidden_layers):\n",
        "    if i == 0:\n",
        "      hidden = Dense(neurons_per_layer[i], activation='leaky_relu', alpha=0.2)(concatenated)\n",
        "      hidden = Dropout(dropout_rate)(hidden)\n",
        "    else:\n",
        "      hidden = Dense(neurons_per_layer[i], activation='leaky_relu', alpha=0.2)(hidden)\n",
        "      hidden = Dropout(dropout_rate)(hidden)\n",
        "      hidden = BatchNormalization()(hidden)\n",
        "\n",
        "\n",
        "  # Output layer for real vs fake prediction (Real/fake classification)\n",
        "  validity_output = Dense(1, activation='sigmoid', name='validity_output')(hidden)\n",
        "\n",
        "  # Output layer for class label prediction (normal/sqli attack classification)   (Auxiliary classification / class_output)\n",
        "  auxiliary_output = Dense(num_classes, activation='softmax', name='auxiliary_output')(hidden)\n",
        "\n",
        "  # Build and compile the discriminator model\n",
        "  discriminator = Model(inputs=[data, label], outputs=[auxiliary_output, validity_output], name=name)    # inputs=data\n",
        "\n",
        "  # auxiliary_output (attack_type maybe) may binary_crossentropy  (0 --> normal / 1 --> SQLi)\n",
        "  discriminator.compile(\n",
        "      loss={'auxiliary_output': 'binary_crossentropy', 'validity_output': 'binary_crossentropy'},    #'auxiliary_output': 'categorical_crossentropy',\n",
        "      optimizer=Adam(learning_rate= '0.0002' , beta_1= '0.5'),\n",
        "      metrics={'auxiliary_output': 'accuracy', 'validity_output': 'accuracy'})\n",
        "\n",
        "  return discriminator\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" ----------------- Apply Discriminater with req. values ------------------ \"\"\"\n",
        "# Define discriminator architectures\n",
        "#discriminator_1 = create_discriminator(2, 'discriminator_1', 3, [100, 100, 100], 0.1)\n",
        "#discriminator_2 = create_discriminator(2, 'discriminator_2', 5, [64, 128, 256, 512, 1024], 0.4)\n",
        "#discriminator_3 = create_discriminator(2, 'discriminator_3', 4, [512, 256, 128, 64], 0.2)\n",
        "\n",
        "\"\"\"\n",
        "# May used in training as EX\n",
        "# Example Data (assuming you have 3 classes and data_dim is 10)\n",
        "real_data = tf.random.normal(shape=(100, 10))  # Sample real data (100 samples, 10 features)\n",
        "label_one_hot = tf.one_hot(tf.random.uniform(shape=(100,), minval=0, maxval=num_classes, dtype=tf.int32), depth=num_classes)  # One-hot encoded labels (100 samples)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "s_3ORAJU0pS9",
        "outputId": "7dfda6cf-1487-468a-ffd5-1d050fc28c48"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' ----------------- Apply Discriminater with req. values ------------------'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train**"
      ],
      "metadata": {
        "id": "jf78A8IV2T8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through training, the generator learns to map points from the latent space into specific output data, which are different each time the model is trained. Taken a step further, new data are then generated using random points in the latent space. So, these points are used to generate specific data. The discriminator distinguishes the new data generated by the generator from the true data distribution.\n",
        "Here both the generator and discriminator models are trained simultaneously and conditioned on the class label. This conditioning enables the generator model, when utilized independently, to generate minor class data within the domain that corresponds to a specific class label. Subsequently, the discriminator undergoes updates to improve its ability to distinguish between real and fake samples in the subsequent round. Additionally, the generator receives updates based on its success or failure in deceiving the discriminator with its generated samples.\n",
        "To Ensure a more balanced training process between the generator and discriminators by three discriminators with different architectures.\n",
        "In this manner, the two models engage in a competitive relationship, exhibiting adversarial behavior in the context of game theory. In this scenario, the concept of zero-sum implies that when the discriminator effectively distinguishes between real and fake samples, it receives a reward, or no adjustments are made to its model parameters. Simultaneously, the generator is penalized with significant updates to its model parameters. Alternatively, when the generator successfully deceives the discriminator, it receives a reward, or no modifications are made to its model parameters. Whereas, the discriminator is penalized. This is the generic GAN approach.\n",
        "The model is trained for 1000 epochs with a batch size of 128. The optimizer is Adam with a learning rate equal to 0.0001. The proposed model allows the generator to train until it produces a new set of data samples that resembles the real distribution of the original dataset.\n",
        "\n",
        "The discriminators undergo separate training, where each of the model weights are designated as non-trainable within the TDCGAN model. This ensures that solely the weights of the generator model are updated during the training process. This trainability modification specifically applies when training the TDCGAN model, not when training the discriminator independently. So, the TDCGAN model is employed to train the generator’s model weights by utilizing the output and error computed by the discriminator models.\n",
        "\n",
        "The primary objective of the training methodology employed in the GAN framework is for the generator to generate fake data that closely resemble real data, and for the discriminator to acquire sufficient knowledge to differentiate between real and fake samples. Both the generator and discriminator are trained until the discriminator can no longer distinguish real data from fake data. This mean that the generated network can estimate the data sample’s distribution and achieve Nash equilibrium.In order to assess the performance of our model with precision, it is customary to divide the data into training and test sets to produce accurate predictions on unseen data.\n",
        "The training set is utilized for model fitting, while the test set is employed to measure the predictive precision of the trained model. The dataset is split into 70% for training and validation and 30% for testing. The training set is divided into minor class data and other class data. The TDCGAN model uses the minor class to generate data. The generator is trained to model the distribution of the anomaly data (minor class), while fixing the discriminator. The output from the generator is fed as input to the discriminator to predict it.\n",
        "The error is estimated, and the generator’s weight is then updated. The training continues until the discriminator cannot distinguish if the input data come from the generator’s output or from the real anomaly dataset. In the training process, we make sure that all architectures undergo an equal number of epochs and that the weights from the final epoch are selected to generate artificial attack samples.\n",
        "We begin by adhering to this iterative training procedure and ultimately utilize the generator to produce attack samples. Eventually, we incorporate the generated attack samples into the training set. By this, we oversample minor classes in the dataset during the training phase. The test dataset is then used to test the model performance.\n"
      ],
      "metadata": {
        "id": "rRgTPUag_Vdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist  # Replace with your dataset\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 1000\n",
        "batch_size = 128\n",
        "learning_rate = 0.0001\n",
        "split_ratio = 0.7  # 70% for training and validation, 30% for testing\n",
        "\n",
        "# Load data (replace with your data loading logic)\n",
        "(x_train, y_train), (_, _) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28 * 28)  # Flatten images (adjust based on data shape)\n",
        "x_train = x_train.astype('float32') / 255.0  # Normalize data\n",
        "\n",
        "# One-hot encode labels (adjust num_classes if needed)\n",
        "num_classes = 10  # Assuming 10 classes\n",
        "y_train = tf.one_hot(y_train, depth=num_classes)\n",
        "\n",
        "# Split data into training, validation, and test sets\n",
        "train_val_split = int(split_ratio * len(x_train))\n",
        "x_train, x_val, y_train, y_val = x_train[:train_val_split], x_train[train_val_split:], y_train[:train_val_split], y_train[train_val_split:]\n",
        "x_test, y_test = x_train[train_val_split:], y_train[train_val_split:]\n",
        "\n",
        "# Define functions for generator and discriminators (replace with your architectures)\n",
        "def create_generator(latent_dim, num_classes):\n",
        "    # ... your generator architecture here ...\n",
        "    return model\n",
        "\n",
        "def create_discriminator(data_dim, num_classes, name):\n",
        "    # ... your discriminator architecture here ...\n",
        "    return model\n",
        "\n",
        "# Create generator and discriminators\n",
        "generator = create_generator(latent_dim=100, num_classes=num_classes)  # Adjust latent space dimension\n",
        "discriminator_1 = create_discriminator(data_dim=28 * 28, num_classes=num_classes, name='discriminator_1')\n",
        "discriminator_2 = create_discriminator(data_dim=28 * 28, num_classes=num_classes, name='discriminator_2')\n",
        "discriminator_3 = create_discriminator(data_dim=28 * 28, num_classes=num_classes, name='discriminator_3')\n",
        "\n",
        "# Define loss functions (replace with appropriate loss functions)\n",
        "generator_loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "discriminator_loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# Optimizers for generator and discriminators\n",
        "generator_optimizer = Adam(learning_rate=learning_rate)\n",
        "discriminator_optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Train generator with all discriminators fixed\n",
        "    for _ in range(1):  # Train generator once per epoch (adjust as needed)\n",
        "        noise = tf.random.normal(shape=(batch_size, latent_dim))  # Sample random noise\n",
        "        label = tf.random.uniform(shape=(batch_size,), minval=0, maxval=num_classes, dtype=tf.int32)  # Random class labels\n",
        "        label = tf.one_hot(label, depth=num_classes)\n",
        "\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            generated_data = generator([noise, label])\n",
        "            real_output = tf.concat([tf.ones_like(generated_data[:, 0:1]), tf.zeros_like(generated_data[:, 1:])], axis=1)  # Expected output for real data (all ones for validity)\n",
        "            for discriminator in [discriminator_1, discriminator_2, discriminator_3]:\n",
        "                discriminator_output = discriminator([generated_data, label])\n",
        "                gen_loss = generator_loss_function(real_output, discriminator_output[0])  # Consider combining losses from all discriminators\n",
        "                gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "\n",
        "        generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "        generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
        "\n",
        "    # Train each discriminator separately with generator fixed"
      ],
      "metadata": {
        "id": "8ShyzxHlBwzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Run code TDCGAN Model**"
      ],
      "metadata": {
        "id": "3pDxPIJgpXtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **run_tdcgan**"
      ],
      "metadata": {
        "id": "ucAOeU9PFjNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tdcgan(X_t, y_t,encoder,name,class_Col,n_classes,features_dim, latent_dim):\n",
        "  # Create a DataFrame\n",
        "  df = pd.DataFrame(data=X_t)\n",
        "  cols = df.columns\n",
        "  df[class_Col] = y_t\n",
        "\n",
        "  # Identify the majority class and count\n",
        "  majority_class = df[class_Col].value_counts().idxmax()\n",
        "  majority_count = len(df[df[class_Col] == majority_class])\n",
        "\n",
        "  # Select only the minority classes data for training TDCGAN\n",
        "  minority_data = df[~df[class_Col].isin([majority_class])]\n",
        "  labels_dic = (minority_data[class_Col].value_counts()).to_dict()\n",
        "\n",
        "  model = TDCGAN(latent_dim,n_classes -1,features_dim ,features_dim)\n",
        "  model.print_parameters_and_summary()\n",
        "  model.plot_architecture()\n",
        "\n",
        "  # train model (minority_data train / unbalanced)\n",
        "  y_tr = minority_data[class_Col]\n",
        "  minority_data.drop([class_Col], axis=1, inplace=True)\n",
        "  model.train(minority_data, y_tr, \" unbalanced \" + name)\n",
        "\n",
        "  # Save the trained model\n",
        "  print(\"Saving model \")\n",
        "  save_model_2(model,name,'tdcgan')\n",
        "\n",
        "  #Generate synthetic samples for the minority classes\n",
        "  print(f\"{TextStyle.BOLD}Generate synthetic features and labels{TextStyle.RESET_ALL}\")\n",
        "  for key, value in labels_dic.items():\n",
        "    labels_dic[key]= majority_count - value\n",
        "  synthetic_data = model.generate_synthetic_data(labels_dic,cols,class_Col)\n",
        "\n",
        "  # Concatenate synthetic samples with the original dataset\n",
        "  fdf = pd.concat([df, synthetic_data], ignore_index=True, sort=False)\n",
        "\n",
        "  # Count the occurrences of each class in the original labels\n",
        "  unique_classes, original_counts = np.unique(encoder.inverse_transform(y_t), return_counts=True)\n",
        "\n",
        "  # Count the occurrences of each class in the balanced labels\n",
        "  unique_resampled_classes, resampled_counts = np.unique(encoder.inverse_transform(fdf[class_Col]), return_counts=True)\n",
        "\n",
        "  # Sort the data from largest to smallest\n",
        "  original_counts, unique_classes = zip(*sorted(zip(original_counts, unique_classes), reverse=True))\n",
        "  #resampled_counts, unique_resampled_classes = zip(*sorted(zip(resampled_counts, unique_resampled_classes), reverse=True))\n",
        "  print('Original Data class counter:\\n', Counter(encoder.inverse_transform(y_t)))\n",
        "  print('Balanced Data class counter:\\n', Counter(encoder.inverse_transform(fdf[class_Col])))\n",
        "\n",
        "  # Create bar positions\n",
        "  x = np.arange(len(unique_classes))\n",
        "\n",
        "  # Set the width of the bars\n",
        "  width = 0.4\n",
        "  # Create the figure and axis objects\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "  # Plot the original class distribution\n",
        "  ax1.bar(x, original_counts, width, color=colors_list)\n",
        "  ax1.set_xlabel('Classes')\n",
        "  ax1.set_ylabel('Counts')\n",
        "  ax1.set_title('Original Class Distribution\\n Dataset ' + name + \"\\n\" + str(X_t.shape))\n",
        "  ax1.set_xticks(x)\n",
        "  ax1.set_xticklabels(unique_classes,rotation='vertical')\n",
        "  #ax1.legend()\n",
        "\n",
        "  # Plot the balanced class distribution\n",
        "  ax2.bar(x, resampled_counts, width, color=colors_list)\n",
        "  ax2.set_xlabel('Classes')\n",
        "  ax2.set_ylabel('Counts')\n",
        "  ax2.set_title('Balanced Class Distribution ' + \"TDCGAN\" + '\\n Dataset ' + name + \"\\n\" + str(fdf.shape))\n",
        "  ax2.set_xticks(x)\n",
        "  ax2.set_xticklabels(unique_classes,rotation='vertical')\n",
        "  #ax2.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  return fdf"
      ],
      "metadata": {
        "id": "D-q7PQv-F3r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Balance Dataset**"
      ],
      "metadata": {
        "id": "qRFSJcisUH6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning from imbalanced dataset is one of the main challenges that ML facing and it's a common problem. Because collecting balanced dataset cann't be guaranteed in many different fields. For example, in the website and application security field that software engineers face, more normal information is usually entered on the website than malicious attacks, which generates unbalanced data. This problem can be solved by balancing the data through reducing the majority classes and increasing the minority classes."
      ],
      "metadata": {
        "id": "ZFyNalH4UYHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Check is balanced***"
      ],
      "metadata": {
        "id": "sKte-Q1ZuU2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "def check_data_isbalance(data, class_Col):\n",
        "    df1 = pd.DataFrame(data)\n",
        "    cat_class = df1[class_Col]\n",
        "\n",
        "    # Count the occurrences of each value in class_Col\n",
        "    count_per_value = Counter(cat_class)\n",
        "\n",
        "    # Check if the counts of the unique values are equal\n",
        "    are_counts_equal = len(set(count_per_value.values())) == 1\n",
        "\n",
        "    print(\"Count of rows for each value in class_Col:\")\n",
        "    for value, count in count_per_value.items():\n",
        "        print(f\"Value {value}: {count} rows\")\n",
        "\n",
        "    if are_counts_equal:\n",
        "        print(\"The counts of values in class_Col are equal.\")\n",
        "    else:\n",
        "        print(\"The counts of values in class_Col are not equal.\")\n",
        "\n",
        "    return count_per_value, are_counts_equal\n",
        "\n",
        "data = {\n",
        "        'sentence' : [ \"' or 1=1\" , 'hi soma', '29%', '--', 'nice'],\n",
        "        'attack_type' : [1, 0, 1, 1, 0]\n",
        "}\n",
        "\n",
        "class_Col = 'attack_type'\n",
        "count_per_value, are_counts_equal = check_data_isbalance(data, class_Col)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eMtfXX7owal",
        "outputId": "6c85b4fd-49a6-4264-da87-b744629e07a8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of rows for each value in class_Col:\n",
            "Value 1: 3 rows\n",
            "Value 0: 2 rows\n",
            "The counts of values in class_Col are not equal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Read random n rows from dataset***"
      ],
      "metadata": {
        "id": "pDxEmj98uZbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" ----------------- Read Random n rows Function ----------------- \"\"\"\n",
        "def read_random_n_rows(df, nrows, percent):\n",
        "    # Check if requested sample size is greater than the total number of rows\n",
        "\t  # nrows --> requested sample size\n",
        "\t  # len(df) --> total number of rows\n",
        "    if nrows >= len(df):\n",
        "        return df\n",
        "\n",
        "    # Randomly select n rows from the DataFrame\n",
        "    random_sample = df.sample(n=nrows, random_state=42)\n",
        "    random_sample.reset_index(drop=True,inplace=True)\n",
        "    return random_sample\n",
        "\n",
        "\n",
        "\"\"\" ----------------- Apply Read Random n rows Function ----------------- \"\"\"\n",
        "#nrows = 12000\n",
        "#random_sample = read_random_n_rows(data, nrows, percent)\n",
        "#random_sample"
      ],
      "metadata": {
        "id": "wDr7F9DXuemd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Balancing dataset***"
      ],
      "metadata": {
        "id": "8d76vjCjuuM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def balancing_data(file_name, data, percent, class_Col):\n",
        "    path = '/content/drive/MyDrive/code/ids/CIC-IDS2017'\n",
        "    percent =  100 #100% from each file (8 files)\n",
        "    data = read_csv_files(path,percent)\n",
        "    data = upclasses(data,' Label',2000)\n",
        "    data[' Label'].value_counts()\n",
        "\n",
        "    #save data to file\n",
        "    path='/content/drive/MyDrive/code/ids/CIC-IDS2017-NEW/'\n",
        "    data.to_csv(path + 'CIC-IDS2017.csv', index=False)\n",
        "\n",
        "\n",
        "    \"\"\"**Selecting subset of 60 from each dataset**\"\"\"\n",
        "\n",
        "    #Read 60 raw from each dataset and save each new 200,000 raw dataset in a new file.\n",
        "    percent =  100 #100% from each file (8 files)\n",
        "    nrows = 60\n",
        "    path = '/content/drive/MyDrive/code/ids/CIC-IDS2017-NEW'\n",
        "    print(\"\\nSubset of 200,000 from each dataset \\n \")\n",
        "    #read 200,000 recored from CIC-IDS2017 without selected rows\n",
        "    data = read_random_n_rows(path + \"/CIC-IDS2017.csv\", nrows,percent)\n",
        "\n",
        "    #save data to file\n",
        "    path='/datasets/balanced/'\n",
        "    data.to_csv(path + 'CIC-IDS2017.csv', index=False)\n",
        "    #plot the data\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(20,5))\n",
        "    colors_list = ['Red','Orange', 'Blue', 'Purple','Green','Pink','Gray','Tan','Lime','Cyan']\n",
        "    r1 = data[' Label'].value_counts().plot(kind='bar',color=colors_list,ax=ax[0])\n",
        "    r1.set_xticklabels(r1.get_xticklabels())\n",
        "    r1.set_title(str(nrows) + \" of the CIC_IDS2017 dataset\\n\" + str(data.shape))\n",
        "    for tick in r1.get_xticklabels():\n",
        "        tick.set_rotation(90)\n",
        "    for rect in r1.patches:\n",
        "        r1.text (rect.get_x() + rect.get_width()  / 2,rect.get_height()+ 0.75, \"  \" + str(round(rect.get_height()/len(data) * 100,4)) + \"%\",rotation='vertical', color='darkolivegreen', fontsize = 7)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" ----------------- Apply Read Random n rows Function ----------------- \"\"\"\n",
        "nrows = 60\n",
        "percent = 100\n",
        "\n",
        "print(\"\\nSubset of \" , nrows , \" from each dataset \\n \")\n",
        "#read nrows recored from dataset file without selected rows\n",
        "random_sample = read_random_n_rows(data_readed_file, nrows, percent)\n",
        "random_sample"
      ],
      "metadata": {
        "id": "-9J3WRN1hhh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Run Code**"
      ],
      "metadata": {
        "id": "JAPkmd_UFwh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\" ----------------- Apply Code ----------------- \"\"\"\n",
        "# Read Data of CSV Files\n",
        "dataset_directory = \"/content/datasets\"   #files_path\n",
        "percent = 100\n",
        "data = read_csv_files(dataset_directory,percent)\n",
        "print('len data : ' , len(data))\n",
        "\n",
        "\n",
        "# Apply pre-processing\n",
        "class_Col = 'attack_type'\n",
        "n_classes, X_train, X_test, y_train, y_test, X_train_tfidf, \\\n",
        "X_test_tfidf, df_X_train_tfidf, df_X_test_tfidf = preprocess_data(data, class_Col, 'csv files')\n",
        "\n",
        "\n",
        "#data = run_tdcgan(X_train.copy(), y_train.copy(),encoder,name,class_Col,n_classes,X_train.shape[1], 100)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s_SYsmIpfPz",
        "outputId": "7f19cf1b-bba2-4dc1-fd71-a786e46dc7cb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[30m------------ Handling Read CSV Files ------------\u001b[0m\n",
            "Your files are:  ['SQLiDataset_3_test.csv']\n",
            "File Path:  /content/datasets/SQLiDataset_3_test.csv\n",
            "Total rows in df/file:  89\n",
            "Total rows in df/file 100%:  89\n",
            "                                             sentence  attack_type  \\\n",
            "28   ( select * from  ( select ( sleep ( 5  )  )  ...            1   \n",
            "50   Experts say violence left 14 adults seven chi...            0   \n",
            "53  -5596'  )  )   )  union all select 4877,4877,4...            1   \n",
            "25   &quot; Mr Edis said Coulson &apos;s involveme...            0   \n",
            "7                                            12:00 PM            0   \n",
            "..                                                ...          ...   \n",
            "88                 \")) or benchmark(10000000,MD5(1))#            1   \n",
            "85                    stief.kenworthy@todomodelos.edu            0   \n",
            "12                                                  5            0   \n",
            "75                                           or a = a            1   \n",
            "4                                                 26%            1   \n",
            "\n",
            "    len_payload attack_type_text  \n",
            "28           70             SQLi  \n",
            "50          109           normal  \n",
            "53           86             SQLi  \n",
            "25          123           normal  \n",
            "7             8           normal  \n",
            "..          ...              ...  \n",
            "88           34             SQLi  \n",
            "85           31           normal  \n",
            "12            1           normal  \n",
            "75            8             SQLi  \n",
            "4             3             SQLi  \n",
            "\n",
            "[89 rows x 4 columns]\n",
            "len data :  89\n",
            "\u001b[1m\u001b[30m------------ Handling Missing Target ------------\u001b[0m\n",
            "Remove rows with missing target, separate target from predictors\n",
            " X:  (89, 3)\n",
            " y:  (89,)\n",
            "n_components :  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test 1**"
      ],
      "metadata": {
        "id": "20ZTUhX-gDI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv1D,\n",
        "    Flatten,\n",
        "    Dense,\n",
        "    Embedding,\n",
        "    Input,\n",
        "    LeakyReLU,\n",
        "    BatchNormalization,\n",
        "    Reshape,\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Hyperparameters (adjust as needed)\n",
        "vocab_size = 10000  # Adjust based on your vocabulary\n",
        "embedding_dim = 128\n",
        "noise_dim = 100\n",
        "latent_dim = 100  # Dimension of latent space for conditional generation\n",
        "\n",
        "def preprocess_text(text):\n",
        "  \"\"\"\n",
        "  Preprocesses a text string for use in the DCGAN model.\n",
        "\n",
        "  Args:\n",
        "      text: The input text string.\n",
        "\n",
        "  Returns:\n",
        "      A list of preprocessed tokens.\n",
        "  \"\"\"\n",
        "  # ... (same preprocessing steps from previous response)\n",
        "\n",
        "# Text embedding layer\n",
        "embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_sentence_length)\n",
        "\n",
        "\n",
        "# Create separate models for generator and discriminator\n",
        "generator = build_generator(noise_dim, latent_dim, embedding_dim)"
      ],
      "metadata": {
        "id": "qZnROnjdq2oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def preprocess_sentences(sentences):\n",
        "  \"\"\"\n",
        "  Preprocesses a list of sentences into one-hot encoded data.\n",
        "\n",
        "  Args:\n",
        "      sentences: A list of strings, where each string represents a sentence.\n",
        "\n",
        "  Returns:\n",
        "      A tuple containing:\n",
        "          - sequences: A list of lists, where each inner list represents the one-hot encoded word indices for a sentence.\n",
        "          - word_index: A dictionary mapping each unique word to its unique index.\n",
        "  \"\"\"\n",
        "\n",
        "  # Count word occurrences\n",
        "  word_counts = Counter()\n",
        "  for sentence in sentences:\n",
        "    words = sentence.lower().split()  # Lowercase and split into words\n",
        "    word_counts.update(words)\n",
        "\n",
        "  # Filter out infrequent words (optional)\n",
        "  min_count = 5  # Adjust minimum count threshold as needed\n",
        "  word_counts = {word: count for word, count in word_counts.items() if count >= min_count}\n",
        "\n",
        "  # Create tokenizer and fit on word counts\n",
        "  tokenizer = Tokenizer(num_words=len(word_counts) + 1)  # +1 for padding\n",
        "  tokenizer.fit_on_texts(word_counts.keys())\n",
        "\n",
        "  # Convert sentences to sequences of word indices\n",
        "  sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "  # Pad sequences to the same length (optional)\n",
        "  max_len = 20  # Adjust maximum sentence length as needed\n",
        "  from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "  sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "  # Create one-hot encoded representation\n",
        "  one_hot_encoded = np.array([tf.one_hot(seq, len(word_counts) + 1) for seq in sequences])\n",
        "\n",
        "  return one_hot_encoded, tokenizer.word_index\n",
        "\n",
        "# Example usage\n",
        "sentences = [\n",
        "    \"select * form dual where x=2 or 1=1\",\n",
        "    \"update table_name set column1=value1 where condition\",\n",
        "    \"insert into table_name (column1, column2) values ('value1', 'value2')\",\n",
        "]\n",
        "\n",
        "one_hot_encoded, word_index = preprocess_sentences(sentences)\n",
        "\n",
        "print(\"One-hot encoded data:\", one_hot_encoded.shape)\n",
        "print(\"Example sentence:\", sentences[0])\n",
        "print(\"Corresponding one-hot encoded sequence:\", one_hot_encoded[0])\n",
        "print(\"Word index:\", word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "62yY834zrIcR",
        "outputId": "882f7d8a-d8e3-414f-b6ae-98532ab8aa8c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-064d05cce656>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m ]\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mone_hot_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"One-hot encoded data:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-064d05cce656>\u001b[0m in \u001b[0;36mpreprocess_sentences\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;31m# Create one-hot encoded representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0mone_hot_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mone_hot_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **test 2**"
      ],
      "metadata": {
        "id": "fTf8nmXNgGMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "# Initialize variables\n",
        "NUM_EPOCHS = 50\n",
        "BUFFER_SIZE = 30000\n",
        "BATCH_SIZE = 28\n",
        "NOISE_DIMENSION = 75\n",
        "UNIQUE_RUN_ID = str(uuid.uuid4())\n",
        "PRINT_STATS_AFTER_BATCH = 50\n",
        "OPTIMIZER_LR = 0.0002\n",
        "OPTIMIZER_BETAS = (0.5, 0.999)\n",
        "WEIGHT_INIT_STDDEV = 0.02\n",
        "\n",
        "GENERATOR_DROPOUT_RATE = 0.1\n",
        "DISCRIMINATOR_DROPOUT_RATE = 0.3      #Adjust the dropout rate to prevent overfitting during training.\n",
        "LEAKY_RELU_ALPA = 0.2"
      ],
      "metadata": {
        "id": "qHLyxt9jgJTg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow # as tf\n",
        "\n",
        "# Initialize loss function, init schema and optimizers\n",
        "cross_entropy_loss = tensorflow.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "weight_init = tensorflow.keras.initializers.RandomNormal(stddev=WEIGHT_INIT_STDDEV)\n",
        "\n",
        "generator_optimizer = tensorflow.keras.optimizers.Adam(OPTIMIZER_LR, \\\n",
        "  beta_1=OPTIMIZER_BETAS[0], beta_2=OPTIMIZER_BETAS[1])\n",
        "\n",
        "discriminator_optimizer = tensorflow.keras.optimizers.Adam(OPTIMIZER_LR, \\\n",
        "  beta_1=OPTIMIZER_BETAS[0], beta_2=OPTIMIZER_BETAS[1])"
      ],
      "metadata": {
        "id": "2Ej44Lnag7We"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_directory_for_run():\n",
        "  \"\"\" Make a directory for this training run. \"\"\"\n",
        "  print(f'Preparing training run {UNIQUE_RUN_ID}')\n",
        "  if not os.path.exists('./runs'):\n",
        "    os.mkdir('./runs')\n",
        "  os.mkdir(f'./runs/{UNIQUE_RUN_ID}')"
      ],
      "metadata": {
        "id": "_W83cbwChc13"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image(generator, epoch = 0, batch = 0):\n",
        "  \"\"\" Generate subplots with generated examples. \"\"\"\n",
        "  images = []\n",
        "  noise = generate_noise(BATCH_SIZE)\n",
        "  images = generator(noise, training=False)\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  for i in range(16):\n",
        "    # Get image and reshape\n",
        "    image = images[i]\n",
        "    image = np.reshape(image, (28, 28))\n",
        "    # Plot\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "  if not os.path.exists(f'./runs/{UNIQUE_RUN_ID}/images'):\n",
        "    os.mkdir(f'./runs/{UNIQUE_RUN_ID}/images')\n",
        "  plt.savefig(f'./runs/{UNIQUE_RUN_ID}/images/epoch{epoch}_batch{batch}.jpg')"
      ],
      "metadata": {
        "id": "yqK99k0hhsgP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path='mnist.npz'\n",
        "def load_data():\n",
        "  \"\"\" Load data \"\"\"\n",
        "  (images, _), (_, _) = tensorflow.keras.datasets.mnist.load_data()\n",
        "  images = images.reshape(images.shape[0], 28, 28, 1)\n",
        "  images = images.astype('float32')\n",
        "  images = (images - 127.5) / 127.5\n",
        "  return tensorflow.data.Dataset.from_tensor_slices(images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "58O7KywWiN33"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D,\n",
        "    Flatten,\n",
        "    Dense,\n",
        "    LeakyReLU,\n",
        "    Dropout,\n",
        "    BatchNormalization,  # Consider adding for training stability\n",
        ")\n",
        "\n",
        "#GENERATOR_DROPOUT_RATE = 0.1\n",
        "\n",
        "def create_generator():\n",
        "  \"\"\" Create Generator \"\"\"\n",
        "  generator = tensorflow.keras.Sequential()\n",
        "\n",
        "  # Input block\n",
        "  generator.add(Dense(7*7*128, use_bias=False, input_shape=(NOISE_DIMENSION,), kernel_initializer=weight_init))\n",
        "  generator.add(BatchNormalization())\n",
        "  generator.add(LeakyReLU())\n",
        "\n",
        "  # Reshape 1D Tensor into 3D\n",
        "  generator.add(Reshape((7, 7, 128)))\n",
        "\n",
        "  # First upsampling block\n",
        "  generator.add(Conv2DTranspose(56, (5, 5), strides=(1, 1), padding='same', use_bias=False, kernel_initializer=weight_init))\n",
        "  generator.add(BatchNormalization())\n",
        "  generator.add(LeakyReLU())\n",
        "\n",
        "  # Second upsampling block\n",
        "  generator.add(Conv2DTranspose(28, (5, 5), strides=(2, 2), padding='same', use_bias=False, \\\n",
        "    kernel_initializer=weight_init))\n",
        "  generator.add(BatchNormalization())\n",
        "  generator.add(LeakyReLU())\n",
        "\n",
        "  # Third upsampling block: note tanh, specific for DCGAN\n",
        "  generator.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh', \\\n",
        "    kernel_initializer=weight_init))\n",
        "\n",
        "  # Return generator\n",
        "  return generator"
      ],
      "metadata": {
        "id": "ptLlB0uJji6K"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_noise(number_of_images = 1, noise_dimension = NOISE_DIMENSION):\n",
        "  \"\"\" Generate noise for number_of_images images, with a specific noise_dimension \"\"\"\n",
        "  return tensorflow.random.normal([number_of_images, noise_dimension])"
      ],
      "metadata": {
        "id": "t4FiSROpllQu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_discriminator():\n",
        "  \"\"\" Create Discriminator \"\"\"\n",
        "  discriminator = tensorflow.keras.Sequential()\n",
        "\n",
        "  # First Convolutional block (CNN Layer 1)\n",
        "  discriminator.add(Conv2D(28, (5, 5), strides=(2, 2), padding='same',    #(32, (3, 3), strides=(1, 1)\n",
        "                                    input_shape=[28, 28, 1], kernel_initializer=weight_init))   #\n",
        "  discriminator.add(LeakyReLU(alpha=LEAKY_RELU_ALPA))\n",
        "  discriminator.add(Dropout(DISCRIMINATOR_DROPOUT_RATE))\n",
        "\n",
        "  # Second Convolutional block (CNN Layer 2)\n",
        "  discriminator.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', kernel_initializer=weight_init))   #(3, 3)     kernel_initializer='he_normal'))\n",
        "  discriminator.add(LeakyReLU(alpha=LEAKY_RELU_ALPA))\n",
        "  discriminator.add(Dropout(DISCRIMINATOR_DROPOUT_RATE))\n",
        "  discriminator.add(BatchNormalization())  # Consider adding for to improve training stability, especially for deeper networks.\n",
        "\n",
        "  # Third Convolutional block (CNN Layer 3)\n",
        "  discriminator.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same', kernel_initializer=weight_init))   #(3, 3)     kernel_initializer='he_normal'))\n",
        "  discriminator.add(LeakyReLU(alpha=LEAKY_RELU_ALPA))\n",
        "  discriminator.add(Dropout(DISCRIMINATOR_DROPOUT_RATE))\n",
        "  discriminator.add(BatchNormalization())  # Consider adding for training stability\n",
        "\n",
        "  # Flatten and generate output prediction\n",
        "  # numerical representations of text (word embeddings) or flattened representations of images.\n",
        "  discriminator.add(Flatten())\n",
        "  discriminator.add(Dense(1, kernel_initializer=weight_init, activation='sigmoid'))  #relu    #kernel_initializer='he_normal'\n",
        "\n",
        "  # Return discriminator\n",
        "  return discriminator\n"
      ],
      "metadata": {
        "id": "tKn39M4im1z6"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_generator_loss(predicted_fake):\n",
        "  \"\"\" Compute cross entropy loss for the generator \"\"\"\n",
        "  return cross_entropy_loss(tensorflow.ones_like(predicted_fake), predicted_fake)\n",
        "\n",
        "\n",
        "def compute_discriminator_loss(predicted_real, predicted_fake):\n",
        "  \"\"\" Compute discriminator loss \"\"\"\n",
        "  loss_on_reals = cross_entropy_loss(tensorflow.ones_like(predicted_real), predicted_real)\n",
        "  loss_on_fakes = cross_entropy_loss(tensorflow.zeros_like(predicted_fake), predicted_fake)  return loss_on_reals + loss_on_fakes"
      ],
      "metadata": {
        "id": "lSMvKxmYoUDZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_models(generator, discriminator, epoch):\n",
        "  \"\"\" Save models at specific point in time. \"\"\"\n",
        "  tensorflow.keras.models.save_model(\n",
        "    generator,\n",
        "    f'./runs/{UNIQUE_RUN_ID}/generator_{epoch}.model',\n",
        "    overwrite=True,\n",
        "    include_optimizer=True,\n",
        "    save_format=None,\n",
        "    signatures=None,\n",
        "    options=None\n",
        "  )\n",
        "  tensorflow.keras.models.save_model(\n",
        "    discriminator,\n",
        "    f'./runs/{UNIQUE_RUN_ID}/discriminator{epoch}.model',\n",
        "    overwrite=True,\n",
        "    include_optimizer=True,\n",
        "    save_format=None,\n",
        "    signatures=None,\n",
        "    options=None\n",
        "  )\n",
        "\n",
        "def print_training_progress(batch, generator_loss, discriminator_loss):\n",
        "  \"\"\" Print training progress. \"\"\"\n",
        "  print('Losses after mini-batch %5d: generator %e, discriminator %e' %\n",
        "        (batch, generator_loss, discriminator_loss))"
      ],
      "metadata": {
        "id": "YzF7bRsApVDR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tensorflow.function\n",
        "def perform_train_step(real_images, generator, discriminator):\n",
        "  \"\"\" Perform one training step with Gradient Tapes \"\"\"\n",
        "  # Generate noise\n",
        "  noise = generate_noise(BATCH_SIZE)\n",
        "  # Feed forward and loss computation for one batch\n",
        "  with tensorflow.GradientTape() as discriminator_tape, \\\n",
        "      tensorflow.GradientTape() as generator_tape:\n",
        "        # Generate images\n",
        "        generated_images = generator(noise, training=True)\n",
        "        # Discriminate generated and real images\n",
        "        discriminated_generated_images = discriminator(generated_images, training=True)\n",
        "        discriminated_real_images = discriminator(real_images, training=True)\n",
        "        # Compute loss\n",
        "        generator_loss = compute_generator_loss(discriminated_generated_images)\n",
        "        discriminator_loss = compute_discriminator_loss(discriminated_real_images, discriminated_generated_images)\n",
        "  # Compute gradients\n",
        "  generator_gradients = generator_tape.gradient(generator_loss, generator.trainable_variables)\n",
        "  discriminator_gradients = discriminator_tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
        "  # Optimize model using gradients\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
        "  # Return generator and discriminator losses\n",
        "  return (generator_loss, discriminator_loss)"
      ],
      "metadata": {
        "id": "pmG7D2NXwZnd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(num_epochs, image_data, generator, discriminator):\n",
        "  \"\"\" Train the GAN \"\"\"\n",
        "  # Perform one training step per batch for every epoch\n",
        "  for epoch_no in range(num_epochs):\n",
        "    num_batches = image_data.__len__()\n",
        "    print(f'Starting epoch {epoch_no+1} with {num_batches} batches...')\n",
        "    batch_no = 0\n",
        "    # Iterate over batches within epoch\n",
        "    for batch in image_data:\n",
        "      generator_loss, discriminator_loss = perform_train_step(batch, generator, discriminator)\n",
        "      batch_no += 1\n",
        "      # Print statistics and generate image after every n-th batch\n",
        "      if batch_no % PRINT_STATS_AFTER_BATCH == 0:\n",
        "        print_training_progress(batch_no, generator_loss, discriminator_loss)\n",
        "        generate_image(generator, epoch_no, batch_no)\n",
        "    # Save models on epoch completion.\n",
        "    save_models(generator, discriminator, epoch_no)\n",
        "  # Finished :-)\n",
        "  print(f'Finished unique run {UNIQUE_RUN_ID}')"
      ],
      "metadata": {
        "id": "SIXoDLsr5Wur"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_gan():\n",
        "  \"\"\" Initialization and training \"\"\"\n",
        "  # Make run directory\n",
        "  make_directory_for_run()\n",
        "  # Set random seed\n",
        "  tensorflow.random.set_seed(42)\n",
        "  # Get image data\n",
        "  data = load_data()\n",
        "  # Create generator and discriminator\n",
        "  generator = create_generator()\n",
        "  discriminator = create_discriminator()\n",
        "  # Train the GAN\n",
        "  print('Training GAN ...')\n",
        "  train_gan(NUM_EPOCHS, data, generator, discriminator)"
      ],
      "metadata": {
        "id": "1oY8uWee5n7n"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  run_gan()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zitXEBvY5yCl",
        "outputId": "595113fd-0a20-44fa-f330-d096311286c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing training run 56983348-bd6a-4e9b-a421-586f759b0dd5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training GAN ...\n",
            "Starting epoch 1 with 469 batches...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch    50: generator 6.083512e-01, discriminator 1.074799e+00\n",
            "Losses after mini-batch   100: generator 9.137599e-01, discriminator 8.789127e-01\n",
            "Losses after mini-batch   150: generator 1.509523e+00, discriminator 1.189188e+00\n",
            "Losses after mini-batch   200: generator 9.621420e-01, discriminator 1.130656e+00\n",
            "Losses after mini-batch   250: generator 7.280799e-01, discriminator 1.197280e+00\n",
            "Losses after mini-batch   300: generator 8.209665e-01, discriminator 1.288694e+00\n",
            "Losses after mini-batch   350: generator 8.899400e-01, discriminator 1.223952e+00\n",
            "Losses after mini-batch   400: generator 8.486003e-01, discriminator 1.359012e+00\n",
            "Losses after mini-batch   450: generator 8.501085e-01, discriminator 1.219294e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 2 with 469 batches...\n",
            "Losses after mini-batch    50: generator 6.809237e-01, discriminator 1.273303e+00\n",
            "Losses after mini-batch   100: generator 7.385290e-01, discriminator 1.350660e+00\n",
            "Losses after mini-batch   150: generator 7.481502e-01, discriminator 1.333109e+00\n",
            "Losses after mini-batch   200: generator 9.120607e-01, discriminator 1.276973e+00\n",
            "Losses after mini-batch   250: generator 1.773452e+00, discriminator 1.662322e+00\n",
            "Losses after mini-batch   300: generator 6.173795e-01, discriminator 1.488950e+00\n",
            "Losses after mini-batch   350: generator 7.118722e-01, discriminator 1.449420e+00\n",
            "Losses after mini-batch   400: generator 7.587179e-01, discriminator 1.378011e+00\n",
            "Losses after mini-batch   450: generator 8.119051e-01, discriminator 1.383590e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 3 with 469 batches...\n",
            "Losses after mini-batch    50: generator 8.229798e-01, discriminator 1.357448e+00\n",
            "Losses after mini-batch   100: generator 6.458889e-01, discriminator 1.388708e+00\n",
            "Losses after mini-batch   150: generator 7.297405e-01, discriminator 1.324257e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-976fa9bba5d5>:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  plt.figure(figsize=(10, 10))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses after mini-batch   200: generator 7.390824e-01, discriminator 1.332322e+00\n",
            "Losses after mini-batch   250: generator 7.832102e-01, discriminator 1.332389e+00\n",
            "Losses after mini-batch   300: generator 7.795953e-01, discriminator 1.318445e+00\n",
            "Losses after mini-batch   350: generator 6.275954e-01, discriminator 1.391288e+00\n",
            "Losses after mini-batch   400: generator 7.088242e-01, discriminator 1.370917e+00\n",
            "Losses after mini-batch   450: generator 7.075280e-01, discriminator 1.375826e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 4 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.104572e-01, discriminator 1.393995e+00\n",
            "Losses after mini-batch   100: generator 6.762830e-01, discriminator 1.358241e+00\n",
            "Losses after mini-batch   150: generator 8.375401e-01, discriminator 1.386255e+00\n",
            "Losses after mini-batch   200: generator 8.043247e-01, discriminator 1.285590e+00\n",
            "Losses after mini-batch   250: generator 7.948279e-01, discriminator 1.324953e+00\n",
            "Losses after mini-batch   300: generator 7.379059e-01, discriminator 1.275347e+00\n",
            "Losses after mini-batch   350: generator 7.316537e-01, discriminator 1.418363e+00\n",
            "Losses after mini-batch   400: generator 7.897706e-01, discriminator 1.365508e+00\n",
            "Losses after mini-batch   450: generator 8.104433e-01, discriminator 1.291961e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 5 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.110983e-01, discriminator 1.409710e+00\n",
            "Losses after mini-batch   100: generator 8.247247e-01, discriminator 1.445961e+00\n",
            "Losses after mini-batch   150: generator 7.745668e-01, discriminator 1.299639e+00\n",
            "Losses after mini-batch   200: generator 6.937304e-01, discriminator 1.348096e+00\n",
            "Losses after mini-batch   250: generator 6.907934e-01, discriminator 1.312740e+00\n",
            "Losses after mini-batch   300: generator 7.758476e-01, discriminator 1.220686e+00\n",
            "Losses after mini-batch   350: generator 7.023464e-01, discriminator 1.335263e+00\n",
            "Losses after mini-batch   400: generator 8.082269e-01, discriminator 1.299961e+00\n",
            "Losses after mini-batch   450: generator 6.808099e-01, discriminator 1.326786e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 6 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.584026e-01, discriminator 1.430982e+00\n",
            "Losses after mini-batch   100: generator 7.817247e-01, discriminator 1.331134e+00\n",
            "Losses after mini-batch   150: generator 8.025573e-01, discriminator 1.272377e+00\n",
            "Losses after mini-batch   200: generator 7.517604e-01, discriminator 1.409113e+00\n",
            "Losses after mini-batch   250: generator 7.087755e-01, discriminator 1.352607e+00\n",
            "Losses after mini-batch   300: generator 8.146472e-01, discriminator 1.326403e+00\n",
            "Losses after mini-batch   350: generator 6.613101e-01, discriminator 1.329649e+00\n",
            "Losses after mini-batch   400: generator 7.837859e-01, discriminator 1.283663e+00\n",
            "Losses after mini-batch   450: generator 7.052074e-01, discriminator 1.347790e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 7 with 469 batches...\n",
            "Losses after mini-batch    50: generator 9.636979e-01, discriminator 1.196661e+00\n",
            "Losses after mini-batch   100: generator 8.345639e-01, discriminator 1.172109e+00\n",
            "Losses after mini-batch   150: generator 8.197868e-01, discriminator 1.303313e+00\n",
            "Losses after mini-batch   200: generator 7.859682e-01, discriminator 1.300655e+00\n",
            "Losses after mini-batch   250: generator 7.431877e-01, discriminator 1.366132e+00\n",
            "Losses after mini-batch   300: generator 8.492558e-01, discriminator 1.300523e+00\n",
            "Losses after mini-batch   350: generator 7.502949e-01, discriminator 1.326118e+00\n",
            "Losses after mini-batch   400: generator 7.998351e-01, discriminator 1.388374e+00\n",
            "Losses after mini-batch   450: generator 7.689468e-01, discriminator 1.297398e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 8 with 469 batches...\n",
            "Losses after mini-batch    50: generator 6.876721e-01, discriminator 1.369058e+00\n",
            "Losses after mini-batch   100: generator 8.389359e-01, discriminator 1.311682e+00\n",
            "Losses after mini-batch   150: generator 7.803372e-01, discriminator 1.304395e+00\n",
            "Losses after mini-batch   200: generator 7.203580e-01, discriminator 1.467545e+00\n",
            "Losses after mini-batch   250: generator 7.202933e-01, discriminator 1.431664e+00\n",
            "Losses after mini-batch   300: generator 7.607494e-01, discriminator 1.278112e+00\n",
            "Losses after mini-batch   350: generator 7.511708e-01, discriminator 1.317309e+00\n",
            "Losses after mini-batch   400: generator 8.003991e-01, discriminator 1.393005e+00\n",
            "Losses after mini-batch   450: generator 6.969006e-01, discriminator 1.375711e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 9 with 469 batches...\n",
            "Losses after mini-batch    50: generator 6.991159e-01, discriminator 1.282141e+00\n",
            "Losses after mini-batch   100: generator 8.401325e-01, discriminator 1.246343e+00\n",
            "Losses after mini-batch   150: generator 7.279503e-01, discriminator 1.345723e+00\n",
            "Losses after mini-batch   200: generator 9.106365e-01, discriminator 1.336384e+00\n",
            "Losses after mini-batch   250: generator 7.120250e-01, discriminator 1.336529e+00\n",
            "Losses after mini-batch   300: generator 7.500929e-01, discriminator 1.335311e+00\n",
            "Losses after mini-batch   350: generator 7.194567e-01, discriminator 1.382813e+00\n",
            "Losses after mini-batch   400: generator 7.272000e-01, discriminator 1.375568e+00\n",
            "Losses after mini-batch   450: generator 7.404460e-01, discriminator 1.364916e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 10 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.300027e-01, discriminator 1.412188e+00\n",
            "Losses after mini-batch   100: generator 7.260762e-01, discriminator 1.377120e+00\n",
            "Losses after mini-batch   150: generator 7.114238e-01, discriminator 1.427187e+00\n",
            "Losses after mini-batch   200: generator 7.040770e-01, discriminator 1.370480e+00\n",
            "Losses after mini-batch   250: generator 7.463863e-01, discriminator 1.382474e+00\n",
            "Losses after mini-batch   300: generator 7.944437e-01, discriminator 1.382963e+00\n",
            "Losses after mini-batch   350: generator 8.169943e-01, discriminator 1.258258e+00\n",
            "Losses after mini-batch   400: generator 8.227261e-01, discriminator 1.356858e+00\n",
            "Losses after mini-batch   450: generator 6.720955e-01, discriminator 1.350700e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 11 with 469 batches...\n",
            "Losses after mini-batch    50: generator 6.906941e-01, discriminator 1.392259e+00\n",
            "Losses after mini-batch   100: generator 8.144113e-01, discriminator 1.397781e+00\n",
            "Losses after mini-batch   150: generator 7.148734e-01, discriminator 1.357025e+00\n",
            "Losses after mini-batch   200: generator 8.617530e-01, discriminator 1.368625e+00\n",
            "Losses after mini-batch   250: generator 7.008206e-01, discriminator 1.352257e+00\n",
            "Losses after mini-batch   300: generator 7.705494e-01, discriminator 1.317620e+00\n",
            "Losses after mini-batch   350: generator 6.800658e-01, discriminator 1.362123e+00\n",
            "Losses after mini-batch   400: generator 7.733262e-01, discriminator 1.331434e+00\n",
            "Losses after mini-batch   450: generator 6.065189e-01, discriminator 1.428172e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 12 with 469 batches...\n",
            "Losses after mini-batch    50: generator 6.339919e-01, discriminator 1.380609e+00\n",
            "Losses after mini-batch   100: generator 7.941056e-01, discriminator 1.362304e+00\n",
            "Losses after mini-batch   150: generator 8.086181e-01, discriminator 1.358355e+00\n",
            "Losses after mini-batch   200: generator 7.387040e-01, discriminator 1.305318e+00\n",
            "Losses after mini-batch   250: generator 7.599280e-01, discriminator 1.307786e+00\n",
            "Losses after mini-batch   300: generator 6.892121e-01, discriminator 1.379345e+00\n",
            "Losses after mini-batch   350: generator 8.065300e-01, discriminator 1.361282e+00\n",
            "Losses after mini-batch   400: generator 7.278396e-01, discriminator 1.332705e+00\n",
            "Losses after mini-batch   450: generator 7.097861e-01, discriminator 1.349742e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 13 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.494816e-01, discriminator 1.331646e+00\n",
            "Losses after mini-batch   100: generator 7.392235e-01, discriminator 1.333606e+00\n",
            "Losses after mini-batch   150: generator 7.161571e-01, discriminator 1.363495e+00\n",
            "Losses after mini-batch   200: generator 6.585975e-01, discriminator 1.368669e+00\n",
            "Losses after mini-batch   250: generator 7.990671e-01, discriminator 1.335069e+00\n",
            "Losses after mini-batch   300: generator 7.866332e-01, discriminator 1.357747e+00\n",
            "Losses after mini-batch   350: generator 6.943421e-01, discriminator 1.320243e+00\n",
            "Losses after mini-batch   400: generator 7.804570e-01, discriminator 1.352312e+00\n",
            "Losses after mini-batch   450: generator 7.225326e-01, discriminator 1.374720e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 14 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.178972e-01, discriminator 1.328936e+00\n",
            "Losses after mini-batch   100: generator 7.377545e-01, discriminator 1.383526e+00\n",
            "Losses after mini-batch   150: generator 7.614180e-01, discriminator 1.281244e+00\n",
            "Losses after mini-batch   200: generator 7.694008e-01, discriminator 1.435360e+00\n",
            "Losses after mini-batch   250: generator 6.567063e-01, discriminator 1.354552e+00\n",
            "Losses after mini-batch   300: generator 6.671706e-01, discriminator 1.336831e+00\n",
            "Losses after mini-batch   350: generator 8.581192e-01, discriminator 1.367675e+00\n",
            "Losses after mini-batch   400: generator 7.035510e-01, discriminator 1.330270e+00\n",
            "Losses after mini-batch   450: generator 7.884624e-01, discriminator 1.327434e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 15 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.312745e-01, discriminator 1.300006e+00\n",
            "Losses after mini-batch   100: generator 6.807362e-01, discriminator 1.356718e+00\n",
            "Losses after mini-batch   150: generator 7.856423e-01, discriminator 1.393342e+00\n",
            "Losses after mini-batch   200: generator 7.223614e-01, discriminator 1.317047e+00\n",
            "Losses after mini-batch   250: generator 6.871568e-01, discriminator 1.363189e+00\n",
            "Losses after mini-batch   300: generator 7.013395e-01, discriminator 1.374240e+00\n",
            "Losses after mini-batch   350: generator 6.614070e-01, discriminator 1.395868e+00\n",
            "Losses after mini-batch   400: generator 7.199530e-01, discriminator 1.345268e+00\n",
            "Losses after mini-batch   450: generator 7.470781e-01, discriminator 1.314036e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 16 with 469 batches...\n",
            "Losses after mini-batch    50: generator 8.942406e-01, discriminator 1.403765e+00\n",
            "Losses after mini-batch   100: generator 8.357998e-01, discriminator 1.388585e+00\n",
            "Losses after mini-batch   150: generator 7.906649e-01, discriminator 1.313509e+00\n",
            "Losses after mini-batch   200: generator 6.547155e-01, discriminator 1.378799e+00\n",
            "Losses after mini-batch   250: generator 6.805367e-01, discriminator 1.360593e+00\n",
            "Losses after mini-batch   300: generator 7.562759e-01, discriminator 1.373563e+00\n",
            "Losses after mini-batch   350: generator 7.774587e-01, discriminator 1.341704e+00\n",
            "Losses after mini-batch   400: generator 8.224108e-01, discriminator 1.335371e+00\n",
            "Losses after mini-batch   450: generator 7.479020e-01, discriminator 1.306810e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 17 with 469 batches...\n",
            "Losses after mini-batch    50: generator 6.967339e-01, discriminator 1.352462e+00\n",
            "Losses after mini-batch   100: generator 8.184362e-01, discriminator 1.404944e+00\n",
            "Losses after mini-batch   150: generator 6.958212e-01, discriminator 1.329258e+00\n",
            "Losses after mini-batch   200: generator 6.925747e-01, discriminator 1.371886e+00\n",
            "Losses after mini-batch   250: generator 6.752118e-01, discriminator 1.338222e+00\n",
            "Losses after mini-batch   300: generator 7.260491e-01, discriminator 1.316138e+00\n",
            "Losses after mini-batch   350: generator 7.939812e-01, discriminator 1.399707e+00\n",
            "Losses after mini-batch   400: generator 8.593982e-01, discriminator 1.357726e+00\n",
            "Losses after mini-batch   450: generator 9.287949e-01, discriminator 1.401946e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 18 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.171499e-01, discriminator 1.310568e+00\n",
            "Losses after mini-batch   100: generator 7.498897e-01, discriminator 1.320726e+00\n",
            "Losses after mini-batch   150: generator 6.745183e-01, discriminator 1.288846e+00\n",
            "Losses after mini-batch   200: generator 6.912855e-01, discriminator 1.322690e+00\n",
            "Losses after mini-batch   250: generator 7.126349e-01, discriminator 1.346330e+00\n",
            "Losses after mini-batch   300: generator 7.247193e-01, discriminator 1.323358e+00\n",
            "Losses after mini-batch   350: generator 7.344874e-01, discriminator 1.385641e+00\n",
            "Losses after mini-batch   400: generator 6.759681e-01, discriminator 1.313085e+00\n",
            "Losses after mini-batch   450: generator 7.289126e-01, discriminator 1.358355e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 19 with 469 batches...\n",
            "Losses after mini-batch    50: generator 8.228992e-01, discriminator 1.309868e+00\n",
            "Losses after mini-batch   100: generator 7.914758e-01, discriminator 1.299712e+00\n",
            "Losses after mini-batch   150: generator 6.739590e-01, discriminator 1.351617e+00\n",
            "Losses after mini-batch   200: generator 7.996088e-01, discriminator 1.350809e+00\n",
            "Losses after mini-batch   250: generator 7.951838e-01, discriminator 1.320334e+00\n",
            "Losses after mini-batch   300: generator 7.915680e-01, discriminator 1.332878e+00\n",
            "Losses after mini-batch   350: generator 6.413935e-01, discriminator 1.350824e+00\n",
            "Losses after mini-batch   400: generator 6.528510e-01, discriminator 1.334697e+00\n",
            "Losses after mini-batch   450: generator 8.588615e-01, discriminator 1.334859e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 20 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.060881e-01, discriminator 1.372640e+00\n",
            "Losses after mini-batch   100: generator 8.945836e-01, discriminator 1.332261e+00\n",
            "Losses after mini-batch   150: generator 7.244557e-01, discriminator 1.312492e+00\n",
            "Losses after mini-batch   200: generator 7.514672e-01, discriminator 1.327451e+00\n",
            "Losses after mini-batch   250: generator 7.743584e-01, discriminator 1.362874e+00\n",
            "Losses after mini-batch   300: generator 8.286767e-01, discriminator 1.339736e+00\n",
            "Losses after mini-batch   350: generator 7.518857e-01, discriminator 1.334325e+00\n",
            "Losses after mini-batch   400: generator 7.730464e-01, discriminator 1.339169e+00\n",
            "Losses after mini-batch   450: generator 7.235564e-01, discriminator 1.328135e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 21 with 469 batches...\n",
            "Losses after mini-batch    50: generator 8.399807e-01, discriminator 1.355547e+00\n",
            "Losses after mini-batch   100: generator 7.812575e-01, discriminator 1.306108e+00\n",
            "Losses after mini-batch   150: generator 8.790507e-01, discriminator 1.313641e+00\n",
            "Losses after mini-batch   200: generator 7.652043e-01, discriminator 1.269333e+00\n",
            "Losses after mini-batch   250: generator 6.763836e-01, discriminator 1.343255e+00\n",
            "Losses after mini-batch   300: generator 7.085800e-01, discriminator 1.321799e+00\n",
            "Losses after mini-batch   350: generator 6.604282e-01, discriminator 1.338071e+00\n",
            "Losses after mini-batch   400: generator 6.754705e-01, discriminator 1.328451e+00\n",
            "Losses after mini-batch   450: generator 7.625000e-01, discriminator 1.289695e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 22 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.533582e-01, discriminator 1.337291e+00\n",
            "Losses after mini-batch   100: generator 7.652332e-01, discriminator 1.364384e+00\n",
            "Losses after mini-batch   150: generator 7.698829e-01, discriminator 1.338515e+00\n",
            "Losses after mini-batch   200: generator 7.725568e-01, discriminator 1.366372e+00\n",
            "Losses after mini-batch   250: generator 7.595330e-01, discriminator 1.327696e+00\n",
            "Losses after mini-batch   300: generator 7.870984e-01, discriminator 1.382498e+00\n",
            "Losses after mini-batch   350: generator 7.132899e-01, discriminator 1.307885e+00\n",
            "Losses after mini-batch   400: generator 7.587049e-01, discriminator 1.297087e+00\n",
            "Losses after mini-batch   450: generator 7.439218e-01, discriminator 1.334371e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 23 with 469 batches...\n",
            "Losses after mini-batch    50: generator 6.307435e-01, discriminator 1.351121e+00\n",
            "Losses after mini-batch   100: generator 8.320514e-01, discriminator 1.357270e+00\n",
            "Losses after mini-batch   150: generator 7.177364e-01, discriminator 1.399015e+00\n",
            "Losses after mini-batch   200: generator 6.492202e-01, discriminator 1.359951e+00\n",
            "Losses after mini-batch   250: generator 7.807273e-01, discriminator 1.329012e+00\n",
            "Losses after mini-batch   300: generator 7.690259e-01, discriminator 1.323602e+00\n",
            "Losses after mini-batch   350: generator 7.039071e-01, discriminator 1.324048e+00\n",
            "Losses after mini-batch   400: generator 9.459196e-01, discriminator 1.350630e+00\n",
            "Losses after mini-batch   450: generator 7.850765e-01, discriminator 1.327817e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 24 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.030617e-01, discriminator 1.355283e+00\n",
            "Losses after mini-batch   100: generator 6.810868e-01, discriminator 1.360924e+00\n",
            "Losses after mini-batch   150: generator 7.523190e-01, discriminator 1.399258e+00\n",
            "Losses after mini-batch   200: generator 7.985705e-01, discriminator 1.314563e+00\n",
            "Losses after mini-batch   250: generator 7.941527e-01, discriminator 1.328188e+00\n",
            "Losses after mini-batch   300: generator 6.911694e-01, discriminator 1.328533e+00\n",
            "Losses after mini-batch   350: generator 8.444154e-01, discriminator 1.323800e+00\n",
            "Losses after mini-batch   400: generator 7.697126e-01, discriminator 1.350508e+00\n",
            "Losses after mini-batch   450: generator 7.705312e-01, discriminator 1.345028e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 25 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.097336e-01, discriminator 1.309530e+00\n",
            "Losses after mini-batch   100: generator 7.036536e-01, discriminator 1.348387e+00\n",
            "Losses after mini-batch   150: generator 7.314169e-01, discriminator 1.310076e+00\n",
            "Losses after mini-batch   200: generator 8.078769e-01, discriminator 1.335173e+00\n",
            "Losses after mini-batch   250: generator 8.437642e-01, discriminator 1.402101e+00\n",
            "Losses after mini-batch   300: generator 6.910769e-01, discriminator 1.350498e+00\n",
            "Losses after mini-batch   350: generator 8.169674e-01, discriminator 1.332184e+00\n",
            "Losses after mini-batch   400: generator 8.733426e-01, discriminator 1.286455e+00\n",
            "Losses after mini-batch   450: generator 6.649932e-01, discriminator 1.395586e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 26 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.632335e-01, discriminator 1.319947e+00\n",
            "Losses after mini-batch   100: generator 8.012977e-01, discriminator 1.345000e+00\n",
            "Losses after mini-batch   150: generator 7.087354e-01, discriminator 1.302290e+00\n",
            "Losses after mini-batch   200: generator 8.877268e-01, discriminator 1.366814e+00\n",
            "Losses after mini-batch   250: generator 8.179091e-01, discriminator 1.320513e+00\n",
            "Losses after mini-batch   300: generator 8.027083e-01, discriminator 1.307445e+00\n",
            "Losses after mini-batch   350: generator 6.218585e-01, discriminator 1.376593e+00\n",
            "Losses after mini-batch   400: generator 6.927058e-01, discriminator 1.328550e+00\n",
            "Losses after mini-batch   450: generator 7.883804e-01, discriminator 1.262524e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 27 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.227151e-01, discriminator 1.387987e+00\n",
            "Losses after mini-batch   100: generator 7.337344e-01, discriminator 1.250718e+00\n",
            "Losses after mini-batch   150: generator 7.663070e-01, discriminator 1.403484e+00\n",
            "Losses after mini-batch   200: generator 7.675667e-01, discriminator 1.352589e+00\n",
            "Losses after mini-batch   250: generator 8.456475e-01, discriminator 1.372984e+00\n",
            "Losses after mini-batch   300: generator 7.952018e-01, discriminator 1.329813e+00\n",
            "Losses after mini-batch   350: generator 6.909213e-01, discriminator 1.345002e+00\n",
            "Losses after mini-batch   400: generator 7.573133e-01, discriminator 1.369924e+00\n",
            "Losses after mini-batch   450: generator 8.068142e-01, discriminator 1.349408e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 28 with 469 batches...\n",
            "Losses after mini-batch    50: generator 8.299317e-01, discriminator 1.361530e+00\n",
            "Losses after mini-batch   100: generator 8.583891e-01, discriminator 1.364361e+00\n",
            "Losses after mini-batch   150: generator 7.984884e-01, discriminator 1.302007e+00\n",
            "Losses after mini-batch   200: generator 6.126646e-01, discriminator 1.398838e+00\n",
            "Losses after mini-batch   250: generator 6.706316e-01, discriminator 1.351156e+00\n",
            "Losses after mini-batch   300: generator 7.411962e-01, discriminator 1.315724e+00\n",
            "Losses after mini-batch   350: generator 8.221917e-01, discriminator 1.402602e+00\n",
            "Losses after mini-batch   400: generator 8.702891e-01, discriminator 1.344037e+00\n",
            "Losses after mini-batch   450: generator 7.951226e-01, discriminator 1.278317e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 29 with 469 batches...\n",
            "Losses after mini-batch    50: generator 8.317080e-01, discriminator 1.339809e+00\n",
            "Losses after mini-batch   100: generator 7.050955e-01, discriminator 1.373411e+00\n",
            "Losses after mini-batch   150: generator 6.731048e-01, discriminator 1.330880e+00\n",
            "Losses after mini-batch   200: generator 7.002077e-01, discriminator 1.380709e+00\n",
            "Losses after mini-batch   250: generator 7.660918e-01, discriminator 1.350064e+00\n",
            "Losses after mini-batch   300: generator 6.910865e-01, discriminator 1.359435e+00\n",
            "Losses after mini-batch   350: generator 6.928551e-01, discriminator 1.364508e+00\n",
            "Losses after mini-batch   400: generator 7.073914e-01, discriminator 1.332851e+00\n",
            "Losses after mini-batch   450: generator 7.990277e-01, discriminator 1.340711e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 30 with 469 batches...\n",
            "Losses after mini-batch    50: generator 6.942466e-01, discriminator 1.365055e+00\n",
            "Losses after mini-batch   100: generator 6.688657e-01, discriminator 1.380544e+00\n",
            "Losses after mini-batch   150: generator 7.472951e-01, discriminator 1.332559e+00\n",
            "Losses after mini-batch   200: generator 9.100275e-01, discriminator 1.325304e+00\n",
            "Losses after mini-batch   250: generator 6.845056e-01, discriminator 1.346795e+00\n",
            "Losses after mini-batch   300: generator 7.981673e-01, discriminator 1.317613e+00\n",
            "Losses after mini-batch   350: generator 8.689693e-01, discriminator 1.323129e+00\n",
            "Losses after mini-batch   400: generator 7.665871e-01, discriminator 1.320024e+00\n",
            "Losses after mini-batch   450: generator 7.705839e-01, discriminator 1.303885e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 31 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.926005e-01, discriminator 1.367060e+00\n",
            "Losses after mini-batch   100: generator 6.997211e-01, discriminator 1.339432e+00\n",
            "Losses after mini-batch   150: generator 6.787305e-01, discriminator 1.344802e+00\n",
            "Losses after mini-batch   200: generator 8.295162e-01, discriminator 1.311346e+00\n",
            "Losses after mini-batch   250: generator 7.960176e-01, discriminator 1.329577e+00\n",
            "Losses after mini-batch   300: generator 8.599155e-01, discriminator 1.290448e+00\n",
            "Losses after mini-batch   350: generator 8.356571e-01, discriminator 1.285888e+00\n",
            "Losses after mini-batch   400: generator 9.757581e-01, discriminator 1.353971e+00\n",
            "Losses after mini-batch   450: generator 8.995804e-01, discriminator 1.312773e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 32 with 469 batches...\n",
            "Losses after mini-batch    50: generator 9.534715e-01, discriminator 1.345188e+00\n",
            "Losses after mini-batch   100: generator 8.492973e-01, discriminator 1.300016e+00\n",
            "Losses after mini-batch   150: generator 7.619184e-01, discriminator 1.293157e+00\n",
            "Losses after mini-batch   200: generator 8.104941e-01, discriminator 1.276798e+00\n",
            "Losses after mini-batch   250: generator 8.604106e-01, discriminator 1.324264e+00\n",
            "Losses after mini-batch   300: generator 6.868139e-01, discriminator 1.373274e+00\n",
            "Losses after mini-batch   350: generator 7.822136e-01, discriminator 1.330409e+00\n",
            "Losses after mini-batch   400: generator 7.783924e-01, discriminator 1.353370e+00\n",
            "Losses after mini-batch   450: generator 6.908836e-01, discriminator 1.363022e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 33 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.856233e-01, discriminator 1.354356e+00\n",
            "Losses after mini-batch   100: generator 8.952855e-01, discriminator 1.332747e+00\n",
            "Losses after mini-batch   150: generator 8.314496e-01, discriminator 1.337868e+00\n",
            "Losses after mini-batch   200: generator 7.113158e-01, discriminator 1.352781e+00\n",
            "Losses after mini-batch   250: generator 6.690710e-01, discriminator 1.384607e+00\n",
            "Losses after mini-batch   300: generator 8.047769e-01, discriminator 1.278260e+00\n",
            "Losses after mini-batch   350: generator 8.096984e-01, discriminator 1.353786e+00\n",
            "Losses after mini-batch   400: generator 8.200168e-01, discriminator 1.344968e+00\n",
            "Losses after mini-batch   450: generator 9.169576e-01, discriminator 1.319175e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 34 with 469 batches...\n",
            "Losses after mini-batch    50: generator 8.689268e-01, discriminator 1.310235e+00\n",
            "Losses after mini-batch   100: generator 6.864870e-01, discriminator 1.341211e+00\n",
            "Losses after mini-batch   150: generator 7.939855e-01, discriminator 1.323745e+00\n",
            "Losses after mini-batch   200: generator 7.893766e-01, discriminator 1.352675e+00\n",
            "Losses after mini-batch   250: generator 7.691847e-01, discriminator 1.326418e+00\n",
            "Losses after mini-batch   300: generator 7.601413e-01, discriminator 1.300229e+00\n",
            "Losses after mini-batch   350: generator 7.394863e-01, discriminator 1.365747e+00\n",
            "Losses after mini-batch   400: generator 7.920951e-01, discriminator 1.334029e+00\n",
            "Losses after mini-batch   450: generator 7.003205e-01, discriminator 1.360284e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 35 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.516238e-01, discriminator 1.338634e+00\n",
            "Losses after mini-batch   100: generator 7.904243e-01, discriminator 1.305820e+00\n",
            "Losses after mini-batch   150: generator 7.128096e-01, discriminator 1.309247e+00\n",
            "Losses after mini-batch   200: generator 8.205422e-01, discriminator 1.376051e+00\n",
            "Losses after mini-batch   250: generator 7.412194e-01, discriminator 1.365420e+00\n",
            "Losses after mini-batch   300: generator 6.647032e-01, discriminator 1.428193e+00\n",
            "Losses after mini-batch   350: generator 7.337234e-01, discriminator 1.284039e+00\n",
            "Losses after mini-batch   400: generator 7.209886e-01, discriminator 1.336158e+00\n",
            "Losses after mini-batch   450: generator 7.262203e-01, discriminator 1.366145e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 36 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.668360e-01, discriminator 1.295006e+00\n",
            "Losses after mini-batch   100: generator 8.110176e-01, discriminator 1.303517e+00\n",
            "Losses after mini-batch   150: generator 7.563549e-01, discriminator 1.350380e+00\n",
            "Losses after mini-batch   200: generator 7.618250e-01, discriminator 1.355858e+00\n",
            "Losses after mini-batch   250: generator 6.401238e-01, discriminator 1.405002e+00\n",
            "Losses after mini-batch   300: generator 6.219871e-01, discriminator 1.378605e+00\n",
            "Losses after mini-batch   350: generator 7.505543e-01, discriminator 1.316518e+00\n",
            "Losses after mini-batch   400: generator 7.753630e-01, discriminator 1.349546e+00\n",
            "Losses after mini-batch   450: generator 8.453493e-01, discriminator 1.399548e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 37 with 469 batches...\n",
            "Losses after mini-batch    50: generator 8.208752e-01, discriminator 1.333485e+00\n",
            "Losses after mini-batch   100: generator 7.646249e-01, discriminator 1.321214e+00\n",
            "Losses after mini-batch   150: generator 7.784506e-01, discriminator 1.392122e+00\n",
            "Losses after mini-batch   200: generator 7.324817e-01, discriminator 1.361850e+00\n",
            "Losses after mini-batch   250: generator 7.250780e-01, discriminator 1.343878e+00\n",
            "Losses after mini-batch   300: generator 7.714447e-01, discriminator 1.358335e+00\n",
            "Losses after mini-batch   350: generator 7.768122e-01, discriminator 1.354947e+00\n",
            "Losses after mini-batch   400: generator 7.447650e-01, discriminator 1.346027e+00\n",
            "Losses after mini-batch   450: generator 7.773774e-01, discriminator 1.313257e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 38 with 469 batches...\n",
            "Losses after mini-batch    50: generator 8.017374e-01, discriminator 1.349824e+00\n",
            "Losses after mini-batch   100: generator 7.450615e-01, discriminator 1.389421e+00\n",
            "Losses after mini-batch   150: generator 7.892028e-01, discriminator 1.373990e+00\n",
            "Losses after mini-batch   200: generator 7.796409e-01, discriminator 1.312718e+00\n",
            "Losses after mini-batch   250: generator 7.334695e-01, discriminator 1.339288e+00\n",
            "Losses after mini-batch   300: generator 8.278584e-01, discriminator 1.289968e+00\n",
            "Losses after mini-batch   350: generator 6.908911e-01, discriminator 1.337841e+00\n",
            "Losses after mini-batch   400: generator 7.738956e-01, discriminator 1.348224e+00\n",
            "Losses after mini-batch   450: generator 8.913228e-01, discriminator 1.357736e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 39 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.297568e-01, discriminator 1.335681e+00\n",
            "Losses after mini-batch   100: generator 7.372870e-01, discriminator 1.290832e+00\n",
            "Losses after mini-batch   150: generator 7.459774e-01, discriminator 1.395645e+00\n",
            "Losses after mini-batch   200: generator 6.911379e-01, discriminator 1.402220e+00\n",
            "Losses after mini-batch   250: generator 7.037437e-01, discriminator 1.386116e+00\n",
            "Losses after mini-batch   300: generator 8.019704e-01, discriminator 1.302573e+00\n",
            "Losses after mini-batch   350: generator 7.692708e-01, discriminator 1.296261e+00\n",
            "Losses after mini-batch   400: generator 7.277949e-01, discriminator 1.393679e+00\n",
            "Losses after mini-batch   450: generator 8.498527e-01, discriminator 1.364574e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 40 with 469 batches...\n",
            "Losses after mini-batch    50: generator 9.038702e-01, discriminator 1.344589e+00\n",
            "Losses after mini-batch   100: generator 7.005734e-01, discriminator 1.376661e+00\n",
            "Losses after mini-batch   150: generator 6.827070e-01, discriminator 1.351630e+00\n",
            "Losses after mini-batch   200: generator 7.875580e-01, discriminator 1.317354e+00\n",
            "Losses after mini-batch   250: generator 7.076335e-01, discriminator 1.335874e+00\n",
            "Losses after mini-batch   300: generator 7.749444e-01, discriminator 1.327024e+00\n",
            "Losses after mini-batch   350: generator 6.558946e-01, discriminator 1.342142e+00\n",
            "Losses after mini-batch   400: generator 7.997264e-01, discriminator 1.391269e+00\n",
            "Losses after mini-batch   450: generator 8.040113e-01, discriminator 1.362373e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 41 with 469 batches...\n",
            "Losses after mini-batch    50: generator 6.248856e-01, discriminator 1.376479e+00\n",
            "Losses after mini-batch   100: generator 8.186157e-01, discriminator 1.310379e+00\n",
            "Losses after mini-batch   150: generator 7.014700e-01, discriminator 1.370069e+00\n",
            "Losses after mini-batch   200: generator 6.493682e-01, discriminator 1.365399e+00\n",
            "Losses after mini-batch   250: generator 7.953185e-01, discriminator 1.319661e+00\n",
            "Losses after mini-batch   300: generator 8.033457e-01, discriminator 1.332400e+00\n",
            "Losses after mini-batch   350: generator 8.105234e-01, discriminator 1.335108e+00\n",
            "Losses after mini-batch   400: generator 8.345898e-01, discriminator 1.333821e+00\n",
            "Losses after mini-batch   450: generator 7.087324e-01, discriminator 1.332968e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 42 with 469 batches...\n",
            "Losses after mini-batch    50: generator 8.823612e-01, discriminator 1.303475e+00\n",
            "Losses after mini-batch   100: generator 6.499320e-01, discriminator 1.363900e+00\n",
            "Losses after mini-batch   150: generator 8.246173e-01, discriminator 1.326762e+00\n",
            "Losses after mini-batch   200: generator 8.901215e-01, discriminator 1.359518e+00\n",
            "Losses after mini-batch   250: generator 8.008792e-01, discriminator 1.292686e+00\n",
            "Losses after mini-batch   300: generator 6.636783e-01, discriminator 1.386125e+00\n",
            "Losses after mini-batch   350: generator 7.044046e-01, discriminator 1.351663e+00\n",
            "Losses after mini-batch   400: generator 7.478016e-01, discriminator 1.362737e+00\n",
            "Losses after mini-batch   450: generator 7.392920e-01, discriminator 1.346088e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 43 with 469 batches...\n",
            "Losses after mini-batch    50: generator 9.021823e-01, discriminator 1.347489e+00\n",
            "Losses after mini-batch   100: generator 8.468453e-01, discriminator 1.364070e+00\n",
            "Losses after mini-batch   150: generator 7.008412e-01, discriminator 1.357675e+00\n",
            "Losses after mini-batch   200: generator 7.320494e-01, discriminator 1.314123e+00\n",
            "Losses after mini-batch   250: generator 7.821287e-01, discriminator 1.309759e+00\n",
            "Losses after mini-batch   300: generator 8.157189e-01, discriminator 1.274502e+00\n",
            "Losses after mini-batch   350: generator 7.110873e-01, discriminator 1.399105e+00\n",
            "Losses after mini-batch   400: generator 7.439080e-01, discriminator 1.341756e+00\n",
            "Losses after mini-batch   450: generator 7.884851e-01, discriminator 1.402885e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 44 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.169980e-01, discriminator 1.340802e+00\n",
            "Losses after mini-batch   100: generator 7.938173e-01, discriminator 1.337631e+00\n",
            "Losses after mini-batch   150: generator 7.152455e-01, discriminator 1.340306e+00\n",
            "Losses after mini-batch   200: generator 6.683345e-01, discriminator 1.381153e+00\n",
            "Losses after mini-batch   250: generator 6.697301e-01, discriminator 1.366428e+00\n",
            "Losses after mini-batch   300: generator 8.746617e-01, discriminator 1.312445e+00\n",
            "Losses after mini-batch   350: generator 7.787442e-01, discriminator 1.362245e+00\n",
            "Losses after mini-batch   400: generator 7.511564e-01, discriminator 1.328067e+00\n",
            "Losses after mini-batch   450: generator 7.802117e-01, discriminator 1.376372e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 45 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.600512e-01, discriminator 1.383873e+00\n",
            "Losses after mini-batch   100: generator 8.160909e-01, discriminator 1.358708e+00\n",
            "Losses after mini-batch   150: generator 7.226414e-01, discriminator 1.354639e+00\n",
            "Losses after mini-batch   200: generator 8.329083e-01, discriminator 1.312028e+00\n",
            "Losses after mini-batch   250: generator 7.039732e-01, discriminator 1.362642e+00\n",
            "Losses after mini-batch   300: generator 7.578528e-01, discriminator 1.323241e+00\n",
            "Losses after mini-batch   350: generator 7.364562e-01, discriminator 1.362296e+00\n",
            "Losses after mini-batch   400: generator 7.791233e-01, discriminator 1.302801e+00\n",
            "Losses after mini-batch   450: generator 9.140904e-01, discriminator 1.364608e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 46 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.639457e-01, discriminator 1.285058e+00\n",
            "Losses after mini-batch   100: generator 7.593336e-01, discriminator 1.352478e+00\n",
            "Losses after mini-batch   150: generator 7.456541e-01, discriminator 1.390308e+00\n",
            "Losses after mini-batch   200: generator 6.982637e-01, discriminator 1.343951e+00\n",
            "Losses after mini-batch   250: generator 7.573276e-01, discriminator 1.327403e+00\n",
            "Losses after mini-batch   300: generator 7.050091e-01, discriminator 1.358659e+00\n",
            "Losses after mini-batch   350: generator 7.636526e-01, discriminator 1.365343e+00\n",
            "Losses after mini-batch   400: generator 7.119561e-01, discriminator 1.324384e+00\n",
            "Losses after mini-batch   450: generator 7.904121e-01, discriminator 1.329813e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 47 with 469 batches...\n",
            "Losses after mini-batch    50: generator 7.064092e-01, discriminator 1.342235e+00\n",
            "Losses after mini-batch   100: generator 7.888452e-01, discriminator 1.315889e+00\n",
            "Losses after mini-batch   150: generator 7.771449e-01, discriminator 1.310146e+00\n",
            "Losses after mini-batch   200: generator 7.131274e-01, discriminator 1.326784e+00\n",
            "Losses after mini-batch   250: generator 8.066496e-01, discriminator 1.327012e+00\n",
            "Losses after mini-batch   300: generator 8.047030e-01, discriminator 1.362932e+00\n",
            "Losses after mini-batch   350: generator 9.941527e-01, discriminator 1.333535e+00\n",
            "Losses after mini-batch   400: generator 7.116566e-01, discriminator 1.300182e+00\n",
            "Losses after mini-batch   450: generator 6.952834e-01, discriminator 1.345784e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 48 with 469 batches...\n"
          ]
        }
      ]
    }
  ]
}